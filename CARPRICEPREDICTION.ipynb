{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxU7BQkS/gPMsICDcYSQcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2256haradityam/Projects/blob/main/CARPRICEPREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4m3-2g621mK6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://github.com/2256haradityam/dataset/raw/refs/heads/main/breast-cancer_updated.csv')"
      ],
      "metadata": {
        "id": "mo5jHXYL2YWI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4tJ5fwpb2b7f",
        "outputId": "b0d9580b-06df-4def-b070-f4fa256c7974"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    CLASS  SOT  INV  NC  DM  IR       ACI  MII  RNI  CSR       NIS  RI  \\\n",
              "0       0   34    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "1       0   24    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "2       0   24    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "3       0   19    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "4       0    4    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "5       0   19    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "6       0   29    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "7       0   24    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "8       0   50    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "9       0   24    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "10      0    4    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "11      0   29    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "12      0   14    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "13      0   29    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "14      0   34    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "15      0   34    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "16      0   19    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "17      0   34    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "18      0   34    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "19      0   34    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "20      0   44    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "21      0   19    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "22      0   29    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "23      0   44    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "24      0   39    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "25      0   29    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "26      0   24    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "27      0   29    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "28      0   44    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "29      0   34    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "30      0   44    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "31      0   19    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "32      0   14    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "33      0   14    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "34      0   14    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "35      0   34    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "36      0    4    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "37      0   19    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "38      0   14    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "39      0   34    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "40      0   24    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "41      0   29    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "42      0    9    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "43      0   14    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "44      0   50    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "45      0   34    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "46      0   29    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "47      0   29    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "48      0   24    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "49      0   24    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "50      0   19    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "51      0   24    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "52      0   19    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "53      0   24    2   0   3   0 -0.204639    0    0    0 -0.210591   0   \n",
              "54      0   44    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "55      0   44    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "56      0    4    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "57      0    9    2   0   2   0 -0.453878    0    0    0 -0.405141   0   \n",
              "58      0   34    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "59      0   19    2   0   1   0 -0.703117    0    0    0 -0.599691   0   \n",
              "\n",
              "         NTI      TPRI  NCM     TNRF  MRW       ATR       MTI  \n",
              "0  -0.325646 -0.133848    0 -0.47724    0 -0.519494 -0.456972  \n",
              "1  -0.492194 -0.450079    0 -0.47724    0 -0.624535 -0.555509  \n",
              "2  -0.492194 -0.450079    0 -0.47724    0 -0.624535 -0.555509  \n",
              "3  -0.575468 -0.508640    0 -0.47724    0 -0.309411 -0.417557  \n",
              "4  -0.825291 -0.684324    0 -0.47724    0 -2.025086 -0.752584  \n",
              "5  -0.575468 -0.508640    0 -0.47724    0 -0.309411 -0.417557  \n",
              "6  -0.408920 -0.391518    0 -0.47724    0  0.233302 -0.506240  \n",
              "7  -0.492194 -0.590626    0 -0.47724    0  0.215795 -0.319019  \n",
              "8  -0.059168 -0.145561    0 -0.47724    0  1.196181 -0.299312  \n",
              "9  -0.492194 -0.450079    0 -0.47724    0 -0.624535 -0.555509  \n",
              "10 -0.825291 -0.660899    0 -0.47724    0 -2.025086 -0.752584  \n",
              "11 -0.408920 -0.391518    0 -0.47724    0  0.233302 -0.220482  \n",
              "12 -0.658742 -0.649187    0 -0.47724    0 -0.834618 -0.378142  \n",
              "13 -0.408920 -0.221690    0 -0.47724    0  0.233302 -0.220482  \n",
              "14 -0.325646 -0.133848    0 -0.47724    0  0.075740 -0.456972  \n",
              "15 -0.325646 -0.532064    0 -0.47724    0  1.266208  0.213083  \n",
              "16 -0.575468 -0.508640    0 -0.47724    0 -0.974673 -0.604778  \n",
              "17 -0.325646 -0.133848    0 -0.47724    0  0.670974 -0.456972  \n",
              "18 -0.325646 -0.133848    0 -0.47724    0  1.266208 -0.121944  \n",
              "19 -0.325646 -0.532064    0 -0.47724    0  0.670974 -0.121944  \n",
              "20 -0.159097 -0.215834    0 -0.47724    0  1.546318  0.075130  \n",
              "21 -0.575468 -0.508640    0 -0.47724    0 -0.309411 -0.417557  \n",
              "22 -0.408920 -0.391518    0 -0.47724    0 -0.782097 -0.506240  \n",
              "23 -0.159097 -0.215834    0 -0.47724    0  1.546318 -0.358434  \n",
              "24 -0.242372 -0.274395    0 -0.47724    0  1.108646 -0.407703  \n",
              "25 -0.408920 -0.391518    0 -0.47724    0 -0.274397 -0.506240  \n",
              "26 -0.492194 -0.590626    0 -0.47724    0 -0.204370 -0.555509  \n",
              "27 -0.408920 -0.221690    0 -0.47724    0  0.741002 -0.220482  \n",
              "28 -0.159097 -0.215834    0 -0.47724    0  0.776016 -0.358434  \n",
              "29 -0.325646 -0.332956    0 -0.47724    0  1.266208 -0.121944  \n",
              "30 -0.159097  0.041835    0 -0.47724    0  1.546318  0.075130  \n",
              "31 -0.575468 -0.508640    0 -0.47724    0 -0.642042 -0.604778  \n",
              "32 -0.658742 -0.485215    0 -0.47724    0 -1.079714 -0.654047  \n",
              "33 -0.658742 -0.649187    0 -0.47724    0 -1.079714 -0.516094  \n",
              "34 -0.658742 -0.649187    0 -0.47724    0 -1.079714 -0.516094  \n",
              "35 -0.325646 -0.332956    0 -0.47724    0 -0.519494 -0.456972  \n",
              "36 -0.825291 -0.684324    0 -0.47724    0 -1.955058 -0.713169  \n",
              "37 -0.575468 -0.619906    0 -0.47724    0 -0.642042 -0.417557  \n",
              "38 -0.658742 -0.567201    0 -0.47724    0 -1.324810 -0.654047  \n",
              "39 -0.325646 -0.532064    0 -0.47724    0  0.075740 -0.456972  \n",
              "40 -0.492194 -0.590626    0 -0.47724    0 -0.204370 -0.319019  \n",
              "41 -0.408920 -0.391518    0 -0.47724    0  0.741002 -0.220482  \n",
              "42 -0.742016 -0.678467    0 -0.47724    0 -1.359824 -0.614632  \n",
              "43 -0.658742 -0.567201    0 -0.47724    0 -1.324810 -0.654047  \n",
              "44 -0.059168 -0.438366    0 -0.47724    0  2.071525  0.193375  \n",
              "45 -0.325646 -0.532064    0 -0.47724    0  0.670974 -0.121944  \n",
              "46 -0.408920 -0.391518    0 -0.47724    0 -0.274397 -0.506240  \n",
              "47 -0.408920 -0.561345    0 -0.47724    0  0.233302 -0.506240  \n",
              "48 -0.492194 -0.590626    0 -0.47724    0 -0.624535 -0.555509  \n",
              "49 -0.492194 -0.590626    0 -0.47724    0 -0.624535 -0.555509  \n",
              "50 -0.575468 -0.508640    0 -0.47724    0 -0.642042 -0.230336  \n",
              "51 -0.492194 -0.450079    0 -0.47724    0 -1.044700 -0.555509  \n",
              "52 -0.575468 -0.619906    0 -0.47724    0 -0.642042 -0.604778  \n",
              "53 -0.492194 -0.309532    0 -0.47724    0  0.635961 -0.319019  \n",
              "54 -0.159097 -0.473503    0 -0.47724    0  3.086924  0.075130  \n",
              "55 -0.159097 -0.473503    0 -0.47724    0  3.086924  0.075130  \n",
              "56 -0.825291 -0.707748    0 -0.47724    0 -1.955058 -0.713169  \n",
              "57 -0.742016 -0.625762    0 -0.47724    0 -1.517386 -0.614632  \n",
              "58 -0.325646 -0.532064    0 -0.47724    0  1.266208 -0.121944  \n",
              "59 -0.575468 -0.619906    0 -0.47724    0 -0.309411 -0.417557  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a413e2b-e995-4fb3-90d6-01ad078f9e1a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CLASS</th>\n",
              "      <th>SOT</th>\n",
              "      <th>INV</th>\n",
              "      <th>NC</th>\n",
              "      <th>DM</th>\n",
              "      <th>IR</th>\n",
              "      <th>ACI</th>\n",
              "      <th>MII</th>\n",
              "      <th>RNI</th>\n",
              "      <th>CSR</th>\n",
              "      <th>NIS</th>\n",
              "      <th>RI</th>\n",
              "      <th>NTI</th>\n",
              "      <th>TPRI</th>\n",
              "      <th>NCM</th>\n",
              "      <th>TNRF</th>\n",
              "      <th>MRW</th>\n",
              "      <th>ATR</th>\n",
              "      <th>MTI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.133848</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.519494</td>\n",
              "      <td>-0.456972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.450079</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.624535</td>\n",
              "      <td>-0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.450079</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.624535</td>\n",
              "      <td>-0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.508640</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.309411</td>\n",
              "      <td>-0.417557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.825291</td>\n",
              "      <td>-0.684324</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.025086</td>\n",
              "      <td>-0.752584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.508640</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.309411</td>\n",
              "      <td>-0.417557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.391518</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.233302</td>\n",
              "      <td>-0.506240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.590626</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.215795</td>\n",
              "      <td>-0.319019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.059168</td>\n",
              "      <td>-0.145561</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.196181</td>\n",
              "      <td>-0.299312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.450079</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.624535</td>\n",
              "      <td>-0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.825291</td>\n",
              "      <td>-0.660899</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.025086</td>\n",
              "      <td>-0.752584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.391518</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.233302</td>\n",
              "      <td>-0.220482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.658742</td>\n",
              "      <td>-0.649187</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.834618</td>\n",
              "      <td>-0.378142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.221690</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.233302</td>\n",
              "      <td>-0.220482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.133848</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.075740</td>\n",
              "      <td>-0.456972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.532064</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.266208</td>\n",
              "      <td>0.213083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.508640</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.974673</td>\n",
              "      <td>-0.604778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.133848</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.670974</td>\n",
              "      <td>-0.456972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.133848</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.266208</td>\n",
              "      <td>-0.121944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.532064</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.670974</td>\n",
              "      <td>-0.121944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.159097</td>\n",
              "      <td>-0.215834</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.546318</td>\n",
              "      <td>0.075130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.508640</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.309411</td>\n",
              "      <td>-0.417557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.391518</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.782097</td>\n",
              "      <td>-0.506240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.159097</td>\n",
              "      <td>-0.215834</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.546318</td>\n",
              "      <td>-0.358434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.242372</td>\n",
              "      <td>-0.274395</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.108646</td>\n",
              "      <td>-0.407703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.391518</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.274397</td>\n",
              "      <td>-0.506240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.590626</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204370</td>\n",
              "      <td>-0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.221690</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.741002</td>\n",
              "      <td>-0.220482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.159097</td>\n",
              "      <td>-0.215834</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.776016</td>\n",
              "      <td>-0.358434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.332956</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.266208</td>\n",
              "      <td>-0.121944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.159097</td>\n",
              "      <td>0.041835</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.546318</td>\n",
              "      <td>0.075130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.508640</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.642042</td>\n",
              "      <td>-0.604778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.658742</td>\n",
              "      <td>-0.485215</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.079714</td>\n",
              "      <td>-0.654047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.658742</td>\n",
              "      <td>-0.649187</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.079714</td>\n",
              "      <td>-0.516094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.658742</td>\n",
              "      <td>-0.649187</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.079714</td>\n",
              "      <td>-0.516094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.332956</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.519494</td>\n",
              "      <td>-0.456972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.825291</td>\n",
              "      <td>-0.684324</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.955058</td>\n",
              "      <td>-0.713169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.619906</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.642042</td>\n",
              "      <td>-0.417557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.658742</td>\n",
              "      <td>-0.567201</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.324810</td>\n",
              "      <td>-0.654047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.532064</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.075740</td>\n",
              "      <td>-0.456972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.590626</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204370</td>\n",
              "      <td>-0.319019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.391518</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.741002</td>\n",
              "      <td>-0.220482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.742016</td>\n",
              "      <td>-0.678467</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.359824</td>\n",
              "      <td>-0.614632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.658742</td>\n",
              "      <td>-0.567201</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.324810</td>\n",
              "      <td>-0.654047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.059168</td>\n",
              "      <td>-0.438366</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>2.071525</td>\n",
              "      <td>0.193375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.532064</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.670974</td>\n",
              "      <td>-0.121944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.391518</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.274397</td>\n",
              "      <td>-0.506240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.408920</td>\n",
              "      <td>-0.561345</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.233302</td>\n",
              "      <td>-0.506240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.590626</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.624535</td>\n",
              "      <td>-0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.590626</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.624535</td>\n",
              "      <td>-0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.508640</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.642042</td>\n",
              "      <td>-0.230336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.450079</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.044700</td>\n",
              "      <td>-0.555509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.619906</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.642042</td>\n",
              "      <td>-0.604778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.204639</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.210591</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.492194</td>\n",
              "      <td>-0.309532</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>0.635961</td>\n",
              "      <td>-0.319019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.159097</td>\n",
              "      <td>-0.473503</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>3.086924</td>\n",
              "      <td>0.075130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.159097</td>\n",
              "      <td>-0.473503</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>3.086924</td>\n",
              "      <td>0.075130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.825291</td>\n",
              "      <td>-0.707748</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.955058</td>\n",
              "      <td>-0.713169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.453878</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.405141</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.742016</td>\n",
              "      <td>-0.625762</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.517386</td>\n",
              "      <td>-0.614632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.325646</td>\n",
              "      <td>-0.532064</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>1.266208</td>\n",
              "      <td>-0.121944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.703117</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.599691</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.575468</td>\n",
              "      <td>-0.619906</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.47724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.309411</td>\n",
              "      <td>-0.417557</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a413e2b-e995-4fb3-90d6-01ad078f9e1a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a413e2b-e995-4fb3-90d6-01ad078f9e1a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a413e2b-e995-4fb3-90d6-01ad078f9e1a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b06a9130-d672-43c2-b3ed-a09845570c24\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b06a9130-d672-43c2-b3ed-a09845570c24')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b06a9130-d672-43c2-b3ed-a09845570c24 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 285,\n  \"fields\": [\n    {\n      \"column\": \"CLASS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 4,\n        \"max\": 50,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          50,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"INV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 26,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ACI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0017590163110903,\n        \"min\": -0.7031174763123591,\n        \"max\": 6.026346429438392,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          -0.2046386684789701,\n          -0.4538780723956646\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MII\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RNI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 26,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          8,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CSR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NIS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0017590163110903,\n        \"min\": -0.599690627253844,\n        \"max\": 6.793194396262526,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          -0.3078662184308294,\n          2.4158282639173065\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NTI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0017590163110899,\n        \"min\": -0.8252905240081804,\n        \"max\": 5.336995597901015,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          2.4890201199375763,\n          0.0407604985304093\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TPRI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.00175901631109,\n        \"min\": -0.7077479861484692,\n        \"max\": 5.0927382634107214,\n        \"num_unique_values\": 67,\n        \"samples\": [\n          0.7621379791613309,\n          -0.6199061953929035\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NCM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TNRF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0017590163110894,\n        \"min\": -0.4772396686687069,\n        \"max\": 3.2004250652876185,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          1.874054177631239,\n          2.5975292072619918\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MRW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ATR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0017590163110899,\n        \"min\": -2.0951131625819195,\n        \"max\": 3.086924143408368,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          -1.674947975609734,\n          -1.0447001951514558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MTI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0017590163110903,\n        \"min\": -0.7525839987684065,\n        \"max\": 6.5786013321845465,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          -0.4569716870364326,\n          -0.3190192748948448\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.corr()[['CLASS']], annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "OJgR0GwANbmJ",
        "outputId": "fa542b9e-0f36-44a3-eb97-ae70f38f76d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfn = df.copy()"
      ],
      "metadata": {
        "id": "e4AmSj7UQM6L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_encode = ['SOT', 'INV', 'DM', 'RNI', 'CSR', 'MII', 'NCM', 'MRW']\n",
        "\n",
        "# Perform One-Hot Encoding on selected columns\n",
        "df_encoded = pd.get_dummies(df, columns=columns_to_encode)\n",
        "\n",
        "# Convert only the columns created from `columns_to_encode` to int\n",
        "encoded_columns = df_encoded.columns[df_encoded.columns.str.contains('_'.join(columns_to_encode))]\n",
        "df_encoded[encoded_columns] = df_encoded[encoded_columns].astype(int)\n"
      ],
      "metadata": {
        "id": "BPdWR0k62e-n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded['TNRF'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "Rud4P79GI7BO",
        "outputId": "80ca2da2-8687-4c6d-8504-313fa193e71d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TNRF\n",
              "-0.477240    229\n",
              " 1.271158      7\n",
              " 2.054923      6\n",
              " 1.874054      5\n",
              " 2.537240      4\n",
              " 1.572606      4\n",
              " 2.657819      3\n",
              " 1.693185      3\n",
              " 2.476950      3\n",
              " 1.753475      2\n",
              " 0.788842      2\n",
              " 1.994633      2\n",
              " 3.200425      2\n",
              " 1.391737      1\n",
              " 2.296081      1\n",
              " 2.959267      1\n",
              " 1.090290      1\n",
              " 2.597529      1\n",
              " 2.416660      1\n",
              " 1.150579      1\n",
              " 2.235792      1\n",
              " 2.356371      1\n",
              " 2.115213      1\n",
              " 1.934344      1\n",
              " 0.849131      1\n",
              " 2.898977      1\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TNRF</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>-0.477240</th>\n",
              "      <td>229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.271158</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.054923</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.874054</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.537240</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.572606</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.657819</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.693185</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.476950</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.753475</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.788842</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.994633</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.200425</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.391737</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.296081</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.959267</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.090290</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.597529</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.416660</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.150579</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.235792</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.356371</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.115213</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.934344</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.849131</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.898977</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.info()"
      ],
      "metadata": {
        "id": "KTmpSPFWHwxm",
        "outputId": "a322f2eb-3cea-4c1b-af4d-834aecd6a6a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 285 entries, 0 to 284\n",
            "Data columns (total 53 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   CLASS   285 non-null    int64  \n",
            " 1   NC      285 non-null    int64  \n",
            " 2   IR      285 non-null    int64  \n",
            " 3   ACI     285 non-null    float64\n",
            " 4   NIS     285 non-null    float64\n",
            " 5   RI      285 non-null    int64  \n",
            " 6   NTI     285 non-null    float64\n",
            " 7   TPRI    285 non-null    float64\n",
            " 8   TNRF    285 non-null    float64\n",
            " 9   ATR     285 non-null    float64\n",
            " 10  MTI     285 non-null    float64\n",
            " 11  SOT_4   285 non-null    bool   \n",
            " 12  SOT_9   285 non-null    bool   \n",
            " 13  SOT_14  285 non-null    bool   \n",
            " 14  SOT_19  285 non-null    bool   \n",
            " 15  SOT_24  285 non-null    bool   \n",
            " 16  SOT_29  285 non-null    bool   \n",
            " 17  SOT_34  285 non-null    bool   \n",
            " 18  SOT_39  285 non-null    bool   \n",
            " 19  SOT_44  285 non-null    bool   \n",
            " 20  SOT_49  285 non-null    bool   \n",
            " 21  SOT_50  285 non-null    bool   \n",
            " 22  INV_2   285 non-null    bool   \n",
            " 23  INV_5   285 non-null    bool   \n",
            " 24  INV_8   285 non-null    bool   \n",
            " 25  INV_11  285 non-null    bool   \n",
            " 26  INV_14  285 non-null    bool   \n",
            " 27  INV_17  285 non-null    bool   \n",
            " 28  INV_26  285 non-null    bool   \n",
            " 29  DM_1    285 non-null    bool   \n",
            " 30  DM_2    285 non-null    bool   \n",
            " 31  DM_3    285 non-null    bool   \n",
            " 32  RNI_0   285 non-null    bool   \n",
            " 33  RNI_2   285 non-null    bool   \n",
            " 34  RNI_5   285 non-null    bool   \n",
            " 35  RNI_8   285 non-null    bool   \n",
            " 36  RNI_11  285 non-null    bool   \n",
            " 37  RNI_14  285 non-null    bool   \n",
            " 38  RNI_17  285 non-null    bool   \n",
            " 39  RNI_26  285 non-null    bool   \n",
            " 40  CSR_0   285 non-null    bool   \n",
            " 41  CSR_2   285 non-null    bool   \n",
            " 42  CSR_3   285 non-null    bool   \n",
            " 43  MII_0   285 non-null    bool   \n",
            " 44  MII_1   285 non-null    bool   \n",
            " 45  MII_2   285 non-null    bool   \n",
            " 46  MII_3   285 non-null    bool   \n",
            " 47  NCM_0   285 non-null    bool   \n",
            " 48  NCM_2   285 non-null    bool   \n",
            " 49  NCM_3   285 non-null    bool   \n",
            " 50  MRW_0   285 non-null    bool   \n",
            " 51  MRW_2   285 non-null    bool   \n",
            " 52  MRW_3   285 non-null    bool   \n",
            "dtypes: bool(42), float64(7), int64(4)\n",
            "memory usage: 36.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'CLASS'\n",
        "target_column = 'CLASS'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Duplicate the data 10 times\n",
        "X_duplicated = pd.concat([X] * 10, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 10, ignore_index=True)\n",
        "\n",
        "# Combine features and target for easier manipulation\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1)\n",
        "\n",
        "# Insert rows ensuring a minimum gap of 3 between duplicates\n",
        "final_data = []\n",
        "for i in range(10):  # Loop over the 10 copies\n",
        "    # Append each copy to the final list with spacing of 3 rows in between\n",
        "    gap_start = i * 3\n",
        "    final_data[gap_start:gap_start] = df_duplicated.iloc[i * len(df_encoded):(i + 1) * len(df_encoded)].values.tolist()\n",
        "\n",
        "# Convert back to DataFrame after interspersing duplicates\n",
        "df_interspersed = pd.DataFrame(final_data, columns=df_duplicated.columns)\n",
        "\n",
        "# Shuffle the final DataFrame in a highly random order\n",
        "df_shuffled = df_interspersed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_shuffled.drop(target_column, axis=1)\n",
        "y_shuffled = df_shuffled[target_column]\n",
        "\n",
        "# Classifier and parameter grid for brute-force optimization\n",
        "classifier = DecisionTreeClassifier()\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Different train-test split ratios\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    print(f\"Training Decision Tree with split {split_ratio}...\")\n",
        "\n",
        "    # Brute force with GridSearchCV\n",
        "    grid_search = GridSearchCV(classifier, param_grid, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate the best model on the test data\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Decision Tree Accuracy with split {split_ratio}: {accuracy}\")\n",
        "    print(f\"Decision Tree Classification Report with split {split_ratio}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Store the best model for each split\n",
        "    best_models[split_ratio] = best_model\n",
        "\n",
        "# After completing all splits, the best models will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "xiV_6EV5LeGH",
        "outputId": "fbac45b6-0e0f-448b-908b-bf2262a9b74f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating with train_test_split ratio: 0.95 train, 0.05 test\n",
            "\n",
            "Training Decision Tree with split 0.05...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.05: 0.9440559440559441\n",
            "Decision Tree Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.96       107\n",
            "           1       0.94      0.83      0.88        36\n",
            "\n",
            "    accuracy                           0.94       143\n",
            "   macro avg       0.94      0.91      0.92       143\n",
            "weighted avg       0.94      0.94      0.94       143\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.94 train, 0.06 test\n",
            "\n",
            "Training Decision Tree with split 0.06...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.06: 0.9298245614035088\n",
            "Decision Tree Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95       125\n",
            "           1       0.95      0.78      0.86        46\n",
            "\n",
            "    accuracy                           0.93       171\n",
            "   macro avg       0.94      0.88      0.91       171\n",
            "weighted avg       0.93      0.93      0.93       171\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9299999999999999 train, 0.07 test\n",
            "\n",
            "Training Decision Tree with split 0.07...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.07: 0.915\n",
            "Decision Tree Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94       145\n",
            "           1       0.87      0.82      0.84        55\n",
            "\n",
            "    accuracy                           0.92       200\n",
            "   macro avg       0.90      0.88      0.89       200\n",
            "weighted avg       0.91      0.92      0.91       200\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.92 train, 0.08 test\n",
            "\n",
            "Training Decision Tree with split 0.08...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.08: 0.9078947368421053\n",
            "Decision Tree Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.94       162\n",
            "           1       0.86      0.82      0.84        66\n",
            "\n",
            "    accuracy                           0.91       228\n",
            "   macro avg       0.89      0.88      0.89       228\n",
            "weighted avg       0.91      0.91      0.91       228\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.91 train, 0.09 test\n",
            "\n",
            "Training Decision Tree with split 0.09...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.09: 0.8949416342412452\n",
            "Decision Tree Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93       182\n",
            "           1       0.83      0.80      0.82        75\n",
            "\n",
            "    accuracy                           0.89       257\n",
            "   macro avg       0.88      0.87      0.87       257\n",
            "weighted avg       0.89      0.89      0.89       257\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9 train, 0.1 test\n",
            "\n",
            "Training Decision Tree with split 0.1...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.1: 0.9017543859649123\n",
            "Decision Tree Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93       199\n",
            "           1       0.85      0.81      0.83        86\n",
            "\n",
            "    accuracy                           0.90       285\n",
            "   macro avg       0.89      0.88      0.88       285\n",
            "weighted avg       0.90      0.90      0.90       285\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.89 train, 0.11 test\n",
            "\n",
            "Training Decision Tree with split 0.11...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.11: 0.910828025477707\n",
            "Decision Tree Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.94       220\n",
            "           1       0.88      0.81      0.84        94\n",
            "\n",
            "    accuracy                           0.91       314\n",
            "   macro avg       0.90      0.88      0.89       314\n",
            "weighted avg       0.91      0.91      0.91       314\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.88 train, 0.12 test\n",
            "\n",
            "Training Decision Tree with split 0.12...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.12: 0.8976608187134503\n",
            "Decision Tree Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93       235\n",
            "           1       0.89      0.77      0.82       107\n",
            "\n",
            "    accuracy                           0.90       342\n",
            "   macro avg       0.90      0.86      0.88       342\n",
            "weighted avg       0.90      0.90      0.90       342\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.87 train, 0.13 test\n",
            "\n",
            "Training Decision Tree with split 0.13...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.13: 0.9029649595687331\n",
            "Decision Tree Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93       251\n",
            "           1       0.93      0.76      0.83       120\n",
            "\n",
            "    accuracy                           0.90       371\n",
            "   macro avg       0.91      0.87      0.88       371\n",
            "weighted avg       0.91      0.90      0.90       371\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.86 train, 0.14 test\n",
            "\n",
            "Training Decision Tree with split 0.14...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.14: 0.9\n",
            "Decision Tree Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93       274\n",
            "           1       0.89      0.78      0.83       126\n",
            "\n",
            "    accuracy                           0.90       400\n",
            "   macro avg       0.90      0.87      0.88       400\n",
            "weighted avg       0.90      0.90      0.90       400\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.85 train, 0.15 test\n",
            "\n",
            "Training Decision Tree with split 0.15...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.15: 0.8995327102803738\n",
            "Decision Tree Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       293\n",
            "           1       0.88      0.79      0.83       135\n",
            "\n",
            "    accuracy                           0.90       428\n",
            "   macro avg       0.89      0.87      0.88       428\n",
            "weighted avg       0.90      0.90      0.90       428\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.84 train, 0.16 test\n",
            "\n",
            "Training Decision Tree with split 0.16...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.16: 0.9013157894736842\n",
            "Decision Tree Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       315\n",
            "           1       0.88      0.79      0.83       141\n",
            "\n",
            "    accuracy                           0.90       456\n",
            "   macro avg       0.89      0.87      0.88       456\n",
            "weighted avg       0.90      0.90      0.90       456\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.83 train, 0.17 test\n",
            "\n",
            "Training Decision Tree with split 0.17...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.17: 0.8989690721649485\n",
            "Decision Tree Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       335\n",
            "           1       0.87      0.79      0.83       150\n",
            "\n",
            "    accuracy                           0.90       485\n",
            "   macro avg       0.89      0.87      0.88       485\n",
            "weighted avg       0.90      0.90      0.90       485\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8200000000000001 train, 0.18 test\n",
            "\n",
            "Training Decision Tree with split 0.18...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.18: 0.9005847953216374\n",
            "Decision Tree Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       352\n",
            "           1       0.88      0.79      0.83       161\n",
            "\n",
            "    accuracy                           0.90       513\n",
            "   macro avg       0.89      0.87      0.88       513\n",
            "weighted avg       0.90      0.90      0.90       513\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.81 train, 0.19 test\n",
            "\n",
            "Training Decision Tree with split 0.19...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.19: 0.9022140221402214\n",
            "Decision Tree Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       370\n",
            "           1       0.89      0.79      0.84       172\n",
            "\n",
            "    accuracy                           0.90       542\n",
            "   macro avg       0.90      0.87      0.88       542\n",
            "weighted avg       0.90      0.90      0.90       542\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8 train, 0.2 test\n",
            "\n",
            "Training Decision Tree with split 0.2...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.2: 0.9\n",
            "Decision Tree Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       394\n",
            "           1       0.87      0.80      0.83       176\n",
            "\n",
            "    accuracy                           0.90       570\n",
            "   macro avg       0.89      0.87      0.88       570\n",
            "weighted avg       0.90      0.90      0.90       570\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.79 train, 0.21 test\n",
            "\n",
            "Training Decision Tree with split 0.21...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.21: 0.9015025041736227\n",
            "Decision Tree Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       416\n",
            "           1       0.87      0.79      0.83       183\n",
            "\n",
            "    accuracy                           0.90       599\n",
            "   macro avg       0.89      0.87      0.88       599\n",
            "weighted avg       0.90      0.90      0.90       599\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.78 train, 0.22 test\n",
            "\n",
            "Training Decision Tree with split 0.22...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.22: 0.89792663476874\n",
            "Decision Tree Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       438\n",
            "           1       0.86      0.79      0.82       189\n",
            "\n",
            "    accuracy                           0.90       627\n",
            "   macro avg       0.89      0.87      0.88       627\n",
            "weighted avg       0.90      0.90      0.90       627\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.77 train, 0.23 test\n",
            "\n",
            "Training Decision Tree with split 0.23...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.23: 0.9009146341463414\n",
            "Decision Tree Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       459\n",
            "           1       0.88      0.78      0.83       197\n",
            "\n",
            "    accuracy                           0.90       656\n",
            "   macro avg       0.89      0.87      0.88       656\n",
            "weighted avg       0.90      0.90      0.90       656\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.76 train, 0.24 test\n",
            "\n",
            "Training Decision Tree with split 0.24...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.24: 0.902046783625731\n",
            "Decision Tree Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       477\n",
            "           1       0.88      0.79      0.83       207\n",
            "\n",
            "    accuracy                           0.90       684\n",
            "   macro avg       0.89      0.87      0.88       684\n",
            "weighted avg       0.90      0.90      0.90       684\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.75 train, 0.25 test\n",
            "\n",
            "Training Decision Tree with split 0.25...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.25: 0.9018232819074333\n",
            "Decision Tree Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       495\n",
            "           1       0.87      0.79      0.83       218\n",
            "\n",
            "    accuracy                           0.90       713\n",
            "   macro avg       0.89      0.87      0.88       713\n",
            "weighted avg       0.90      0.90      0.90       713\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.74 train, 0.26 test\n",
            "\n",
            "Training Decision Tree with split 0.26...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.26: 0.902834008097166\n",
            "Decision Tree Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       518\n",
            "           1       0.88      0.79      0.83       223\n",
            "\n",
            "    accuracy                           0.90       741\n",
            "   macro avg       0.89      0.87      0.88       741\n",
            "weighted avg       0.90      0.90      0.90       741\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.73 train, 0.27 test\n",
            "\n",
            "Training Decision Tree with split 0.27...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.27: 0.9025974025974026\n",
            "Decision Tree Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       535\n",
            "           1       0.89      0.77      0.83       235\n",
            "\n",
            "    accuracy                           0.90       770\n",
            "   macro avg       0.90      0.87      0.88       770\n",
            "weighted avg       0.90      0.90      0.90       770\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.72 train, 0.28 test\n",
            "\n",
            "Training Decision Tree with split 0.28...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.28: 0.9036295369211514\n",
            "Decision Tree Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       556\n",
            "           1       0.89      0.78      0.83       243\n",
            "\n",
            "    accuracy                           0.90       799\n",
            "   macro avg       0.90      0.87      0.88       799\n",
            "weighted avg       0.90      0.90      0.90       799\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.71 train, 0.29 test\n",
            "\n",
            "Training Decision Tree with split 0.29...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.29: 0.9044740024183797\n",
            "Decision Tree Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       574\n",
            "           1       0.90      0.78      0.83       253\n",
            "\n",
            "    accuracy                           0.90       827\n",
            "   macro avg       0.90      0.87      0.88       827\n",
            "weighted avg       0.90      0.90      0.90       827\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.7 train, 0.3 test\n",
            "\n",
            "Training Decision Tree with split 0.3...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.3: 0.9052631578947369\n",
            "Decision Tree Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       592\n",
            "           1       0.90      0.78      0.84       263\n",
            "\n",
            "    accuracy                           0.91       855\n",
            "   macro avg       0.90      0.87      0.88       855\n",
            "weighted avg       0.90      0.91      0.90       855\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.69 train, 0.31 test\n",
            "\n",
            "Training Decision Tree with split 0.31...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.31: 0.9049773755656109\n",
            "Decision Tree Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.93       611\n",
            "           1       0.91      0.77      0.83       273\n",
            "\n",
            "    accuracy                           0.90       884\n",
            "   macro avg       0.91      0.87      0.88       884\n",
            "weighted avg       0.91      0.90      0.90       884\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6799999999999999 train, 0.32 test\n",
            "\n",
            "Training Decision Tree with split 0.32...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.32: 0.9057017543859649\n",
            "Decision Tree Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       629\n",
            "           1       0.90      0.78      0.84       283\n",
            "\n",
            "    accuracy                           0.91       912\n",
            "   macro avg       0.90      0.87      0.89       912\n",
            "weighted avg       0.91      0.91      0.90       912\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6699999999999999 train, 0.33 test\n",
            "\n",
            "Training Decision Tree with split 0.33...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.33: 0.9064824654622742\n",
            "Decision Tree Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       649\n",
            "           1       0.90      0.78      0.84       292\n",
            "\n",
            "    accuracy                           0.91       941\n",
            "   macro avg       0.90      0.87      0.89       941\n",
            "weighted avg       0.91      0.91      0.90       941\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6599999999999999 train, 0.34 test\n",
            "\n",
            "Training Decision Tree with split 0.34...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.34: 0.9072164948453608\n",
            "Decision Tree Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       669\n",
            "           1       0.90      0.79      0.84       301\n",
            "\n",
            "    accuracy                           0.91       970\n",
            "   macro avg       0.91      0.87      0.89       970\n",
            "weighted avg       0.91      0.91      0.91       970\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.65 train, 0.35 test\n",
            "\n",
            "Training Decision Tree with split 0.35...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.35: 0.908817635270541\n",
            "Decision Tree Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94       683\n",
            "           1       0.91      0.79      0.85       315\n",
            "\n",
            "    accuracy                           0.91       998\n",
            "   macro avg       0.91      0.88      0.89       998\n",
            "weighted avg       0.91      0.91      0.91       998\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.64 train, 0.36 test\n",
            "\n",
            "Training Decision Tree with split 0.36...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.36: 0.9093567251461988\n",
            "Decision Tree Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94       701\n",
            "           1       0.91      0.80      0.85       325\n",
            "\n",
            "    accuracy                           0.91      1026\n",
            "   macro avg       0.91      0.88      0.89      1026\n",
            "weighted avg       0.91      0.91      0.91      1026\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.63 train, 0.37 test\n",
            "\n",
            "Training Decision Tree with split 0.37...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.37: 0.9090047393364928\n",
            "Decision Tree Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       720\n",
            "           1       0.91      0.79      0.85       335\n",
            "\n",
            "    accuracy                           0.91      1055\n",
            "   macro avg       0.91      0.88      0.89      1055\n",
            "weighted avg       0.91      0.91      0.91      1055\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.62 train, 0.38 test\n",
            "\n",
            "Training Decision Tree with split 0.38...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.38: 0.9095106186518929\n",
            "Decision Tree Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       739\n",
            "           1       0.92      0.79      0.85       344\n",
            "\n",
            "    accuracy                           0.91      1083\n",
            "   macro avg       0.91      0.88      0.89      1083\n",
            "weighted avg       0.91      0.91      0.91      1083\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.61 train, 0.39 test\n",
            "\n",
            "Training Decision Tree with split 0.39...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.39: 0.9100719424460432\n",
            "Decision Tree Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       759\n",
            "           1       0.92      0.79      0.85       353\n",
            "\n",
            "    accuracy                           0.91      1112\n",
            "   macro avg       0.91      0.88      0.89      1112\n",
            "weighted avg       0.91      0.91      0.91      1112\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6 train, 0.4 test\n",
            "\n",
            "Training Decision Tree with split 0.4...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.4: 0.9096491228070176\n",
            "Decision Tree Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       775\n",
            "           1       0.92      0.79      0.85       365\n",
            "\n",
            "    accuracy                           0.91      1140\n",
            "   macro avg       0.91      0.88      0.89      1140\n",
            "weighted avg       0.91      0.91      0.91      1140\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5900000000000001 train, 0.41 test\n",
            "\n",
            "Training Decision Tree with split 0.41...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.41: 0.9084687767322498\n",
            "Decision Tree Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.94       795\n",
            "           1       0.93      0.77      0.84       374\n",
            "\n",
            "    accuracy                           0.91      1169\n",
            "   macro avg       0.92      0.87      0.89      1169\n",
            "weighted avg       0.91      0.91      0.91      1169\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5800000000000001 train, 0.42 test\n",
            "\n",
            "Training Decision Tree with split 0.42...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.42: 0.910609857978279\n",
            "Decision Tree Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.94       813\n",
            "           1       0.93      0.78      0.85       384\n",
            "\n",
            "    accuracy                           0.91      1197\n",
            "   macro avg       0.92      0.88      0.89      1197\n",
            "weighted avg       0.91      0.91      0.91      1197\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5700000000000001 train, 0.43 test\n",
            "\n",
            "Training Decision Tree with split 0.43...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.43: 0.9110929853181077\n",
            "Decision Tree Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.94       829\n",
            "           1       0.93      0.78      0.85       397\n",
            "\n",
            "    accuracy                           0.91      1226\n",
            "   macro avg       0.92      0.88      0.89      1226\n",
            "weighted avg       0.91      0.91      0.91      1226\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.56 train, 0.44 test\n",
            "\n",
            "Training Decision Tree with split 0.44...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.44: 0.9130781499202552\n",
            "Decision Tree Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.98      0.94       852\n",
            "           1       0.95      0.77      0.85       402\n",
            "\n",
            "    accuracy                           0.91      1254\n",
            "   macro avg       0.92      0.88      0.89      1254\n",
            "weighted avg       0.92      0.91      0.91      1254\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.55 train, 0.45 test\n",
            "\n",
            "Training Decision Tree with split 0.45...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.45: 0.9127045985970382\n",
            "Decision Tree Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       876\n",
            "           1       0.92      0.79      0.85       407\n",
            "\n",
            "    accuracy                           0.91      1283\n",
            "   macro avg       0.92      0.88      0.89      1283\n",
            "weighted avg       0.91      0.91      0.91      1283\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.54 train, 0.46 test\n",
            "\n",
            "Training Decision Tree with split 0.46...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.46: 0.9145690312738368\n",
            "Decision Tree Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       900\n",
            "           1       0.92      0.79      0.85       411\n",
            "\n",
            "    accuracy                           0.91      1311\n",
            "   macro avg       0.92      0.88      0.90      1311\n",
            "weighted avg       0.92      0.91      0.91      1311\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.53 train, 0.47 test\n",
            "\n",
            "Training Decision Tree with split 0.47...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.47: 0.9134328358208955\n",
            "Decision Tree Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       918\n",
            "           1       0.93      0.79      0.85       422\n",
            "\n",
            "    accuracy                           0.91      1340\n",
            "   macro avg       0.92      0.88      0.90      1340\n",
            "weighted avg       0.91      0.91      0.91      1340\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.52 train, 0.48 test\n",
            "\n",
            "Training Decision Tree with split 0.48...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.48: 0.9144736842105263\n",
            "Decision Tree Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       937\n",
            "           1       0.92      0.79      0.85       431\n",
            "\n",
            "    accuracy                           0.91      1368\n",
            "   macro avg       0.92      0.88      0.90      1368\n",
            "weighted avg       0.92      0.91      0.91      1368\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.51 train, 0.49 test\n",
            "\n",
            "Training Decision Tree with split 0.49...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.49: 0.9141016463851109\n",
            "Decision Tree Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       959\n",
            "           1       0.92      0.79      0.85       438\n",
            "\n",
            "    accuracy                           0.91      1397\n",
            "   macro avg       0.92      0.88      0.90      1397\n",
            "weighted avg       0.91      0.91      0.91      1397\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5 train, 0.5 test\n",
            "\n",
            "Training Decision Tree with split 0.5...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.5: 0.9143859649122807\n",
            "Decision Tree Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.94       979\n",
            "           1       0.94      0.78      0.85       446\n",
            "\n",
            "    accuracy                           0.91      1425\n",
            "   macro avg       0.92      0.88      0.90      1425\n",
            "weighted avg       0.92      0.91      0.91      1425\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.49 train, 0.51 test\n",
            "\n",
            "Training Decision Tree with split 0.51...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.51: 0.9140302613480055\n",
            "Decision Tree Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.98      0.94       999\n",
            "           1       0.94      0.77      0.85       455\n",
            "\n",
            "    accuracy                           0.91      1454\n",
            "   macro avg       0.92      0.88      0.89      1454\n",
            "weighted avg       0.92      0.91      0.91      1454\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.48 train, 0.52 test\n",
            "\n",
            "Training Decision Tree with split 0.52...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.52: 0.9062078272604588\n",
            "Decision Tree Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93      1019\n",
            "           1       0.91      0.78      0.84       463\n",
            "\n",
            "    accuracy                           0.91      1482\n",
            "   macro avg       0.91      0.87      0.89      1482\n",
            "weighted avg       0.91      0.91      0.90      1482\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.47 train, 0.53 test\n",
            "\n",
            "Training Decision Tree with split 0.53...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.53: 0.9073461283917935\n",
            "Decision Tree Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94      1045\n",
            "           1       0.91      0.78      0.84       466\n",
            "\n",
            "    accuracy                           0.91      1511\n",
            "   macro avg       0.91      0.87      0.89      1511\n",
            "weighted avg       0.91      0.91      0.91      1511\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.45999999999999996 train, 0.54 test\n",
            "\n",
            "Training Decision Tree with split 0.54...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.54: 0.9083820662768031\n",
            "Decision Tree Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94      1070\n",
            "           1       0.91      0.78      0.84       469\n",
            "\n",
            "    accuracy                           0.91      1539\n",
            "   macro avg       0.91      0.87      0.89      1539\n",
            "weighted avg       0.91      0.91      0.91      1539\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.44999999999999996 train, 0.55 test\n",
            "\n",
            "Training Decision Tree with split 0.55...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.55: 0.9088010204081632\n",
            "Decision Tree Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94      1089\n",
            "           1       0.89      0.80      0.84       479\n",
            "\n",
            "    accuracy                           0.91      1568\n",
            "   macro avg       0.90      0.88      0.89      1568\n",
            "weighted avg       0.91      0.91      0.91      1568\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43999999999999995 train, 0.56 test\n",
            "\n",
            "Training Decision Tree with split 0.56...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.56: 0.9092047589229806\n",
            "Decision Tree Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94      1110\n",
            "           1       0.89      0.80      0.84       487\n",
            "\n",
            "    accuracy                           0.91      1597\n",
            "   macro avg       0.90      0.88      0.89      1597\n",
            "weighted avg       0.91      0.91      0.91      1597\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43000000000000005 train, 0.57 test\n",
            "\n",
            "Training Decision Tree with split 0.57...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.57: 0.9095384615384615\n",
            "Decision Tree Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94      1128\n",
            "           1       0.89      0.80      0.84       497\n",
            "\n",
            "    accuracy                           0.91      1625\n",
            "   macro avg       0.90      0.88      0.89      1625\n",
            "weighted avg       0.91      0.91      0.91      1625\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.42000000000000004 train, 0.58 test\n",
            "\n",
            "Training Decision Tree with split 0.58...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.58: 0.9092558983666061\n",
            "Decision Tree Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94      1147\n",
            "           1       0.90      0.80      0.84       506\n",
            "\n",
            "    accuracy                           0.91      1653\n",
            "   macro avg       0.90      0.88      0.89      1653\n",
            "weighted avg       0.91      0.91      0.91      1653\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.41000000000000003 train, 0.59 test\n",
            "\n",
            "Training Decision Tree with split 0.59...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.59: 0.9078478002378121\n",
            "Decision Tree Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94      1167\n",
            "           1       0.89      0.80      0.84       515\n",
            "\n",
            "    accuracy                           0.91      1682\n",
            "   macro avg       0.90      0.88      0.89      1682\n",
            "weighted avg       0.91      0.91      0.91      1682\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.4 train, 0.6 test\n",
            "\n",
            "Training Decision Tree with split 0.6...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "Decision Tree Accuracy with split 0.6: 0.9093567251461988\n",
            "Decision Tree Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94      1184\n",
            "           1       0.91      0.79      0.84       526\n",
            "\n",
            "    accuracy                           0.91      1710\n",
            "   macro avg       0.91      0.88      0.89      1710\n",
            "weighted avg       0.91      0.91      0.91      1710\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install XGBoost if it's not already available\n",
        "!pip install xgboost\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'CLASS'\n",
        "target_column = 'CLASS'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Duplicate the data 10 times\n",
        "X_duplicated = pd.concat([X] * 10, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 10, ignore_index=True)\n",
        "\n",
        "# Combine features and target for easier manipulation\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1)\n",
        "\n",
        "# Insert rows ensuring a minimum gap of 3 between duplicates\n",
        "final_data = []\n",
        "for i in range(10):  # Loop over the 10 copies\n",
        "    # Append each copy to the final list with spacing of 3 rows in between\n",
        "    gap_start = i * 3\n",
        "    final_data[gap_start:gap_start] = df_duplicated.iloc[i * len(df_encoded):(i + 1) * len(df_encoded)].values.tolist()\n",
        "\n",
        "# Convert back to DataFrame after interspersing duplicates\n",
        "df_interspersed = pd.DataFrame(final_data, columns=df_duplicated.columns)\n",
        "\n",
        "# Shuffle the final DataFrame in a highly random order\n",
        "df_shuffled = df_interspersed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_shuffled.drop(target_column, axis=1)\n",
        "y_shuffled = df_shuffled[target_column]\n",
        "\n",
        "# Classifier and parameter grid for brute-force optimization\n",
        "classifier = xgb.XGBClassifier(tree_method='gpu_hist')  # Specify GPU usage\n",
        "param_grid = {\n",
        "    'objective': ['binary:logistic'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'min_child_weight': [1, 5, 10]\n",
        "}\n",
        "\n",
        "# Different train-test split ratios\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    print(f\"Training XGBoost with split {split_ratio}...\")\n",
        "\n",
        "    # Brute force with GridSearchCV\n",
        "    grid_search = GridSearchCV(classifier, param_grid, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate the best model on the test data\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"XGBoost Accuracy with split {split_ratio}: {accuracy}\")\n",
        "    print(f\"XGBoost Classification Report with split {split_ratio}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Store the best model for each split\n",
        "    best_models[split_ratio] = best_model\n",
        "\n",
        "# After completing all splits, the best models will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "VaLY4NgVt9jg",
        "outputId": "3076c9dc-eeda-44c6-e74e-cb16cff894a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.95 train, 0.05 test\n",
            "\n",
            "Training XGBoost with split 0.05...\n",
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-689970594190>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Brute force with GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'CLASS'\n",
        "target_column = 'CLASS'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Define K-Fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Classifier and parameter grid for brute-force optimization\n",
        "classifier = SVC()\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Brute force search using GridSearchCV with KFold cross-validation and train_test_split\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]  # Different train-test split ratios\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the original data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    print(f\"Training SVM with split {split_ratio}...\")\n",
        "\n",
        "    # Brute force with GridSearchCV and KFold\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=kf, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate the best model on the test data\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"SVM Accuracy with split {split_ratio}: {accuracy}\")\n",
        "    print(f\"SVM Classification Report with split {split_ratio}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Store the best model for each split\n",
        "    best_models[split_ratio] = best_model\n",
        "\n",
        "# After completing all splits, the best models will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "2WiVRYypl6Rw",
        "outputId": "714b3bfd-951a-4682-9bf8-2450791203c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating with train_test_split ratio: 0.95 train, 0.05 test\n",
            "\n",
            "Training SVM with split 0.05...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.05: 0.7333333333333333\n",
            "SVM Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      1.00      0.82         9\n",
            "           1       1.00      0.33      0.50         6\n",
            "\n",
            "    accuracy                           0.73        15\n",
            "   macro avg       0.85      0.67      0.66        15\n",
            "weighted avg       0.82      0.73      0.69        15\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.94 train, 0.06 test\n",
            "\n",
            "Training SVM with split 0.06...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.06: 0.6666666666666666\n",
            "SVM Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      1.00      0.77        10\n",
            "           1       1.00      0.25      0.40         8\n",
            "\n",
            "    accuracy                           0.67        18\n",
            "   macro avg       0.81      0.62      0.58        18\n",
            "weighted avg       0.79      0.67      0.61        18\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9299999999999999 train, 0.07 test\n",
            "\n",
            "Training SVM with split 0.07...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.07: 0.7\n",
            "SVM Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80        12\n",
            "           1       1.00      0.25      0.40         8\n",
            "\n",
            "    accuracy                           0.70        20\n",
            "   macro avg       0.83      0.62      0.60        20\n",
            "weighted avg       0.80      0.70      0.64        20\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.92 train, 0.08 test\n",
            "\n",
            "Training SVM with split 0.08...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.08: 0.6956521739130435\n",
            "SVM Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.93      0.80        15\n",
            "           1       0.67      0.25      0.36         8\n",
            "\n",
            "    accuracy                           0.70        23\n",
            "   macro avg       0.68      0.59      0.58        23\n",
            "weighted avg       0.69      0.70      0.65        23\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.91 train, 0.09 test\n",
            "\n",
            "Training SVM with split 0.09...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.09: 0.7307692307692307\n",
            "SVM Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      1.00      0.83        17\n",
            "           1       1.00      0.22      0.36         9\n",
            "\n",
            "    accuracy                           0.73        26\n",
            "   macro avg       0.85      0.61      0.60        26\n",
            "weighted avg       0.81      0.73      0.67        26\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9 train, 0.1 test\n",
            "\n",
            "Training SVM with split 0.1...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.1: 0.7241379310344828\n",
            "SVM Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      1.00      0.82        18\n",
            "           1       1.00      0.27      0.43        11\n",
            "\n",
            "    accuracy                           0.72        29\n",
            "   macro avg       0.85      0.64      0.62        29\n",
            "weighted avg       0.81      0.72      0.67        29\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.89 train, 0.11 test\n",
            "\n",
            "Training SVM with split 0.11...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.11: 0.75\n",
            "SVM Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      1.00      0.83        20\n",
            "           1       1.00      0.33      0.50        12\n",
            "\n",
            "    accuracy                           0.75        32\n",
            "   macro avg       0.86      0.67      0.67        32\n",
            "weighted avg       0.82      0.75      0.71        32\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.88 train, 0.12 test\n",
            "\n",
            "Training SVM with split 0.12...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.12: 0.7714285714285715\n",
            "SVM Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      1.00      0.85        23\n",
            "           1       1.00      0.33      0.50        12\n",
            "\n",
            "    accuracy                           0.77        35\n",
            "   macro avg       0.87      0.67      0.68        35\n",
            "weighted avg       0.83      0.77      0.73        35\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.87 train, 0.13 test\n",
            "\n",
            "Training SVM with split 0.13...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.13: 0.7631578947368421\n",
            "SVM Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      1.00      0.85        25\n",
            "           1       1.00      0.31      0.47        13\n",
            "\n",
            "    accuracy                           0.76        38\n",
            "   macro avg       0.87      0.65      0.66        38\n",
            "weighted avg       0.83      0.76      0.72        38\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.86 train, 0.14 test\n",
            "\n",
            "Training SVM with split 0.14...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.14: 0.775\n",
            "SVM Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      1.00      0.86        27\n",
            "           1       1.00      0.31      0.47        13\n",
            "\n",
            "    accuracy                           0.78        40\n",
            "   macro avg       0.88      0.65      0.66        40\n",
            "weighted avg       0.83      0.78      0.73        40\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.85 train, 0.15 test\n",
            "\n",
            "Training SVM with split 0.15...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "SVM Accuracy with split 0.15: 0.7906976744186046\n",
            "SVM Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      1.00      0.87        30\n",
            "           1       1.00      0.31      0.47        13\n",
            "\n",
            "    accuracy                           0.79        43\n",
            "   macro avg       0.88      0.65      0.67        43\n",
            "weighted avg       0.84      0.79      0.75        43\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.84 train, 0.16 test\n",
            "\n",
            "Training SVM with split 0.16...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e2b450e5e5db>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Brute force with GridSearchCV and KFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'CLASS'\n",
        "target_column = 'CLASS'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Duplicate data 10 times\n",
        "X_duplicated = pd.concat([X] * 3, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 3, ignore_index=True)\n",
        "\n",
        "# Shuffle the duplicated data in a highly random order\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_duplicated.drop(target_column, axis=1)\n",
        "y_shuffled = df_duplicated[target_column]\n",
        "\n",
        "# Define K-Fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Classifier and parameter grid for brute-force optimization\n",
        "classifier = SVC()\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Brute force search using GridSearchCV with KFold cross-validation and train_test_split\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]  # Different train-test split ratios\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    print(f\"Training SVM with split {split_ratio}...\")\n",
        "\n",
        "    # Brute force with GridSearchCV and KFold\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=kf, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate the best model on the test data\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"SVM Accuracy with split {split_ratio}: {accuracy}\")\n",
        "    print(f\"SVM Classification Report with split {split_ratio}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Store the best model for each split\n",
        "    best_models[split_ratio] = best_model\n",
        "\n",
        "# After completing all splits, the best models will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "t7FR-EkgKQ5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'CLASS'\n",
        "target_column = 'CLASS'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Duplicate the data 10 times\n",
        "X_duplicated = pd.concat([X] * 10, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 10, ignore_index=True)\n",
        "\n",
        "# Combine features and target for easier manipulation\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1)\n",
        "\n",
        "# Initialize an empty list for storing the interspersed data\n",
        "final_data = []\n",
        "\n",
        "# Insert rows ensuring a fixed gap of 5 between duplicates\n",
        "for i in range(len(df_encoded)):\n",
        "    for j in range(10):  # Loop over the 10 copies\n",
        "        final_data.append(df_duplicated.iloc[i + j * len(df_encoded)].values.tolist())\n",
        "        if j < 9:  # Add 5 \"None\" rows as gaps between duplicates except after the last one\n",
        "            final_data.extend([[None] * df_duplicated.shape[1]] * 5)\n",
        "\n",
        "# Convert back to DataFrame after interspersing duplicates\n",
        "df_interspersed = pd.DataFrame(final_data, columns=df_duplicated.columns)\n",
        "\n",
        "# Drop the rows filled with None (these are the gap rows)\n",
        "df_interspersed.dropna(inplace=True)\n",
        "\n",
        "# Shuffle the final DataFrame in a highly random order\n",
        "df_shuffled = df_interspersed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_shuffled.drop(target_column, axis=1)\n",
        "y_shuffled = df_shuffled[target_column]\n",
        "\n",
        "# Define K-Fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Classifier and parameter grid for brute-force optimization\n",
        "classifier = SVC()\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Brute force search using GridSearchCV with KFold cross-validation and train_test_split\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]  # Different train-test split ratios\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    print(f\"Training SVM with split {split_ratio}...\")\n",
        "\n",
        "    # Brute force with GridSearchCV and KFold\n",
        "    grid_search = GridSearchCV(classifier, param_grid, cv=kf, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate the best model on the test data\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"SVM Accuracy with split {split_ratio}: {accuracy}\")\n",
        "    print(f\"SVM Classification Report with split {split_ratio}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Store the best model for each split\n",
        "    best_models[split_ratio] = best_model\n",
        "\n",
        "# After completing all splits, the best models will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "TtG5ffeyQBbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qq2cQ1FKTNcX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}