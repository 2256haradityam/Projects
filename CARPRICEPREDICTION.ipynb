{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN5Dr5A1Kja6qtZvhQcH0ZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2256haradityam/Projects/blob/main/CARPRICEPREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4m3-2g621mK6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://github.com/2256haradityam/dataset/raw/refs/heads/main/bcn.csv')"
      ],
      "metadata": {
        "id": "mo5jHXYL2YWI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4tJ5fwpb2b7f",
        "outputId": "86f94659-0f82-4a92-cb5b-7a421f1e675d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   class  age  menopause  tumor-size  inv-nodes  node-caps  deg-malig  breast  \\\n",
              "0      0   39          1          34          2          0          3       1   \n",
              "1      0   49          1          24          2          0          2       2   \n",
              "2      0   49          1          24          2          0          2       1   \n",
              "3      0   69          2          19          2          0          2       2   \n",
              "4      0   49          1           4          2          0          2       2   \n",
              "\n",
              "   breast-quad  irradiat  \n",
              "0            1         0  \n",
              "1            3         0  \n",
              "2            1         0  \n",
              "3            2         0  \n",
              "4            4         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b135c898-fca7-4ae4-a0bc-1e3bae05da8b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>age</th>\n",
              "      <th>menopause</th>\n",
              "      <th>tumor-size</th>\n",
              "      <th>inv-nodes</th>\n",
              "      <th>node-caps</th>\n",
              "      <th>deg-malig</th>\n",
              "      <th>breast</th>\n",
              "      <th>breast-quad</th>\n",
              "      <th>irradiat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>69</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b135c898-fca7-4ae4-a0bc-1e3bae05da8b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b135c898-fca7-4ae4-a0bc-1e3bae05da8b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b135c898-fca7-4ae4-a0bc-1e3bae05da8b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-90cdb14a-5862-4fa9-ad9c-27754ef9e70c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90cdb14a-5862-4fa9-ad9c-27754ef9e70c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-90cdb14a-5862-4fa9-ad9c-27754ef9e70c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 285,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 29,\n        \"max\": 79,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          39,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"menopause\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tumor-size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 4,\n        \"max\": 54,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          54,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inv-nodes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 26,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"node-caps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deg-malig\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"breast\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"breast-quad\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"irradiat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfn = df.copy()"
      ],
      "metadata": {
        "id": "e4AmSj7UQM6L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfn['breast'].value_counts()"
      ],
      "metadata": {
        "id": "iLrvjQjbHoD0",
        "outputId": "69e366f4-416d-4d25-bab6-c21a5bc0b7f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "breast\n",
              "1    151\n",
              "2    134\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breast</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfn['breast'] = dfn['breast'].map({1: 0, 2: 1})"
      ],
      "metadata": {
        "id": "Av3CIQTsHTMl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_encode = ['age', 'inv-nodes', 'deg-malig', 'tumor-size', 'irradiat',  'breast-quad', 'menopause']\n",
        "\n",
        "# Perform One-Hot Encoding on selected columns\n",
        "df_encoded = pd.get_dummies(dfn, columns=columns_to_encode)\n",
        "\n",
        "# Convert only the columns created from `columns_to_encode` to int\n",
        "encoded_columns = df_encoded.columns[df_encoded.columns.str.contains('_'.join(columns_to_encode))]\n",
        "df_encoded[encoded_columns] = df_encoded[encoded_columns].astype(int)\n"
      ],
      "metadata": {
        "id": "BPdWR0k62e-n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Define ranges for numerator and denominator power ratios\n",
        "numerator_powers = range(1, 20)\n",
        "denominator_powers = range(1, 20)\n",
        "\n",
        "# Initialize variables to store the best power ratios and correlation\n",
        "best_numerator_power = 0\n",
        "best_denominator_power = 0\n",
        "best_correlation = -1\n",
        "\n",
        "# Iterate through different power ratios\n",
        "for numerator_power in numerator_powers:\n",
        "    for denominator_power in denominator_powers:\n",
        "        dfn['new_column'] = ((dfn['tumor-size'] * dfn['inv-nodes'] * dfn['deg-malig'])**numerator_power)/((dfn['age']*dfn['menopause'])**denominator_power)\n",
        "        correlation = pearsonr(dfn['new_column'], dfn['class'])[0]\n",
        "        if correlation > best_correlation:\n",
        "            best_numerator_power = numerator_power\n",
        "            best_denominator_power = denominator_power\n",
        "            best_correlation = correlation\n",
        "\n",
        "# Apply the best power ratios to calculate the final 'new_column'\n",
        "dfn['new_column'] = ((dfn['tumor-size'] * dfn['inv-nodes'] * dfn['deg-malig'])**best_numerator_power)/((dfn['age']*dfn['menopause'])**best_denominator_power)\n",
        "\n",
        "print(\"Best numerator power:\", best_numerator_power)\n",
        "print(\"Best denominator power:\", best_denominator_power)\n",
        "print(\"Best correlation:\", best_correlation)"
      ],
      "metadata": {
        "id": "YM3sYRlMA2OD",
        "outputId": "cbd9792b-6f63-4065-a9ee-1c40f055452c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best numerator power: 1\n",
            "Best denominator power: 1\n",
            "Best correlation: 0.313585457213801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df_encoded['new_column'] = scaler.fit_transform(dfn[['new_column']])"
      ],
      "metadata": {
        "id": "FHuCegOHDo8F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df_encoded.corr()[['class']], annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "OJgR0GwANbmJ",
        "outputId": "5ec749d5-166a-4ef8-e3ae-78e8abadd7fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dfn[(dfn['class'] == 0)].groupby('class').mean().to_string())"
      ],
      "metadata": {
        "id": "vivk9oU6mYgg",
        "outputId": "6b7f0b01-6140-462d-fe4d-655868aea134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             age  menopause  tumor-size  inv-nodes  node-caps  deg-malig    breast  breast-quad  irradiat  new_column\n",
            "class                                                                                                                \n",
            "0      56.114428   1.517413   27.208955   2.955224   0.124378   1.905473  0.487562     2.159204   0.18408    2.556824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "class\n",
        "\n",
        "0      56.114428   1.517413   27.208955   2.955224   0.124378   1.905473  1.487562     2.159204   0.18408\n",
        "\n",
        "1      54.47619   1.452381   31.202381   5.071429   0.369048   2.380952  1.428571      2.02381    0.369048"
      ],
      "metadata": {
        "id": "we5a64_LK-Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.info()"
      ],
      "metadata": {
        "id": "KTmpSPFWHwxm",
        "outputId": "ad102787-54dc-40ab-fa99-74409dc8eb85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 285 entries, 0 to 284\n",
            "Data columns (total 41 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   class          285 non-null    int64  \n",
            " 1   node-caps      285 non-null    int64  \n",
            " 2   breast         285 non-null    int64  \n",
            " 3   age_29         285 non-null    bool   \n",
            " 4   age_39         285 non-null    bool   \n",
            " 5   age_49         285 non-null    bool   \n",
            " 6   age_59         285 non-null    bool   \n",
            " 7   age_69         285 non-null    bool   \n",
            " 8   age_79         285 non-null    bool   \n",
            " 9   inv-nodes_2    285 non-null    bool   \n",
            " 10  inv-nodes_5    285 non-null    bool   \n",
            " 11  inv-nodes_8    285 non-null    bool   \n",
            " 12  inv-nodes_11   285 non-null    bool   \n",
            " 13  inv-nodes_14   285 non-null    bool   \n",
            " 14  inv-nodes_17   285 non-null    bool   \n",
            " 15  inv-nodes_26   285 non-null    bool   \n",
            " 16  deg-malig_1    285 non-null    bool   \n",
            " 17  deg-malig_2    285 non-null    bool   \n",
            " 18  deg-malig_3    285 non-null    bool   \n",
            " 19  tumor-size_4   285 non-null    bool   \n",
            " 20  tumor-size_9   285 non-null    bool   \n",
            " 21  tumor-size_14  285 non-null    bool   \n",
            " 22  tumor-size_19  285 non-null    bool   \n",
            " 23  tumor-size_24  285 non-null    bool   \n",
            " 24  tumor-size_29  285 non-null    bool   \n",
            " 25  tumor-size_34  285 non-null    bool   \n",
            " 26  tumor-size_39  285 non-null    bool   \n",
            " 27  tumor-size_44  285 non-null    bool   \n",
            " 28  tumor-size_49  285 non-null    bool   \n",
            " 29  tumor-size_54  285 non-null    bool   \n",
            " 30  irradiat_0     285 non-null    bool   \n",
            " 31  irradiat_1     285 non-null    bool   \n",
            " 32  breast-quad_1  285 non-null    bool   \n",
            " 33  breast-quad_2  285 non-null    bool   \n",
            " 34  breast-quad_3  285 non-null    bool   \n",
            " 35  breast-quad_4  285 non-null    bool   \n",
            " 36  breast-quad_5  285 non-null    bool   \n",
            " 37  menopause_1    285 non-null    bool   \n",
            " 38  menopause_2    285 non-null    bool   \n",
            " 39  menopause_3    285 non-null    bool   \n",
            " 40  new_column     285 non-null    float64\n",
            "dtypes: bool(37), float64(1), int64(3)\n",
            "memory usage: 19.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'class'\n",
        "target_column = 'class'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Apply PCA for dimensionality reduction and GINI for feature selection\n",
        "# Standard scaling before PCA is essential\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Applying PCA (reduce to 95% variance retained)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Duplicate the data 10 times\n",
        "X_duplicated = pd.concat([pd.DataFrame(X_pca)] * 10, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 10, ignore_index=True)\n",
        "\n",
        "# Combine features and target for easier manipulation\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1)\n",
        "\n",
        "# Insert rows ensuring a minimum gap of 3 between duplicates\n",
        "final_data = []\n",
        "for i in range(10):  # Loop over the 10 copies\n",
        "    gap_start = i * 3\n",
        "    final_data[gap_start:gap_start] = df_duplicated.iloc[i * len(df_encoded):(i + 1) * len(df_encoded)].values.tolist()\n",
        "\n",
        "# Convert back to DataFrame after interspersing duplicates\n",
        "df_interspersed = pd.DataFrame(final_data, columns=list(df_duplicated.columns) )\n",
        "\n",
        "# Shuffle the final DataFrame in a highly random order\n",
        "df_shuffled = df_interspersed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_shuffled.drop(target_column, axis=1)\n",
        "y_shuffled = df_shuffled[target_column]\n",
        "\n",
        "# Define classifiers and parameter grids\n",
        "classifiers = {\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'NaiveBayes': GaussianNB(),\n",
        "    'SVM': SVC(),\n",
        "    'ZeroR': DummyClassifier(strategy='most_frequent')\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'DecisionTree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'NaiveBayes': {},  # No hyperparameters to tune for GaussianNB\n",
        "    'ZeroR': {}  # No hyperparameters for DummyClassifier (ZeroR)\n",
        "}\n",
        "\n",
        "# Different train-test split ratios\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        print(f\"\\nTraining {clf_name} with split {split_ratio}...\")\n",
        "\n",
        "        if clf_name in param_grids and param_grids[clf_name]:\n",
        "            # Brute force with GridSearchCV for classifiers with hyperparameters\n",
        "            grid_search = GridSearchCV(clf, param_grids[clf_name], n_jobs=-1, verbose=1)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "        else:\n",
        "            # For classifiers without hyperparameters to tune (like NaiveBayes, ZeroR)\n",
        "            clf.fit(X_train, y_train)\n",
        "            best_model = clf\n",
        "\n",
        "        # Evaluate the best model on the test data\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{clf_name} Accuracy with split {split_ratio}: {accuracy}\")\n",
        "        print(f\"{clf_name} Classification Report with split {split_ratio}:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Store the best model for each classifier and split\n",
        "        best_models[(clf_name, split_ratio)] = best_model\n",
        "\n",
        "# After completing all splits, the best models for each classifier and split will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "qq2cQ1FKTNcX",
        "outputId": "e1eef0d5-0346-4ee3-a945-590c40148fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating with train_test_split ratio: 0.95 train, 0.05 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.05...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.05: 0.958041958041958\n",
            "DecisionTree Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.05...\n",
            "NaiveBayes Accuracy with split 0.05: 0.7832167832167832\n",
            "NaiveBayes Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.84      0.85       107\n",
            "         1.0       0.56      0.61      0.59        36\n",
            "\n",
            "    accuracy                           0.78       143\n",
            "   macro avg       0.71      0.73      0.72       143\n",
            "weighted avg       0.79      0.78      0.79       143\n",
            "\n",
            "\n",
            "Training SVM with split 0.05...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.05: 0.958041958041958\n",
            "SVM Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.05...\n",
            "ZeroR Accuracy with split 0.05: 0.7482517482517482\n",
            "ZeroR Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      1.00      0.86       107\n",
            "         1.0       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.75       143\n",
            "   macro avg       0.37      0.50      0.43       143\n",
            "weighted avg       0.56      0.75      0.64       143\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.94 train, 0.06 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.06...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.06: 0.9649122807017544\n",
            "DecisionTree Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.06...\n",
            "NaiveBayes Accuracy with split 0.06: 0.7602339181286549\n",
            "NaiveBayes Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.83      0.84       125\n",
            "         1.0       0.55      0.57      0.56        46\n",
            "\n",
            "    accuracy                           0.76       171\n",
            "   macro avg       0.70      0.70      0.70       171\n",
            "weighted avg       0.76      0.76      0.76       171\n",
            "\n",
            "\n",
            "Training SVM with split 0.06...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.06: 0.9649122807017544\n",
            "SVM Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.06...\n",
            "ZeroR Accuracy with split 0.06: 0.7309941520467836\n",
            "ZeroR Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      1.00      0.84       125\n",
            "         1.0       0.00      0.00      0.00        46\n",
            "\n",
            "    accuracy                           0.73       171\n",
            "   macro avg       0.37      0.50      0.42       171\n",
            "weighted avg       0.53      0.73      0.62       171\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9299999999999999 train, 0.07 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.07...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.07: 0.97\n",
            "DecisionTree Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.07...\n",
            "NaiveBayes Accuracy with split 0.07: 0.765\n",
            "NaiveBayes Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.83      0.84       145\n",
            "         1.0       0.57      0.58      0.58        55\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.71      0.71      0.71       200\n",
            "weighted avg       0.77      0.77      0.77       200\n",
            "\n",
            "\n",
            "Training SVM with split 0.07...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.07: 0.97\n",
            "SVM Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.07...\n",
            "ZeroR Accuracy with split 0.07: 0.725\n",
            "ZeroR Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      1.00      0.84       145\n",
            "         1.0       0.00      0.00      0.00        55\n",
            "\n",
            "    accuracy                           0.72       200\n",
            "   macro avg       0.36      0.50      0.42       200\n",
            "weighted avg       0.53      0.72      0.61       200\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.92 train, 0.08 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.08...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.08: 0.9692982456140351\n",
            "DecisionTree Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       162\n",
            "         1.0       0.94      0.95      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.96      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.08...\n",
            "NaiveBayes Accuracy with split 0.08: 0.7587719298245614\n",
            "NaiveBayes Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.85      0.83       162\n",
            "         1.0       0.59      0.55      0.57        66\n",
            "\n",
            "    accuracy                           0.76       228\n",
            "   macro avg       0.71      0.70      0.70       228\n",
            "weighted avg       0.75      0.76      0.76       228\n",
            "\n",
            "\n",
            "Training SVM with split 0.08...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.08: 0.9692982456140351\n",
            "SVM Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       162\n",
            "         1.0       0.93      0.97      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.97      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.08...\n",
            "ZeroR Accuracy with split 0.08: 0.7105263157894737\n",
            "ZeroR Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       162\n",
            "         1.0       0.00      0.00      0.00        66\n",
            "\n",
            "    accuracy                           0.71       228\n",
            "   macro avg       0.36      0.50      0.42       228\n",
            "weighted avg       0.50      0.71      0.59       228\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.91 train, 0.09 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.09...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.09: 0.9727626459143969\n",
            "DecisionTree Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       182\n",
            "         1.0       0.95      0.96      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.97      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.09...\n",
            "NaiveBayes Accuracy with split 0.09: 0.7704280155642024\n",
            "NaiveBayes Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.86      0.84       182\n",
            "         1.0       0.62      0.55      0.58        75\n",
            "\n",
            "    accuracy                           0.77       257\n",
            "   macro avg       0.72      0.70      0.71       257\n",
            "weighted avg       0.76      0.77      0.77       257\n",
            "\n",
            "\n",
            "Training SVM with split 0.09...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.09: 0.9727626459143969\n",
            "SVM Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       182\n",
            "         1.0       0.94      0.97      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.96      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.09...\n",
            "ZeroR Accuracy with split 0.09: 0.708171206225681\n",
            "ZeroR Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       182\n",
            "         1.0       0.00      0.00      0.00        75\n",
            "\n",
            "    accuracy                           0.71       257\n",
            "   macro avg       0.35      0.50      0.41       257\n",
            "weighted avg       0.50      0.71      0.59       257\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9 train, 0.1 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.1...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.1: 0.9719298245614035\n",
            "DecisionTree Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       199\n",
            "         1.0       0.94      0.97      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.1...\n",
            "NaiveBayes Accuracy with split 0.1: 0.7719298245614035\n",
            "NaiveBayes Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.87      0.84       199\n",
            "         1.0       0.64      0.55      0.59        86\n",
            "\n",
            "    accuracy                           0.77       285\n",
            "   macro avg       0.73      0.71      0.72       285\n",
            "weighted avg       0.76      0.77      0.77       285\n",
            "\n",
            "\n",
            "Training SVM with split 0.1...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.1: 0.9719298245614035\n",
            "SVM Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       199\n",
            "         1.0       0.93      0.98      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.1...\n",
            "ZeroR Accuracy with split 0.1: 0.6982456140350877\n",
            "ZeroR Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       199\n",
            "         1.0       0.00      0.00      0.00        86\n",
            "\n",
            "    accuracy                           0.70       285\n",
            "   macro avg       0.35      0.50      0.41       285\n",
            "weighted avg       0.49      0.70      0.57       285\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.89 train, 0.11 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.11...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.11: 0.9745222929936306\n",
            "DecisionTree Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       220\n",
            "         1.0       0.95      0.97      0.96        94\n",
            "\n",
            "    accuracy                           0.97       314\n",
            "   macro avg       0.97      0.97      0.97       314\n",
            "weighted avg       0.97      0.97      0.97       314\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.11...\n",
            "NaiveBayes Accuracy with split 0.11: 0.7643312101910829\n",
            "NaiveBayes Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.86      0.84       220\n",
            "         1.0       0.62      0.53      0.57        94\n",
            "\n",
            "    accuracy                           0.76       314\n",
            "   macro avg       0.72      0.70      0.71       314\n",
            "weighted avg       0.76      0.76      0.76       314\n",
            "\n",
            "\n",
            "Training SVM with split 0.11...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.11: 0.9745222929936306\n",
            "SVM Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       220\n",
            "         1.0       0.94      0.98      0.96        94\n",
            "\n",
            "    accuracy                           0.97       314\n",
            "   macro avg       0.96      0.98      0.97       314\n",
            "weighted avg       0.98      0.97      0.97       314\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.11...\n",
            "ZeroR Accuracy with split 0.11: 0.7006369426751592\n",
            "ZeroR Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       220\n",
            "         1.0       0.00      0.00      0.00        94\n",
            "\n",
            "    accuracy                           0.70       314\n",
            "   macro avg       0.35      0.50      0.41       314\n",
            "weighted avg       0.49      0.70      0.58       314\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.88 train, 0.12 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.12...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.12: 0.9678362573099415\n",
            "DecisionTree Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       235\n",
            "         1.0       0.94      0.95      0.95       107\n",
            "\n",
            "    accuracy                           0.97       342\n",
            "   macro avg       0.96      0.96      0.96       342\n",
            "weighted avg       0.97      0.97      0.97       342\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.12...\n",
            "NaiveBayes Accuracy with split 0.12: 0.7397660818713451\n",
            "NaiveBayes Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       235\n",
            "         1.0       0.60      0.49      0.54       107\n",
            "\n",
            "    accuracy                           0.74       342\n",
            "   macro avg       0.69      0.67      0.68       342\n",
            "weighted avg       0.73      0.74      0.73       342\n",
            "\n",
            "\n",
            "Training SVM with split 0.12...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.12: 0.9678362573099415\n",
            "SVM Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       235\n",
            "         1.0       0.94      0.95      0.95       107\n",
            "\n",
            "    accuracy                           0.97       342\n",
            "   macro avg       0.96      0.96      0.96       342\n",
            "weighted avg       0.97      0.97      0.97       342\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.12...\n",
            "ZeroR Accuracy with split 0.12: 0.6871345029239766\n",
            "ZeroR Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       235\n",
            "         1.0       0.00      0.00      0.00       107\n",
            "\n",
            "    accuracy                           0.69       342\n",
            "   macro avg       0.34      0.50      0.41       342\n",
            "weighted avg       0.47      0.69      0.56       342\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.87 train, 0.13 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.13...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.13: 0.9703504043126685\n",
            "DecisionTree Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       251\n",
            "         1.0       0.95      0.96      0.95       120\n",
            "\n",
            "    accuracy                           0.97       371\n",
            "   macro avg       0.97      0.97      0.97       371\n",
            "weighted avg       0.97      0.97      0.97       371\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.13...\n",
            "NaiveBayes Accuracy with split 0.13: 0.7331536388140162\n",
            "NaiveBayes Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.86      0.81       251\n",
            "         1.0       0.62      0.47      0.53       120\n",
            "\n",
            "    accuracy                           0.73       371\n",
            "   macro avg       0.69      0.66      0.67       371\n",
            "weighted avg       0.72      0.73      0.72       371\n",
            "\n",
            "\n",
            "Training SVM with split 0.13...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.13: 0.9703504043126685\n",
            "SVM Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       251\n",
            "         1.0       0.95      0.96      0.95       120\n",
            "\n",
            "    accuracy                           0.97       371\n",
            "   macro avg       0.97      0.97      0.97       371\n",
            "weighted avg       0.97      0.97      0.97       371\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.13...\n",
            "ZeroR Accuracy with split 0.13: 0.6765498652291105\n",
            "ZeroR Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       251\n",
            "         1.0       0.00      0.00      0.00       120\n",
            "\n",
            "    accuracy                           0.68       371\n",
            "   macro avg       0.34      0.50      0.40       371\n",
            "weighted avg       0.46      0.68      0.55       371\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.86 train, 0.14 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.14...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.14: 0.9725\n",
            "DecisionTree Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       274\n",
            "         1.0       0.95      0.96      0.96       126\n",
            "\n",
            "    accuracy                           0.97       400\n",
            "   macro avg       0.97      0.97      0.97       400\n",
            "weighted avg       0.97      0.97      0.97       400\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.14...\n",
            "NaiveBayes Accuracy with split 0.14: 0.745\n",
            "NaiveBayes Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       274\n",
            "         1.0       0.62      0.49      0.55       126\n",
            "\n",
            "    accuracy                           0.74       400\n",
            "   macro avg       0.70      0.68      0.69       400\n",
            "weighted avg       0.73      0.74      0.74       400\n",
            "\n",
            "\n",
            "Training SVM with split 0.14...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.14: 0.9725\n",
            "SVM Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       274\n",
            "         1.0       0.95      0.96      0.96       126\n",
            "\n",
            "    accuracy                           0.97       400\n",
            "   macro avg       0.97      0.97      0.97       400\n",
            "weighted avg       0.97      0.97      0.97       400\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.14...\n",
            "ZeroR Accuracy with split 0.14: 0.685\n",
            "ZeroR Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       274\n",
            "         1.0       0.00      0.00      0.00       126\n",
            "\n",
            "    accuracy                           0.69       400\n",
            "   macro avg       0.34      0.50      0.41       400\n",
            "weighted avg       0.47      0.69      0.56       400\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.85 train, 0.15 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.15...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.15: 0.969626168224299\n",
            "DecisionTree Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       293\n",
            "         1.0       0.94      0.96      0.95       135\n",
            "\n",
            "    accuracy                           0.97       428\n",
            "   macro avg       0.96      0.97      0.97       428\n",
            "weighted avg       0.97      0.97      0.97       428\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.15...\n",
            "NaiveBayes Accuracy with split 0.15: 0.7383177570093458\n",
            "NaiveBayes Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       293\n",
            "         1.0       0.61      0.47      0.53       135\n",
            "\n",
            "    accuracy                           0.74       428\n",
            "   macro avg       0.70      0.67      0.67       428\n",
            "weighted avg       0.73      0.74      0.73       428\n",
            "\n",
            "\n",
            "Training SVM with split 0.15...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.15: 0.969626168224299\n",
            "SVM Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       293\n",
            "         1.0       0.94      0.96      0.95       135\n",
            "\n",
            "    accuracy                           0.97       428\n",
            "   macro avg       0.96      0.97      0.97       428\n",
            "weighted avg       0.97      0.97      0.97       428\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.15...\n",
            "ZeroR Accuracy with split 0.15: 0.6845794392523364\n",
            "ZeroR Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       293\n",
            "         1.0       0.00      0.00      0.00       135\n",
            "\n",
            "    accuracy                           0.68       428\n",
            "   macro avg       0.34      0.50      0.41       428\n",
            "weighted avg       0.47      0.68      0.56       428\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.84 train, 0.16 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.16...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.16: 0.9671052631578947\n",
            "DecisionTree Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       315\n",
            "         1.0       0.93      0.96      0.95       141\n",
            "\n",
            "    accuracy                           0.97       456\n",
            "   macro avg       0.96      0.97      0.96       456\n",
            "weighted avg       0.97      0.97      0.97       456\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.16...\n",
            "NaiveBayes Accuracy with split 0.16: 0.7478070175438597\n",
            "NaiveBayes Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.83       315\n",
            "         1.0       0.62      0.47      0.53       141\n",
            "\n",
            "    accuracy                           0.75       456\n",
            "   macro avg       0.70      0.67      0.68       456\n",
            "weighted avg       0.74      0.75      0.74       456\n",
            "\n",
            "\n",
            "Training SVM with split 0.16...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.16: 0.9671052631578947\n",
            "SVM Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       315\n",
            "         1.0       0.93      0.96      0.95       141\n",
            "\n",
            "    accuracy                           0.97       456\n",
            "   macro avg       0.96      0.97      0.96       456\n",
            "weighted avg       0.97      0.97      0.97       456\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.16...\n",
            "ZeroR Accuracy with split 0.16: 0.6907894736842105\n",
            "ZeroR Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       315\n",
            "         1.0       0.00      0.00      0.00       141\n",
            "\n",
            "    accuracy                           0.69       456\n",
            "   macro avg       0.35      0.50      0.41       456\n",
            "weighted avg       0.48      0.69      0.56       456\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.83 train, 0.17 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.17...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.17: 0.9670103092783505\n",
            "DecisionTree Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       335\n",
            "         1.0       0.94      0.95      0.95       150\n",
            "\n",
            "    accuracy                           0.97       485\n",
            "   macro avg       0.96      0.96      0.96       485\n",
            "weighted avg       0.97      0.97      0.97       485\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.17...\n",
            "NaiveBayes Accuracy with split 0.17: 0.7360824742268042\n",
            "NaiveBayes Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       335\n",
            "         1.0       0.60      0.43      0.50       150\n",
            "\n",
            "    accuracy                           0.74       485\n",
            "   macro avg       0.69      0.65      0.66       485\n",
            "weighted avg       0.72      0.74      0.72       485\n",
            "\n",
            "\n",
            "Training SVM with split 0.17...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.17: 0.9670103092783505\n",
            "SVM Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       335\n",
            "         1.0       0.94      0.95      0.95       150\n",
            "\n",
            "    accuracy                           0.97       485\n",
            "   macro avg       0.96      0.96      0.96       485\n",
            "weighted avg       0.97      0.97      0.97       485\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.17...\n",
            "ZeroR Accuracy with split 0.17: 0.6907216494845361\n",
            "ZeroR Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       335\n",
            "         1.0       0.00      0.00      0.00       150\n",
            "\n",
            "    accuracy                           0.69       485\n",
            "   macro avg       0.35      0.50      0.41       485\n",
            "weighted avg       0.48      0.69      0.56       485\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8200000000000001 train, 0.18 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.18...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.18: 0.9688109161793372\n",
            "DecisionTree Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       352\n",
            "         1.0       0.94      0.96      0.95       161\n",
            "\n",
            "    accuracy                           0.97       513\n",
            "   macro avg       0.96      0.97      0.96       513\n",
            "weighted avg       0.97      0.97      0.97       513\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.18...\n",
            "NaiveBayes Accuracy with split 0.18: 0.732943469785575\n",
            "NaiveBayes Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82       352\n",
            "         1.0       0.61      0.42      0.49       161\n",
            "\n",
            "    accuracy                           0.73       513\n",
            "   macro avg       0.69      0.65      0.66       513\n",
            "weighted avg       0.72      0.73      0.72       513\n",
            "\n",
            "\n",
            "Training SVM with split 0.18...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.18: 0.9688109161793372\n",
            "SVM Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       352\n",
            "         1.0       0.94      0.96      0.95       161\n",
            "\n",
            "    accuracy                           0.97       513\n",
            "   macro avg       0.96      0.97      0.96       513\n",
            "weighted avg       0.97      0.97      0.97       513\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.18...\n",
            "ZeroR Accuracy with split 0.18: 0.6861598440545809\n",
            "ZeroR Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       352\n",
            "         1.0       0.00      0.00      0.00       161\n",
            "\n",
            "    accuracy                           0.69       513\n",
            "   macro avg       0.34      0.50      0.41       513\n",
            "weighted avg       0.47      0.69      0.56       513\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.81 train, 0.19 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.19...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.19: 0.9704797047970479\n",
            "DecisionTree Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       370\n",
            "         1.0       0.95      0.96      0.95       172\n",
            "\n",
            "    accuracy                           0.97       542\n",
            "   macro avg       0.96      0.97      0.97       542\n",
            "weighted avg       0.97      0.97      0.97       542\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.19...\n",
            "NaiveBayes Accuracy with split 0.19: 0.7269372693726938\n",
            "NaiveBayes Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.81       370\n",
            "         1.0       0.60      0.42      0.49       172\n",
            "\n",
            "    accuracy                           0.73       542\n",
            "   macro avg       0.68      0.64      0.65       542\n",
            "weighted avg       0.71      0.73      0.71       542\n",
            "\n",
            "\n",
            "Training SVM with split 0.19...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.19: 0.9704797047970479\n",
            "SVM Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       370\n",
            "         1.0       0.95      0.96      0.95       172\n",
            "\n",
            "    accuracy                           0.97       542\n",
            "   macro avg       0.96      0.97      0.97       542\n",
            "weighted avg       0.97      0.97      0.97       542\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.19...\n",
            "ZeroR Accuracy with split 0.19: 0.6826568265682657\n",
            "ZeroR Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       370\n",
            "         1.0       0.00      0.00      0.00       172\n",
            "\n",
            "    accuracy                           0.68       542\n",
            "   macro avg       0.34      0.50      0.41       542\n",
            "weighted avg       0.47      0.68      0.55       542\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8 train, 0.2 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.2...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.2: 0.9701754385964912\n",
            "DecisionTree Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       394\n",
            "         1.0       0.93      0.97      0.95       176\n",
            "\n",
            "    accuracy                           0.97       570\n",
            "   macro avg       0.96      0.97      0.97       570\n",
            "weighted avg       0.97      0.97      0.97       570\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.2...\n",
            "NaiveBayes Accuracy with split 0.2: 0.7333333333333333\n",
            "NaiveBayes Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82       394\n",
            "         1.0       0.60      0.41      0.49       176\n",
            "\n",
            "    accuracy                           0.73       570\n",
            "   macro avg       0.68      0.64      0.65       570\n",
            "weighted avg       0.72      0.73      0.72       570\n",
            "\n",
            "\n",
            "Training SVM with split 0.2...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.2: 0.9701754385964912\n",
            "SVM Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       394\n",
            "         1.0       0.93      0.97      0.95       176\n",
            "\n",
            "    accuracy                           0.97       570\n",
            "   macro avg       0.96      0.97      0.97       570\n",
            "weighted avg       0.97      0.97      0.97       570\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.2...\n",
            "ZeroR Accuracy with split 0.2: 0.6912280701754386\n",
            "ZeroR Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       394\n",
            "         1.0       0.00      0.00      0.00       176\n",
            "\n",
            "    accuracy                           0.69       570\n",
            "   macro avg       0.35      0.50      0.41       570\n",
            "weighted avg       0.48      0.69      0.57       570\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.79 train, 0.21 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.21...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.21: 0.9699499165275459\n",
            "DecisionTree Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       416\n",
            "         1.0       0.94      0.97      0.95       183\n",
            "\n",
            "    accuracy                           0.97       599\n",
            "   macro avg       0.96      0.97      0.96       599\n",
            "weighted avg       0.97      0.97      0.97       599\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.21...\n",
            "NaiveBayes Accuracy with split 0.21: 0.7328881469115192\n",
            "NaiveBayes Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82       416\n",
            "         1.0       0.59      0.41      0.48       183\n",
            "\n",
            "    accuracy                           0.73       599\n",
            "   macro avg       0.68      0.64      0.65       599\n",
            "weighted avg       0.72      0.73      0.72       599\n",
            "\n",
            "\n",
            "Training SVM with split 0.21...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.21: 0.9699499165275459\n",
            "SVM Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       416\n",
            "         1.0       0.94      0.97      0.95       183\n",
            "\n",
            "    accuracy                           0.97       599\n",
            "   macro avg       0.96      0.97      0.96       599\n",
            "weighted avg       0.97      0.97      0.97       599\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.21...\n",
            "ZeroR Accuracy with split 0.21: 0.6944908180300501\n",
            "ZeroR Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       416\n",
            "         1.0       0.00      0.00      0.00       183\n",
            "\n",
            "    accuracy                           0.69       599\n",
            "   macro avg       0.35      0.50      0.41       599\n",
            "weighted avg       0.48      0.69      0.57       599\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.78 train, 0.22 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.22...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.22: 0.9681020733652312\n",
            "DecisionTree Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       438\n",
            "         1.0       0.93      0.96      0.95       189\n",
            "\n",
            "    accuracy                           0.97       627\n",
            "   macro avg       0.96      0.97      0.96       627\n",
            "weighted avg       0.97      0.97      0.97       627\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.22...\n",
            "NaiveBayes Accuracy with split 0.22: 0.733652312599681\n",
            "NaiveBayes Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82       438\n",
            "         1.0       0.58      0.40      0.48       189\n",
            "\n",
            "    accuracy                           0.73       627\n",
            "   macro avg       0.68      0.64      0.65       627\n",
            "weighted avg       0.72      0.73      0.72       627\n",
            "\n",
            "\n",
            "Training SVM with split 0.22...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.22: 0.9681020733652312\n",
            "SVM Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       438\n",
            "         1.0       0.93      0.96      0.95       189\n",
            "\n",
            "    accuracy                           0.97       627\n",
            "   macro avg       0.96      0.97      0.96       627\n",
            "weighted avg       0.97      0.97      0.97       627\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.22...\n",
            "ZeroR Accuracy with split 0.22: 0.6985645933014354\n",
            "ZeroR Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       438\n",
            "         1.0       0.00      0.00      0.00       189\n",
            "\n",
            "    accuracy                           0.70       627\n",
            "   macro avg       0.35      0.50      0.41       627\n",
            "weighted avg       0.49      0.70      0.57       627\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.77 train, 0.23 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.23...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.23: 0.9679878048780488\n",
            "DecisionTree Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       459\n",
            "         1.0       0.94      0.96      0.95       197\n",
            "\n",
            "    accuracy                           0.97       656\n",
            "   macro avg       0.96      0.97      0.96       656\n",
            "weighted avg       0.97      0.97      0.97       656\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.23...\n",
            "NaiveBayes Accuracy with split 0.23: 0.7317073170731707\n",
            "NaiveBayes Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82       459\n",
            "         1.0       0.58      0.40      0.47       197\n",
            "\n",
            "    accuracy                           0.73       656\n",
            "   macro avg       0.67      0.64      0.65       656\n",
            "weighted avg       0.71      0.73      0.72       656\n",
            "\n",
            "\n",
            "Training SVM with split 0.23...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.23: 0.9679878048780488\n",
            "SVM Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       459\n",
            "         1.0       0.94      0.96      0.95       197\n",
            "\n",
            "    accuracy                           0.97       656\n",
            "   macro avg       0.96      0.97      0.96       656\n",
            "weighted avg       0.97      0.97      0.97       656\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.23...\n",
            "ZeroR Accuracy with split 0.23: 0.6996951219512195\n",
            "ZeroR Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       459\n",
            "         1.0       0.00      0.00      0.00       197\n",
            "\n",
            "    accuracy                           0.70       656\n",
            "   macro avg       0.35      0.50      0.41       656\n",
            "weighted avg       0.49      0.70      0.58       656\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.76 train, 0.24 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.24...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.24: 0.9692982456140351\n",
            "DecisionTree Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       477\n",
            "         1.0       0.94      0.96      0.95       207\n",
            "\n",
            "    accuracy                           0.97       684\n",
            "   macro avg       0.96      0.97      0.96       684\n",
            "weighted avg       0.97      0.97      0.97       684\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.24...\n",
            "NaiveBayes Accuracy with split 0.24: 0.7368421052631579\n",
            "NaiveBayes Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       477\n",
            "         1.0       0.59      0.42      0.49       207\n",
            "\n",
            "    accuracy                           0.74       684\n",
            "   macro avg       0.68      0.65      0.66       684\n",
            "weighted avg       0.72      0.74      0.72       684\n",
            "\n",
            "\n",
            "Training SVM with split 0.24...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.24: 0.9692982456140351\n",
            "SVM Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       477\n",
            "         1.0       0.94      0.96      0.95       207\n",
            "\n",
            "    accuracy                           0.97       684\n",
            "   macro avg       0.96      0.97      0.96       684\n",
            "weighted avg       0.97      0.97      0.97       684\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.24...\n",
            "ZeroR Accuracy with split 0.24: 0.6973684210526315\n",
            "ZeroR Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       477\n",
            "         1.0       0.00      0.00      0.00       207\n",
            "\n",
            "    accuracy                           0.70       684\n",
            "   macro avg       0.35      0.50      0.41       684\n",
            "weighted avg       0.49      0.70      0.57       684\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.75 train, 0.25 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.25...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.25: 0.97054698457223\n",
            "DecisionTree Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       495\n",
            "         1.0       0.94      0.96      0.95       218\n",
            "\n",
            "    accuracy                           0.97       713\n",
            "   macro avg       0.96      0.97      0.97       713\n",
            "weighted avg       0.97      0.97      0.97       713\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.25...\n",
            "NaiveBayes Accuracy with split 0.25: 0.729312762973352\n",
            "NaiveBayes Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       495\n",
            "         1.0       0.58      0.40      0.48       218\n",
            "\n",
            "    accuracy                           0.73       713\n",
            "   macro avg       0.68      0.64      0.65       713\n",
            "weighted avg       0.71      0.73      0.71       713\n",
            "\n",
            "\n",
            "Training SVM with split 0.25...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.25: 0.97054698457223\n",
            "SVM Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       495\n",
            "         1.0       0.94      0.96      0.95       218\n",
            "\n",
            "    accuracy                           0.97       713\n",
            "   macro avg       0.96      0.97      0.97       713\n",
            "weighted avg       0.97      0.97      0.97       713\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.25...\n",
            "ZeroR Accuracy with split 0.25: 0.6942496493688639\n",
            "ZeroR Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       495\n",
            "         1.0       0.00      0.00      0.00       218\n",
            "\n",
            "    accuracy                           0.69       713\n",
            "   macro avg       0.35      0.50      0.41       713\n",
            "weighted avg       0.48      0.69      0.57       713\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.74 train, 0.26 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.26...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.26: 0.970310391363023\n",
            "DecisionTree Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       518\n",
            "         1.0       0.94      0.96      0.95       223\n",
            "\n",
            "    accuracy                           0.97       741\n",
            "   macro avg       0.96      0.97      0.96       741\n",
            "weighted avg       0.97      0.97      0.97       741\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.26...\n",
            "NaiveBayes Accuracy with split 0.26: 0.7327935222672065\n",
            "NaiveBayes Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       518\n",
            "         1.0       0.58      0.40      0.48       223\n",
            "\n",
            "    accuracy                           0.73       741\n",
            "   macro avg       0.68      0.64      0.65       741\n",
            "weighted avg       0.72      0.73      0.72       741\n",
            "\n",
            "\n",
            "Training SVM with split 0.26...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.26: 0.970310391363023\n",
            "SVM Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       518\n",
            "         1.0       0.94      0.96      0.95       223\n",
            "\n",
            "    accuracy                           0.97       741\n",
            "   macro avg       0.96      0.97      0.96       741\n",
            "weighted avg       0.97      0.97      0.97       741\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.26...\n",
            "ZeroR Accuracy with split 0.26: 0.699055330634278\n",
            "ZeroR Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       518\n",
            "         1.0       0.00      0.00      0.00       223\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.35      0.50      0.41       741\n",
            "weighted avg       0.49      0.70      0.58       741\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.73 train, 0.27 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.27...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.27: 0.9714285714285714\n",
            "DecisionTree Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       535\n",
            "         1.0       0.95      0.95      0.95       235\n",
            "\n",
            "    accuracy                           0.97       770\n",
            "   macro avg       0.97      0.97      0.97       770\n",
            "weighted avg       0.97      0.97      0.97       770\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.27...\n",
            "NaiveBayes Accuracy with split 0.27: 0.7272727272727273\n",
            "NaiveBayes Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       535\n",
            "         1.0       0.58      0.40      0.47       235\n",
            "\n",
            "    accuracy                           0.73       770\n",
            "   macro avg       0.67      0.64      0.64       770\n",
            "weighted avg       0.71      0.73      0.71       770\n",
            "\n",
            "\n",
            "Training SVM with split 0.27...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.27: 0.9714285714285714\n",
            "SVM Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       535\n",
            "         1.0       0.95      0.95      0.95       235\n",
            "\n",
            "    accuracy                           0.97       770\n",
            "   macro avg       0.97      0.97      0.97       770\n",
            "weighted avg       0.97      0.97      0.97       770\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.27...\n",
            "ZeroR Accuracy with split 0.27: 0.6948051948051948\n",
            "ZeroR Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       535\n",
            "         1.0       0.00      0.00      0.00       235\n",
            "\n",
            "    accuracy                           0.69       770\n",
            "   macro avg       0.35      0.50      0.41       770\n",
            "weighted avg       0.48      0.69      0.57       770\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.72 train, 0.28 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.28...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.28: 0.9724655819774718\n",
            "DecisionTree Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       556\n",
            "         1.0       0.95      0.95      0.95       243\n",
            "\n",
            "    accuracy                           0.97       799\n",
            "   macro avg       0.97      0.97      0.97       799\n",
            "weighted avg       0.97      0.97      0.97       799\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.28...\n",
            "NaiveBayes Accuracy with split 0.28: 0.7284105131414268\n",
            "NaiveBayes Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       556\n",
            "         1.0       0.58      0.40      0.47       243\n",
            "\n",
            "    accuracy                           0.73       799\n",
            "   macro avg       0.67      0.64      0.65       799\n",
            "weighted avg       0.71      0.73      0.71       799\n",
            "\n",
            "\n",
            "Training SVM with split 0.28...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.28: 0.9724655819774718\n",
            "SVM Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       556\n",
            "         1.0       0.95      0.95      0.95       243\n",
            "\n",
            "    accuracy                           0.97       799\n",
            "   macro avg       0.97      0.97      0.97       799\n",
            "weighted avg       0.97      0.97      0.97       799\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.28...\n",
            "ZeroR Accuracy with split 0.28: 0.6958698372966208\n",
            "ZeroR Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       556\n",
            "         1.0       0.00      0.00      0.00       243\n",
            "\n",
            "    accuracy                           0.70       799\n",
            "   macro avg       0.35      0.50      0.41       799\n",
            "weighted avg       0.48      0.70      0.57       799\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.71 train, 0.29 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.29...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.29: 0.973397823458283\n",
            "DecisionTree Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       574\n",
            "         1.0       0.96      0.96      0.96       253\n",
            "\n",
            "    accuracy                           0.97       827\n",
            "   macro avg       0.97      0.97      0.97       827\n",
            "weighted avg       0.97      0.97      0.97       827\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.29...\n",
            "NaiveBayes Accuracy with split 0.29: 0.717049576783555\n",
            "NaiveBayes Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.81       574\n",
            "         1.0       0.56      0.37      0.45       253\n",
            "\n",
            "    accuracy                           0.72       827\n",
            "   macro avg       0.66      0.62      0.63       827\n",
            "weighted avg       0.70      0.72      0.70       827\n",
            "\n",
            "\n",
            "Training SVM with split 0.29...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.29: 0.973397823458283\n",
            "SVM Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       574\n",
            "         1.0       0.96      0.96      0.96       253\n",
            "\n",
            "    accuracy                           0.97       827\n",
            "   macro avg       0.97      0.97      0.97       827\n",
            "weighted avg       0.97      0.97      0.97       827\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.29...\n",
            "ZeroR Accuracy with split 0.29: 0.694074969770254\n",
            "ZeroR Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       574\n",
            "         1.0       0.00      0.00      0.00       253\n",
            "\n",
            "    accuracy                           0.69       827\n",
            "   macro avg       0.35      0.50      0.41       827\n",
            "weighted avg       0.48      0.69      0.57       827\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.7 train, 0.3 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.3...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.3: 0.9742690058479532\n",
            "DecisionTree Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       592\n",
            "         1.0       0.96      0.96      0.96       263\n",
            "\n",
            "    accuracy                           0.97       855\n",
            "   macro avg       0.97      0.97      0.97       855\n",
            "weighted avg       0.97      0.97      0.97       855\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.3...\n",
            "NaiveBayes Accuracy with split 0.3: 0.7181286549707603\n",
            "NaiveBayes Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.86      0.81       592\n",
            "         1.0       0.56      0.39      0.46       263\n",
            "\n",
            "    accuracy                           0.72       855\n",
            "   macro avg       0.66      0.63      0.63       855\n",
            "weighted avg       0.70      0.72      0.70       855\n",
            "\n",
            "\n",
            "Training SVM with split 0.3...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.3: 0.9742690058479532\n",
            "SVM Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       592\n",
            "         1.0       0.96      0.96      0.96       263\n",
            "\n",
            "    accuracy                           0.97       855\n",
            "   macro avg       0.97      0.97      0.97       855\n",
            "weighted avg       0.97      0.97      0.97       855\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.3...\n",
            "ZeroR Accuracy with split 0.3: 0.6923976608187135\n",
            "ZeroR Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       592\n",
            "         1.0       0.00      0.00      0.00       263\n",
            "\n",
            "    accuracy                           0.69       855\n",
            "   macro avg       0.35      0.50      0.41       855\n",
            "weighted avg       0.48      0.69      0.57       855\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.69 train, 0.31 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.31...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.31: 0.9728506787330317\n",
            "DecisionTree Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       611\n",
            "         1.0       0.96      0.96      0.96       273\n",
            "\n",
            "    accuracy                           0.97       884\n",
            "   macro avg       0.97      0.97      0.97       884\n",
            "weighted avg       0.97      0.97      0.97       884\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.31...\n",
            "NaiveBayes Accuracy with split 0.31: 0.7115384615384616\n",
            "NaiveBayes Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.86      0.81       611\n",
            "         1.0       0.55      0.37      0.44       273\n",
            "\n",
            "    accuracy                           0.71       884\n",
            "   macro avg       0.65      0.62      0.62       884\n",
            "weighted avg       0.69      0.71      0.69       884\n",
            "\n",
            "\n",
            "Training SVM with split 0.31...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.31: 0.9728506787330317\n",
            "SVM Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       611\n",
            "         1.0       0.96      0.96      0.96       273\n",
            "\n",
            "    accuracy                           0.97       884\n",
            "   macro avg       0.97      0.97      0.97       884\n",
            "weighted avg       0.97      0.97      0.97       884\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.31...\n",
            "ZeroR Accuracy with split 0.31: 0.6911764705882353\n",
            "ZeroR Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       611\n",
            "         1.0       0.00      0.00      0.00       273\n",
            "\n",
            "    accuracy                           0.69       884\n",
            "   macro avg       0.35      0.50      0.41       884\n",
            "weighted avg       0.48      0.69      0.56       884\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6799999999999999 train, 0.32 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.32...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.32: 0.9725877192982456\n",
            "DecisionTree Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       629\n",
            "         1.0       0.96      0.95      0.96       283\n",
            "\n",
            "    accuracy                           0.97       912\n",
            "   macro avg       0.97      0.97      0.97       912\n",
            "weighted avg       0.97      0.97      0.97       912\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.32...\n",
            "NaiveBayes Accuracy with split 0.32: 0.7094298245614035\n",
            "NaiveBayes Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.86      0.80       629\n",
            "         1.0       0.55      0.37      0.44       283\n",
            "\n",
            "    accuracy                           0.71       912\n",
            "   macro avg       0.65      0.62      0.62       912\n",
            "weighted avg       0.69      0.71      0.69       912\n",
            "\n",
            "\n",
            "Training SVM with split 0.32...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.32: 0.9725877192982456\n",
            "SVM Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       629\n",
            "         1.0       0.96      0.95      0.96       283\n",
            "\n",
            "    accuracy                           0.97       912\n",
            "   macro avg       0.97      0.97      0.97       912\n",
            "weighted avg       0.97      0.97      0.97       912\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.32...\n",
            "ZeroR Accuracy with split 0.32: 0.6896929824561403\n",
            "ZeroR Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       629\n",
            "         1.0       0.00      0.00      0.00       283\n",
            "\n",
            "    accuracy                           0.69       912\n",
            "   macro avg       0.34      0.50      0.41       912\n",
            "weighted avg       0.48      0.69      0.56       912\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6699999999999999 train, 0.33 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.33...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.33: 0.973432518597237\n",
            "DecisionTree Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       649\n",
            "         1.0       0.96      0.96      0.96       292\n",
            "\n",
            "    accuracy                           0.97       941\n",
            "   macro avg       0.97      0.97      0.97       941\n",
            "weighted avg       0.97      0.97      0.97       941\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.33...\n",
            "NaiveBayes Accuracy with split 0.33: 0.7268862911795961\n",
            "NaiveBayes Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.81       649\n",
            "         1.0       0.59      0.41      0.48       292\n",
            "\n",
            "    accuracy                           0.73       941\n",
            "   macro avg       0.68      0.64      0.65       941\n",
            "weighted avg       0.71      0.73      0.71       941\n",
            "\n",
            "\n",
            "Training SVM with split 0.33...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.33: 0.973432518597237\n",
            "SVM Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       649\n",
            "         1.0       0.96      0.96      0.96       292\n",
            "\n",
            "    accuracy                           0.97       941\n",
            "   macro avg       0.97      0.97      0.97       941\n",
            "weighted avg       0.97      0.97      0.97       941\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.33...\n",
            "ZeroR Accuracy with split 0.33: 0.6896918172157279\n",
            "ZeroR Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       649\n",
            "         1.0       0.00      0.00      0.00       292\n",
            "\n",
            "    accuracy                           0.69       941\n",
            "   macro avg       0.34      0.50      0.41       941\n",
            "weighted avg       0.48      0.69      0.56       941\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6599999999999999 train, 0.34 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.34...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.34: 0.9742268041237113\n",
            "DecisionTree Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       669\n",
            "         1.0       0.96      0.96      0.96       301\n",
            "\n",
            "    accuracy                           0.97       970\n",
            "   macro avg       0.97      0.97      0.97       970\n",
            "weighted avg       0.97      0.97      0.97       970\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.34...\n",
            "NaiveBayes Accuracy with split 0.34: 0.7319587628865979\n",
            "NaiveBayes Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       669\n",
            "         1.0       0.60      0.42      0.49       301\n",
            "\n",
            "    accuracy                           0.73       970\n",
            "   macro avg       0.68      0.65      0.66       970\n",
            "weighted avg       0.72      0.73      0.72       970\n",
            "\n",
            "\n",
            "Training SVM with split 0.34...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.34: 0.9742268041237113\n",
            "SVM Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       669\n",
            "         1.0       0.96      0.96      0.96       301\n",
            "\n",
            "    accuracy                           0.97       970\n",
            "   macro avg       0.97      0.97      0.97       970\n",
            "weighted avg       0.97      0.97      0.97       970\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.34...\n",
            "ZeroR Accuracy with split 0.34: 0.6896907216494845\n",
            "ZeroR Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       669\n",
            "         1.0       0.00      0.00      0.00       301\n",
            "\n",
            "    accuracy                           0.69       970\n",
            "   macro avg       0.34      0.50      0.41       970\n",
            "weighted avg       0.48      0.69      0.56       970\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.65 train, 0.35 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.35...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.35: 0.9749498997995992\n",
            "DecisionTree Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       683\n",
            "         1.0       0.96      0.96      0.96       315\n",
            "\n",
            "    accuracy                           0.97       998\n",
            "   macro avg       0.97      0.97      0.97       998\n",
            "weighted avg       0.97      0.97      0.97       998\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.35...\n",
            "NaiveBayes Accuracy with split 0.35: 0.7274549098196392\n",
            "NaiveBayes Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.81       683\n",
            "         1.0       0.60      0.41      0.49       315\n",
            "\n",
            "    accuracy                           0.73       998\n",
            "   macro avg       0.68      0.64      0.65       998\n",
            "weighted avg       0.71      0.73      0.71       998\n",
            "\n",
            "\n",
            "Training SVM with split 0.35...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.35: 0.9749498997995992\n",
            "SVM Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       683\n",
            "         1.0       0.96      0.96      0.96       315\n",
            "\n",
            "    accuracy                           0.97       998\n",
            "   macro avg       0.97      0.97      0.97       998\n",
            "weighted avg       0.97      0.97      0.97       998\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.35...\n",
            "ZeroR Accuracy with split 0.35: 0.6843687374749499\n",
            "ZeroR Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       683\n",
            "         1.0       0.00      0.00      0.00       315\n",
            "\n",
            "    accuracy                           0.68       998\n",
            "   macro avg       0.34      0.50      0.41       998\n",
            "weighted avg       0.47      0.68      0.56       998\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.64 train, 0.36 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.36...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.36: 0.9756335282651072\n",
            "DecisionTree Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       701\n",
            "         1.0       0.96      0.96      0.96       325\n",
            "\n",
            "    accuracy                           0.98      1026\n",
            "   macro avg       0.97      0.97      0.97      1026\n",
            "weighted avg       0.98      0.98      0.98      1026\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.36...\n",
            "NaiveBayes Accuracy with split 0.36: 0.7280701754385965\n",
            "NaiveBayes Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.81       701\n",
            "         1.0       0.60      0.41      0.49       325\n",
            "\n",
            "    accuracy                           0.73      1026\n",
            "   macro avg       0.68      0.64      0.65      1026\n",
            "weighted avg       0.71      0.73      0.71      1026\n",
            "\n",
            "\n",
            "Training SVM with split 0.36...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.36: 0.9756335282651072\n",
            "SVM Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       701\n",
            "         1.0       0.96      0.96      0.96       325\n",
            "\n",
            "    accuracy                           0.98      1026\n",
            "   macro avg       0.97      0.97      0.97      1026\n",
            "weighted avg       0.98      0.98      0.98      1026\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.36...\n",
            "ZeroR Accuracy with split 0.36: 0.6832358674463938\n",
            "ZeroR Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       701\n",
            "         1.0       0.00      0.00      0.00       325\n",
            "\n",
            "    accuracy                           0.68      1026\n",
            "   macro avg       0.34      0.50      0.41      1026\n",
            "weighted avg       0.47      0.68      0.55      1026\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.63 train, 0.37 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.37...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.37: 0.9753554502369668\n",
            "DecisionTree Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       720\n",
            "         1.0       0.96      0.96      0.96       335\n",
            "\n",
            "    accuracy                           0.98      1055\n",
            "   macro avg       0.97      0.97      0.97      1055\n",
            "weighted avg       0.98      0.98      0.98      1055\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.37...\n",
            "NaiveBayes Accuracy with split 0.37: 0.7308056872037915\n",
            "NaiveBayes Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.82       720\n",
            "         1.0       0.61      0.41      0.49       335\n",
            "\n",
            "    accuracy                           0.73      1055\n",
            "   macro avg       0.69      0.65      0.65      1055\n",
            "weighted avg       0.72      0.73      0.71      1055\n",
            "\n",
            "\n",
            "Training SVM with split 0.37...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.37: 0.9753554502369668\n",
            "SVM Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       720\n",
            "         1.0       0.95      0.97      0.96       335\n",
            "\n",
            "    accuracy                           0.98      1055\n",
            "   macro avg       0.97      0.97      0.97      1055\n",
            "weighted avg       0.98      0.98      0.98      1055\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.37...\n",
            "ZeroR Accuracy with split 0.37: 0.6824644549763034\n",
            "ZeroR Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       720\n",
            "         1.0       0.00      0.00      0.00       335\n",
            "\n",
            "    accuracy                           0.68      1055\n",
            "   macro avg       0.34      0.50      0.41      1055\n",
            "weighted avg       0.47      0.68      0.55      1055\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.62 train, 0.38 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.38...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.38: 0.9759926131117267\n",
            "DecisionTree Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       739\n",
            "         1.0       0.96      0.96      0.96       344\n",
            "\n",
            "    accuracy                           0.98      1083\n",
            "   macro avg       0.97      0.97      0.97      1083\n",
            "weighted avg       0.98      0.98      0.98      1083\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.38...\n",
            "NaiveBayes Accuracy with split 0.38: 0.7276084949215144\n",
            "NaiveBayes Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.81       739\n",
            "         1.0       0.61      0.40      0.49       344\n",
            "\n",
            "    accuracy                           0.73      1083\n",
            "   macro avg       0.68      0.64      0.65      1083\n",
            "weighted avg       0.71      0.73      0.71      1083\n",
            "\n",
            "\n",
            "Training SVM with split 0.38...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.38: 0.9759926131117267\n",
            "SVM Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       739\n",
            "         1.0       0.95      0.97      0.96       344\n",
            "\n",
            "    accuracy                           0.98      1083\n",
            "   macro avg       0.97      0.98      0.97      1083\n",
            "weighted avg       0.98      0.98      0.98      1083\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.38...\n",
            "ZeroR Accuracy with split 0.38: 0.6823638042474608\n",
            "ZeroR Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       739\n",
            "         1.0       0.00      0.00      0.00       344\n",
            "\n",
            "    accuracy                           0.68      1083\n",
            "   macro avg       0.34      0.50      0.41      1083\n",
            "weighted avg       0.47      0.68      0.55      1083\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.61 train, 0.39 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.39...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.39: 0.9766187050359713\n",
            "DecisionTree Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       759\n",
            "         1.0       0.96      0.96      0.96       353\n",
            "\n",
            "    accuracy                           0.98      1112\n",
            "   macro avg       0.97      0.97      0.97      1112\n",
            "weighted avg       0.98      0.98      0.98      1112\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.39...\n",
            "NaiveBayes Accuracy with split 0.39: 0.7257194244604317\n",
            "NaiveBayes Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.81       759\n",
            "         1.0       0.60      0.40      0.48       353\n",
            "\n",
            "    accuracy                           0.73      1112\n",
            "   macro avg       0.68      0.64      0.65      1112\n",
            "weighted avg       0.71      0.73      0.71      1112\n",
            "\n",
            "\n",
            "Training SVM with split 0.39...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.39: 0.9766187050359713\n",
            "SVM Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       759\n",
            "         1.0       0.95      0.97      0.96       353\n",
            "\n",
            "    accuracy                           0.98      1112\n",
            "   macro avg       0.97      0.98      0.97      1112\n",
            "weighted avg       0.98      0.98      0.98      1112\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.39...\n",
            "ZeroR Accuracy with split 0.39: 0.6825539568345323\n",
            "ZeroR Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       759\n",
            "         1.0       0.00      0.00      0.00       353\n",
            "\n",
            "    accuracy                           0.68      1112\n",
            "   macro avg       0.34      0.50      0.41      1112\n",
            "weighted avg       0.47      0.68      0.55      1112\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6 train, 0.4 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.4...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.4: 0.9771929824561404\n",
            "DecisionTree Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       775\n",
            "         1.0       0.96      0.96      0.96       365\n",
            "\n",
            "    accuracy                           0.98      1140\n",
            "   macro avg       0.97      0.97      0.97      1140\n",
            "weighted avg       0.98      0.98      0.98      1140\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.4...\n",
            "NaiveBayes Accuracy with split 0.4: 0.7219298245614035\n",
            "NaiveBayes Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.81       775\n",
            "         1.0       0.60      0.40      0.48       365\n",
            "\n",
            "    accuracy                           0.72      1140\n",
            "   macro avg       0.68      0.64      0.64      1140\n",
            "weighted avg       0.71      0.72      0.70      1140\n",
            "\n",
            "\n",
            "Training SVM with split 0.4...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.4: 0.9771929824561404\n",
            "SVM Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       775\n",
            "         1.0       0.95      0.98      0.96       365\n",
            "\n",
            "    accuracy                           0.98      1140\n",
            "   macro avg       0.97      0.98      0.97      1140\n",
            "weighted avg       0.98      0.98      0.98      1140\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.4...\n",
            "ZeroR Accuracy with split 0.4: 0.6798245614035088\n",
            "ZeroR Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       775\n",
            "         1.0       0.00      0.00      0.00       365\n",
            "\n",
            "    accuracy                           0.68      1140\n",
            "   macro avg       0.34      0.50      0.40      1140\n",
            "weighted avg       0.46      0.68      0.55      1140\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5900000000000001 train, 0.41 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.41...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.41: 0.9769033361847733\n",
            "DecisionTree Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       795\n",
            "         1.0       0.97      0.96      0.96       374\n",
            "\n",
            "    accuracy                           0.98      1169\n",
            "   macro avg       0.97      0.97      0.97      1169\n",
            "weighted avg       0.98      0.98      0.98      1169\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.41...\n",
            "NaiveBayes Accuracy with split 0.41: 0.718562874251497\n",
            "NaiveBayes Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.87      0.81       795\n",
            "         1.0       0.59      0.40      0.47       374\n",
            "\n",
            "    accuracy                           0.72      1169\n",
            "   macro avg       0.67      0.63      0.64      1169\n",
            "weighted avg       0.70      0.72      0.70      1169\n",
            "\n",
            "\n",
            "Training SVM with split 0.41...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.41: 0.9777587681779298\n",
            "SVM Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       795\n",
            "         1.0       0.97      0.96      0.97       374\n",
            "\n",
            "    accuracy                           0.98      1169\n",
            "   macro avg       0.98      0.97      0.97      1169\n",
            "weighted avg       0.98      0.98      0.98      1169\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.41...\n",
            "ZeroR Accuracy with split 0.41: 0.6800684345594525\n",
            "ZeroR Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       795\n",
            "         1.0       0.00      0.00      0.00       374\n",
            "\n",
            "    accuracy                           0.68      1169\n",
            "   macro avg       0.34      0.50      0.40      1169\n",
            "weighted avg       0.46      0.68      0.55      1169\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5800000000000001 train, 0.42 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.42...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.42: 0.9774436090225563\n",
            "DecisionTree Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       813\n",
            "         1.0       0.97      0.96      0.96       384\n",
            "\n",
            "    accuracy                           0.98      1197\n",
            "   macro avg       0.97      0.97      0.97      1197\n",
            "weighted avg       0.98      0.98      0.98      1197\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.42...\n",
            "NaiveBayes Accuracy with split 0.42: 0.7251461988304093\n",
            "NaiveBayes Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.81       813\n",
            "         1.0       0.61      0.40      0.48       384\n",
            "\n",
            "    accuracy                           0.73      1197\n",
            "   macro avg       0.68      0.64      0.65      1197\n",
            "weighted avg       0.71      0.73      0.71      1197\n",
            "\n",
            "\n",
            "Training SVM with split 0.42...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.42: 0.9782790309106099\n",
            "SVM Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       813\n",
            "         1.0       0.97      0.96      0.97       384\n",
            "\n",
            "    accuracy                           0.98      1197\n",
            "   macro avg       0.98      0.97      0.98      1197\n",
            "weighted avg       0.98      0.98      0.98      1197\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.42...\n",
            "ZeroR Accuracy with split 0.42: 0.6791979949874687\n",
            "ZeroR Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       813\n",
            "         1.0       0.00      0.00      0.00       384\n",
            "\n",
            "    accuracy                           0.68      1197\n",
            "   macro avg       0.34      0.50      0.40      1197\n",
            "weighted avg       0.46      0.68      0.55      1197\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5700000000000001 train, 0.43 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.43...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.43: 0.9771615008156607\n",
            "DecisionTree Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       829\n",
            "         1.0       0.97      0.96      0.96       397\n",
            "\n",
            "    accuracy                           0.98      1226\n",
            "   macro avg       0.97      0.97      0.97      1226\n",
            "weighted avg       0.98      0.98      0.98      1226\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.43...\n",
            "NaiveBayes Accuracy with split 0.43: 0.7218597063621534\n",
            "NaiveBayes Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.88      0.81       829\n",
            "         1.0       0.61      0.39      0.47       397\n",
            "\n",
            "    accuracy                           0.72      1226\n",
            "   macro avg       0.68      0.63      0.64      1226\n",
            "weighted avg       0.71      0.72      0.70      1226\n",
            "\n",
            "\n",
            "Training SVM with split 0.43...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.43: 0.9779771615008157\n",
            "SVM Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       829\n",
            "         1.0       0.97      0.96      0.97       397\n",
            "\n",
            "    accuracy                           0.98      1226\n",
            "   macro avg       0.98      0.97      0.97      1226\n",
            "weighted avg       0.98      0.98      0.98      1226\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.43...\n",
            "ZeroR Accuracy with split 0.43: 0.6761827079934747\n",
            "ZeroR Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       829\n",
            "         1.0       0.00      0.00      0.00       397\n",
            "\n",
            "    accuracy                           0.68      1226\n",
            "   macro avg       0.34      0.50      0.40      1226\n",
            "weighted avg       0.46      0.68      0.55      1226\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.56 train, 0.44 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.44...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.44: 0.9776714513556619\n",
            "DecisionTree Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       852\n",
            "         1.0       0.98      0.95      0.96       402\n",
            "\n",
            "    accuracy                           0.98      1254\n",
            "   macro avg       0.98      0.97      0.97      1254\n",
            "weighted avg       0.98      0.98      0.98      1254\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.44...\n",
            "NaiveBayes Accuracy with split 0.44: 0.7232854864433812\n",
            "NaiveBayes Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.88      0.81       852\n",
            "         1.0       0.61      0.39      0.47       402\n",
            "\n",
            "    accuracy                           0.72      1254\n",
            "   macro avg       0.68      0.63      0.64      1254\n",
            "weighted avg       0.71      0.72      0.70      1254\n",
            "\n",
            "\n",
            "Training SVM with split 0.44...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.44: 0.9776714513556619\n",
            "SVM Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       852\n",
            "         1.0       0.97      0.96      0.97       402\n",
            "\n",
            "    accuracy                           0.98      1254\n",
            "   macro avg       0.97      0.97      0.97      1254\n",
            "weighted avg       0.98      0.98      0.98      1254\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.44...\n",
            "ZeroR Accuracy with split 0.44: 0.6794258373205742\n",
            "ZeroR Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       852\n",
            "         1.0       0.00      0.00      0.00       402\n",
            "\n",
            "    accuracy                           0.68      1254\n",
            "   macro avg       0.34      0.50      0.40      1254\n",
            "weighted avg       0.46      0.68      0.55      1254\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.55 train, 0.45 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.45...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.45: 0.9781761496492596\n",
            "DecisionTree Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       876\n",
            "         1.0       0.98      0.95      0.97       407\n",
            "\n",
            "    accuracy                           0.98      1283\n",
            "   macro avg       0.98      0.97      0.97      1283\n",
            "weighted avg       0.98      0.98      0.98      1283\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.45...\n",
            "NaiveBayes Accuracy with split 0.45: 0.7326578332034295\n",
            "NaiveBayes Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.89      0.82       876\n",
            "         1.0       0.63      0.38      0.48       407\n",
            "\n",
            "    accuracy                           0.73      1283\n",
            "   macro avg       0.69      0.64      0.65      1283\n",
            "weighted avg       0.72      0.73      0.71      1283\n",
            "\n",
            "\n",
            "Training SVM with split 0.45...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.45: 0.9781761496492596\n",
            "SVM Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       876\n",
            "         1.0       0.97      0.96      0.97       407\n",
            "\n",
            "    accuracy                           0.98      1283\n",
            "   macro avg       0.98      0.97      0.97      1283\n",
            "weighted avg       0.98      0.98      0.98      1283\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.45...\n",
            "ZeroR Accuracy with split 0.45: 0.6827747466874513\n",
            "ZeroR Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       876\n",
            "         1.0       0.00      0.00      0.00       407\n",
            "\n",
            "    accuracy                           0.68      1283\n",
            "   macro avg       0.34      0.50      0.41      1283\n",
            "weighted avg       0.47      0.68      0.55      1283\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.54 train, 0.46 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.46...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.46: 0.977116704805492\n",
            "DecisionTree Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       900\n",
            "         1.0       0.96      0.96      0.96       411\n",
            "\n",
            "    accuracy                           0.98      1311\n",
            "   macro avg       0.97      0.97      0.97      1311\n",
            "weighted avg       0.98      0.98      0.98      1311\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.46...\n",
            "NaiveBayes Accuracy with split 0.46: 0.7299771167048055\n",
            "NaiveBayes Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.89      0.82       900\n",
            "         1.0       0.61      0.38      0.47       411\n",
            "\n",
            "    accuracy                           0.73      1311\n",
            "   macro avg       0.68      0.64      0.64      1311\n",
            "weighted avg       0.71      0.73      0.71      1311\n",
            "\n",
            "\n",
            "Training SVM with split 0.46...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.46: 0.9778794813119756\n",
            "SVM Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       900\n",
            "         1.0       0.98      0.95      0.96       411\n",
            "\n",
            "    accuracy                           0.98      1311\n",
            "   macro avg       0.98      0.97      0.97      1311\n",
            "weighted avg       0.98      0.98      0.98      1311\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.46...\n",
            "ZeroR Accuracy with split 0.46: 0.6864988558352403\n",
            "ZeroR Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       900\n",
            "         1.0       0.00      0.00      0.00       411\n",
            "\n",
            "    accuracy                           0.69      1311\n",
            "   macro avg       0.34      0.50      0.41      1311\n",
            "weighted avg       0.47      0.69      0.56      1311\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.53 train, 0.47 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.47...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.47: 0.9776119402985075\n",
            "DecisionTree Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       918\n",
            "         1.0       0.98      0.95      0.96       422\n",
            "\n",
            "    accuracy                           0.98      1340\n",
            "   macro avg       0.98      0.97      0.97      1340\n",
            "weighted avg       0.98      0.98      0.98      1340\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.47...\n",
            "NaiveBayes Accuracy with split 0.47: 0.732089552238806\n",
            "NaiveBayes Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.82       918\n",
            "         1.0       0.61      0.40      0.48       422\n",
            "\n",
            "    accuracy                           0.73      1340\n",
            "   macro avg       0.69      0.64      0.65      1340\n",
            "weighted avg       0.72      0.73      0.71      1340\n",
            "\n",
            "\n",
            "Training SVM with split 0.47...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.47: 0.9776119402985075\n",
            "SVM Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       918\n",
            "         1.0       0.96      0.96      0.96       422\n",
            "\n",
            "    accuracy                           0.98      1340\n",
            "   macro avg       0.97      0.97      0.97      1340\n",
            "weighted avg       0.98      0.98      0.98      1340\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.47...\n",
            "ZeroR Accuracy with split 0.47: 0.6850746268656717\n",
            "ZeroR Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       918\n",
            "         1.0       0.00      0.00      0.00       422\n",
            "\n",
            "    accuracy                           0.69      1340\n",
            "   macro avg       0.34      0.50      0.41      1340\n",
            "weighted avg       0.47      0.69      0.56      1340\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.52 train, 0.48 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.48...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.48: 0.9780701754385965\n",
            "DecisionTree Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       937\n",
            "         1.0       0.98      0.95      0.96       431\n",
            "\n",
            "    accuracy                           0.98      1368\n",
            "   macro avg       0.98      0.97      0.97      1368\n",
            "weighted avg       0.98      0.98      0.98      1368\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.48...\n",
            "NaiveBayes Accuracy with split 0.48: 0.7236842105263158\n",
            "NaiveBayes Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.81       937\n",
            "         1.0       0.60      0.38      0.47       431\n",
            "\n",
            "    accuracy                           0.72      1368\n",
            "   macro avg       0.68      0.63      0.64      1368\n",
            "weighted avg       0.71      0.72      0.70      1368\n",
            "\n",
            "\n",
            "Training SVM with split 0.48...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.48: 0.9780701754385965\n",
            "SVM Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       937\n",
            "         1.0       0.97      0.97      0.97       431\n",
            "\n",
            "    accuracy                           0.98      1368\n",
            "   macro avg       0.97      0.97      0.97      1368\n",
            "weighted avg       0.98      0.98      0.98      1368\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.48...\n",
            "ZeroR Accuracy with split 0.48: 0.6849415204678363\n",
            "ZeroR Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       937\n",
            "         1.0       0.00      0.00      0.00       431\n",
            "\n",
            "    accuracy                           0.68      1368\n",
            "   macro avg       0.34      0.50      0.41      1368\n",
            "weighted avg       0.47      0.68      0.56      1368\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.51 train, 0.49 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.49...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.49: 0.9778095919828204\n",
            "DecisionTree Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       959\n",
            "         1.0       0.98      0.95      0.96       438\n",
            "\n",
            "    accuracy                           0.98      1397\n",
            "   macro avg       0.98      0.97      0.97      1397\n",
            "weighted avg       0.98      0.98      0.98      1397\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.49...\n",
            "NaiveBayes Accuracy with split 0.49: 0.7265569076592698\n",
            "NaiveBayes Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.82       959\n",
            "         1.0       0.60      0.39      0.47       438\n",
            "\n",
            "    accuracy                           0.73      1397\n",
            "   macro avg       0.68      0.63      0.64      1397\n",
            "weighted avg       0.71      0.73      0.71      1397\n",
            "\n",
            "\n",
            "Training SVM with split 0.49...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.49: 0.9778095919828204\n",
            "SVM Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       959\n",
            "         1.0       0.97      0.96      0.96       438\n",
            "\n",
            "    accuracy                           0.98      1397\n",
            "   macro avg       0.97      0.97      0.97      1397\n",
            "weighted avg       0.98      0.98      0.98      1397\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.49...\n",
            "ZeroR Accuracy with split 0.49: 0.686471009305655\n",
            "ZeroR Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       959\n",
            "         1.0       0.00      0.00      0.00       438\n",
            "\n",
            "    accuracy                           0.69      1397\n",
            "   macro avg       0.34      0.50      0.41      1397\n",
            "weighted avg       0.47      0.69      0.56      1397\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5 train, 0.5 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.5...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.5: 0.9775438596491228\n",
            "DecisionTree Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       979\n",
            "         1.0       0.98      0.95      0.96       446\n",
            "\n",
            "    accuracy                           0.98      1425\n",
            "   macro avg       0.98      0.97      0.97      1425\n",
            "weighted avg       0.98      0.98      0.98      1425\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.5...\n",
            "NaiveBayes Accuracy with split 0.5: 0.7263157894736842\n",
            "NaiveBayes Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.81       979\n",
            "         1.0       0.59      0.40      0.48       446\n",
            "\n",
            "    accuracy                           0.73      1425\n",
            "   macro avg       0.68      0.64      0.65      1425\n",
            "weighted avg       0.71      0.73      0.71      1425\n",
            "\n",
            "\n",
            "Training SVM with split 0.5...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.5: 0.9775438596491228\n",
            "SVM Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       979\n",
            "         1.0       0.97      0.96      0.96       446\n",
            "\n",
            "    accuracy                           0.98      1425\n",
            "   macro avg       0.97      0.97      0.97      1425\n",
            "weighted avg       0.98      0.98      0.98      1425\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.5...\n",
            "ZeroR Accuracy with split 0.5: 0.6870175438596491\n",
            "ZeroR Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       979\n",
            "         1.0       0.00      0.00      0.00       446\n",
            "\n",
            "    accuracy                           0.69      1425\n",
            "   macro avg       0.34      0.50      0.41      1425\n",
            "weighted avg       0.47      0.69      0.56      1425\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.49 train, 0.51 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.51...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.51: 0.9766162310866575\n",
            "DecisionTree Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       999\n",
            "         1.0       0.98      0.95      0.96       455\n",
            "\n",
            "    accuracy                           0.98      1454\n",
            "   macro avg       0.98      0.97      0.97      1454\n",
            "weighted avg       0.98      0.98      0.98      1454\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.51...\n",
            "NaiveBayes Accuracy with split 0.51: 0.7290233837689133\n",
            "NaiveBayes Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.82       999\n",
            "         1.0       0.60      0.39      0.48       455\n",
            "\n",
            "    accuracy                           0.73      1454\n",
            "   macro avg       0.68      0.64      0.65      1454\n",
            "weighted avg       0.71      0.73      0.71      1454\n",
            "\n",
            "\n",
            "Training SVM with split 0.51...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.51: 0.9766162310866575\n",
            "SVM Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       999\n",
            "         1.0       0.98      0.95      0.96       455\n",
            "\n",
            "    accuracy                           0.98      1454\n",
            "   macro avg       0.98      0.97      0.97      1454\n",
            "weighted avg       0.98      0.98      0.98      1454\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.51...\n",
            "ZeroR Accuracy with split 0.51: 0.68707015130674\n",
            "ZeroR Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       999\n",
            "         1.0       0.00      0.00      0.00       455\n",
            "\n",
            "    accuracy                           0.69      1454\n",
            "   macro avg       0.34      0.50      0.41      1454\n",
            "weighted avg       0.47      0.69      0.56      1454\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.48 train, 0.52 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.52...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.52: 0.9763832658569501\n",
            "DecisionTree Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1019\n",
            "         1.0       0.99      0.94      0.96       463\n",
            "\n",
            "    accuracy                           0.98      1482\n",
            "   macro avg       0.98      0.97      0.97      1482\n",
            "weighted avg       0.98      0.98      0.98      1482\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.52...\n",
            "NaiveBayes Accuracy with split 0.52: 0.7240215924426451\n",
            "NaiveBayes Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.88      0.81      1019\n",
            "         1.0       0.59      0.39      0.47       463\n",
            "\n",
            "    accuracy                           0.72      1482\n",
            "   macro avg       0.67      0.63      0.64      1482\n",
            "weighted avg       0.71      0.72      0.71      1482\n",
            "\n",
            "\n",
            "Training SVM with split 0.52...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.52: 0.9763832658569501\n",
            "SVM Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1019\n",
            "         1.0       0.99      0.94      0.96       463\n",
            "\n",
            "    accuracy                           0.98      1482\n",
            "   macro avg       0.98      0.97      0.97      1482\n",
            "weighted avg       0.98      0.98      0.98      1482\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.52...\n",
            "ZeroR Accuracy with split 0.52: 0.6875843454790823\n",
            "ZeroR Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81      1019\n",
            "         1.0       0.00      0.00      0.00       463\n",
            "\n",
            "    accuracy                           0.69      1482\n",
            "   macro avg       0.34      0.50      0.41      1482\n",
            "weighted avg       0.47      0.69      0.56      1482\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.47 train, 0.53 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.53...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.53: 0.9761747187293184\n",
            "DecisionTree Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1045\n",
            "         1.0       0.97      0.95      0.96       466\n",
            "\n",
            "    accuracy                           0.98      1511\n",
            "   macro avg       0.98      0.97      0.97      1511\n",
            "weighted avg       0.98      0.98      0.98      1511\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.53...\n",
            "NaiveBayes Accuracy with split 0.53: 0.7220383851753805\n",
            "NaiveBayes Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.81      1045\n",
            "         1.0       0.57      0.39      0.46       466\n",
            "\n",
            "    accuracy                           0.72      1511\n",
            "   macro avg       0.67      0.63      0.64      1511\n",
            "weighted avg       0.70      0.72      0.70      1511\n",
            "\n",
            "\n",
            "Training SVM with split 0.53...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.53: 0.9761747187293184\n",
            "SVM Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1045\n",
            "         1.0       0.97      0.95      0.96       466\n",
            "\n",
            "    accuracy                           0.98      1511\n",
            "   macro avg       0.98      0.97      0.97      1511\n",
            "weighted avg       0.98      0.98      0.98      1511\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.53...\n",
            "ZeroR Accuracy with split 0.53: 0.6915949702183984\n",
            "ZeroR Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1045\n",
            "         1.0       0.00      0.00      0.00       466\n",
            "\n",
            "    accuracy                           0.69      1511\n",
            "   macro avg       0.35      0.50      0.41      1511\n",
            "weighted avg       0.48      0.69      0.57      1511\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.45999999999999996 train, 0.54 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.54...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.54: 0.9766081871345029\n",
            "DecisionTree Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1070\n",
            "         1.0       0.97      0.95      0.96       469\n",
            "\n",
            "    accuracy                           0.98      1539\n",
            "   macro avg       0.98      0.97      0.97      1539\n",
            "weighted avg       0.98      0.98      0.98      1539\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.54...\n",
            "NaiveBayes Accuracy with split 0.54: 0.7199480181936322\n",
            "NaiveBayes Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.81      1070\n",
            "         1.0       0.56      0.39      0.46       469\n",
            "\n",
            "    accuracy                           0.72      1539\n",
            "   macro avg       0.66      0.63      0.63      1539\n",
            "weighted avg       0.70      0.72      0.70      1539\n",
            "\n",
            "\n",
            "Training SVM with split 0.54...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.54: 0.9766081871345029\n",
            "SVM Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1070\n",
            "         1.0       0.97      0.95      0.96       469\n",
            "\n",
            "    accuracy                           0.98      1539\n",
            "   macro avg       0.98      0.97      0.97      1539\n",
            "weighted avg       0.98      0.98      0.98      1539\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.54...\n",
            "ZeroR Accuracy with split 0.54: 0.6952566601689408\n",
            "ZeroR Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82      1070\n",
            "         1.0       0.00      0.00      0.00       469\n",
            "\n",
            "    accuracy                           0.70      1539\n",
            "   macro avg       0.35      0.50      0.41      1539\n",
            "weighted avg       0.48      0.70      0.57      1539\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.44999999999999996 train, 0.55 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.55...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.55: 0.9770408163265306\n",
            "DecisionTree Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1089\n",
            "         1.0       0.97      0.95      0.96       479\n",
            "\n",
            "    accuracy                           0.98      1568\n",
            "   macro avg       0.98      0.97      0.97      1568\n",
            "weighted avg       0.98      0.98      0.98      1568\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.55...\n",
            "NaiveBayes Accuracy with split 0.55: 0.7257653061224489\n",
            "NaiveBayes Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.87      0.82      1089\n",
            "         1.0       0.58      0.39      0.46       479\n",
            "\n",
            "    accuracy                           0.73      1568\n",
            "   macro avg       0.67      0.63      0.64      1568\n",
            "weighted avg       0.71      0.73      0.71      1568\n",
            "\n",
            "\n",
            "Training SVM with split 0.55...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.55: 0.9770408163265306\n",
            "SVM Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1089\n",
            "         1.0       0.97      0.95      0.96       479\n",
            "\n",
            "    accuracy                           0.98      1568\n",
            "   macro avg       0.98      0.97      0.97      1568\n",
            "weighted avg       0.98      0.98      0.98      1568\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.55...\n",
            "ZeroR Accuracy with split 0.55: 0.6945153061224489\n",
            "ZeroR Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1089\n",
            "         1.0       0.00      0.00      0.00       479\n",
            "\n",
            "    accuracy                           0.69      1568\n",
            "   macro avg       0.35      0.50      0.41      1568\n",
            "weighted avg       0.48      0.69      0.57      1568\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43999999999999995 train, 0.56 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.56...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.56: 0.9774577332498434\n",
            "DecisionTree Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1110\n",
            "         1.0       0.97      0.95      0.96       487\n",
            "\n",
            "    accuracy                           0.98      1597\n",
            "   macro avg       0.98      0.97      0.97      1597\n",
            "weighted avg       0.98      0.98      0.98      1597\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.56...\n",
            "NaiveBayes Accuracy with split 0.56: 0.7451471509079524\n",
            "NaiveBayes Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.89      0.83      1110\n",
            "         1.0       0.62      0.42      0.50       487\n",
            "\n",
            "    accuracy                           0.75      1597\n",
            "   macro avg       0.70      0.65      0.67      1597\n",
            "weighted avg       0.73      0.75      0.73      1597\n",
            "\n",
            "\n",
            "Training SVM with split 0.56...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.56: 0.9774577332498434\n",
            "SVM Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1110\n",
            "         1.0       0.97      0.95      0.96       487\n",
            "\n",
            "    accuracy                           0.98      1597\n",
            "   macro avg       0.98      0.97      0.97      1597\n",
            "weighted avg       0.98      0.98      0.98      1597\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.56...\n",
            "ZeroR Accuracy with split 0.56: 0.6950532247964935\n",
            "ZeroR Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82      1110\n",
            "         1.0       0.00      0.00      0.00       487\n",
            "\n",
            "    accuracy                           0.70      1597\n",
            "   macro avg       0.35      0.50      0.41      1597\n",
            "weighted avg       0.48      0.70      0.57      1597\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43000000000000005 train, 0.57 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.57...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.57: 0.9778461538461538\n",
            "DecisionTree Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1128\n",
            "         1.0       0.98      0.95      0.96       497\n",
            "\n",
            "    accuracy                           0.98      1625\n",
            "   macro avg       0.98      0.97      0.97      1625\n",
            "weighted avg       0.98      0.98      0.98      1625\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.57...\n",
            "NaiveBayes Accuracy with split 0.57: 0.7378461538461538\n",
            "NaiveBayes Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.82      1128\n",
            "         1.0       0.60      0.42      0.50       497\n",
            "\n",
            "    accuracy                           0.74      1625\n",
            "   macro avg       0.69      0.65      0.66      1625\n",
            "weighted avg       0.72      0.74      0.72      1625\n",
            "\n",
            "\n",
            "Training SVM with split 0.57...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.57: 0.9778461538461538\n",
            "SVM Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1128\n",
            "         1.0       0.98      0.95      0.96       497\n",
            "\n",
            "    accuracy                           0.98      1625\n",
            "   macro avg       0.98      0.97      0.97      1625\n",
            "weighted avg       0.98      0.98      0.98      1625\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.57...\n",
            "ZeroR Accuracy with split 0.57: 0.6941538461538461\n",
            "ZeroR Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1128\n",
            "         1.0       0.00      0.00      0.00       497\n",
            "\n",
            "    accuracy                           0.69      1625\n",
            "   macro avg       0.35      0.50      0.41      1625\n",
            "weighted avg       0.48      0.69      0.57      1625\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.42000000000000004 train, 0.58 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.58...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.58: 0.9776164549304295\n",
            "DecisionTree Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1147\n",
            "         1.0       0.98      0.95      0.96       506\n",
            "\n",
            "    accuracy                           0.98      1653\n",
            "   macro avg       0.98      0.97      0.97      1653\n",
            "weighted avg       0.98      0.98      0.98      1653\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.58...\n",
            "NaiveBayes Accuracy with split 0.58: 0.7416817906836055\n",
            "NaiveBayes Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.83      1147\n",
            "         1.0       0.61      0.42      0.50       506\n",
            "\n",
            "    accuracy                           0.74      1653\n",
            "   macro avg       0.69      0.65      0.66      1653\n",
            "weighted avg       0.73      0.74      0.73      1653\n",
            "\n",
            "\n",
            "Training SVM with split 0.58...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.58: 0.9776164549304295\n",
            "SVM Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98      1147\n",
            "         1.0       0.96      0.96      0.96       506\n",
            "\n",
            "    accuracy                           0.98      1653\n",
            "   macro avg       0.97      0.97      0.97      1653\n",
            "weighted avg       0.98      0.98      0.98      1653\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.58...\n",
            "ZeroR Accuracy with split 0.58: 0.6938898971566848\n",
            "ZeroR Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1147\n",
            "         1.0       0.00      0.00      0.00       506\n",
            "\n",
            "    accuracy                           0.69      1653\n",
            "   macro avg       0.35      0.50      0.41      1653\n",
            "weighted avg       0.48      0.69      0.57      1653\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.41000000000000003 train, 0.59 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.59...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.59: 0.9774078478002378\n",
            "DecisionTree Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1167\n",
            "         1.0       0.97      0.95      0.96       515\n",
            "\n",
            "    accuracy                           0.98      1682\n",
            "   macro avg       0.98      0.97      0.97      1682\n",
            "weighted avg       0.98      0.98      0.98      1682\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.59...\n",
            "NaiveBayes Accuracy with split 0.59: 0.7354340071343638\n",
            "NaiveBayes Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82      1167\n",
            "         1.0       0.60      0.43      0.50       515\n",
            "\n",
            "    accuracy                           0.74      1682\n",
            "   macro avg       0.68      0.65      0.66      1682\n",
            "weighted avg       0.72      0.74      0.72      1682\n",
            "\n",
            "\n",
            "Training SVM with split 0.59...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.59: 0.9774078478002378\n",
            "SVM Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98      1167\n",
            "         1.0       0.96      0.96      0.96       515\n",
            "\n",
            "    accuracy                           0.98      1682\n",
            "   macro avg       0.97      0.97      0.97      1682\n",
            "weighted avg       0.98      0.98      0.98      1682\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.59...\n",
            "ZeroR Accuracy with split 0.59: 0.6938168846611177\n",
            "ZeroR Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1167\n",
            "         1.0       0.00      0.00      0.00       515\n",
            "\n",
            "    accuracy                           0.69      1682\n",
            "   macro avg       0.35      0.50      0.41      1682\n",
            "weighted avg       0.48      0.69      0.57      1682\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.4 train, 0.6 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.6...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.6: 0.9777777777777777\n",
            "DecisionTree Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1184\n",
            "         1.0       0.97      0.95      0.96       526\n",
            "\n",
            "    accuracy                           0.98      1710\n",
            "   macro avg       0.98      0.97      0.97      1710\n",
            "weighted avg       0.98      0.98      0.98      1710\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.6...\n",
            "NaiveBayes Accuracy with split 0.6: 0.7391812865497076\n",
            "NaiveBayes Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82      1184\n",
            "         1.0       0.61      0.43      0.50       526\n",
            "\n",
            "    accuracy                           0.74      1710\n",
            "   macro avg       0.69      0.65      0.66      1710\n",
            "weighted avg       0.72      0.74      0.72      1710\n",
            "\n",
            "\n",
            "Training SVM with split 0.6...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.6: 0.9777777777777777\n",
            "SVM Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98      1184\n",
            "         1.0       0.96      0.96      0.96       526\n",
            "\n",
            "    accuracy                           0.98      1710\n",
            "   macro avg       0.97      0.97      0.97      1710\n",
            "weighted avg       0.98      0.98      0.98      1710\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.6...\n",
            "ZeroR Accuracy with split 0.6: 0.6923976608187135\n",
            "ZeroR Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1184\n",
            "         1.0       0.00      0.00      0.00       526\n",
            "\n",
            "    accuracy                           0.69      1710\n",
            "   macro avg       0.35      0.50      0.41      1710\n",
            "weighted avg       0.48      0.69      0.57      1710\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'class'\n",
        "target_column = 'class'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Apply PCA for dimensionality reduction and GINI for feature selection\n",
        "# Standard scaling before PCA is essential\n",
        "#scaler = StandardScaler()\n",
        "#X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Applying PCA (reduce to 95% variance retained)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Duplicate the data 10 times\n",
        "X_duplicated = pd.concat([pd.DataFrame(X_pca)] * 10, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 10, ignore_index=True)\n",
        "\n",
        "# Combine features and target for easier manipulation\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1)\n",
        "\n",
        "# Insert rows ensuring a minimum gap of 3 between duplicates\n",
        "final_data = []\n",
        "for i in range(10):  # Loop over the 10 copies\n",
        "    gap_start = i * 3\n",
        "    final_data[gap_start:gap_start] = df_duplicated.iloc[i * len(df_encoded):(i + 1) * len(df_encoded)].values.tolist()\n",
        "\n",
        "# Convert back to DataFrame after interspersing duplicates\n",
        "df_interspersed = pd.DataFrame(final_data, columns=list(df_duplicated.columns) )\n",
        "\n",
        "# Shuffle the final DataFrame in a highly random order\n",
        "df_shuffled = df_interspersed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_shuffled.drop(target_column, axis=1)\n",
        "y_shuffled = df_shuffled[target_column]\n",
        "\n",
        "# Define classifiers and parameter grids\n",
        "classifiers = {\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'NaiveBayes': GaussianNB(),\n",
        "    'SVM': SVC(),\n",
        "    'ZeroR': DummyClassifier(strategy='most_frequent')\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'DecisionTree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'NaiveBayes': {},  # No hyperparameters to tune for GaussianNB\n",
        "    'ZeroR': {}  # No hyperparameters for DummyClassifier (ZeroR)\n",
        "}\n",
        "\n",
        "# Different train-test split ratios\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        print(f\"\\nTraining {clf_name} with split {split_ratio}...\")\n",
        "\n",
        "        if clf_name in param_grids and param_grids[clf_name]:\n",
        "            # Brute force with GridSearchCV for classifiers with hyperparameters\n",
        "            grid_search = GridSearchCV(clf, param_grids[clf_name], n_jobs=-1, verbose=1)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "        else:\n",
        "            # For classifiers without hyperparameters to tune (like NaiveBayes, ZeroR)\n",
        "            clf.fit(X_train, y_train)\n",
        "            best_model = clf\n",
        "\n",
        "        # Evaluate the best model on the test data\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{clf_name} Accuracy with split {split_ratio}: {accuracy}\")\n",
        "        print(f\"{clf_name} Classification Report with split {split_ratio}:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Store the best model for each classifier and split\n",
        "        best_models[(clf_name, split_ratio)] = best_model\n",
        "\n",
        "# After completing all splits, the best models for each classifier and split will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "PRiGf8naI7Mi",
        "outputId": "62faa5ae-4580-4065-a472-45923a11ea1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating with train_test_split ratio: 0.95 train, 0.05 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.05...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.05: 0.958041958041958\n",
            "DecisionTree Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.05...\n",
            "NaiveBayes Accuracy with split 0.05: 0.7832167832167832\n",
            "NaiveBayes Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.86      0.86       107\n",
            "         1.0       0.57      0.56      0.56        36\n",
            "\n",
            "    accuracy                           0.78       143\n",
            "   macro avg       0.71      0.71      0.71       143\n",
            "weighted avg       0.78      0.78      0.78       143\n",
            "\n",
            "\n",
            "Training SVM with split 0.05...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.05: 0.958041958041958\n",
            "SVM Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.05...\n",
            "ZeroR Accuracy with split 0.05: 0.7482517482517482\n",
            "ZeroR Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      1.00      0.86       107\n",
            "         1.0       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.75       143\n",
            "   macro avg       0.37      0.50      0.43       143\n",
            "weighted avg       0.56      0.75      0.64       143\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.94 train, 0.06 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.06...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.06: 0.9649122807017544\n",
            "DecisionTree Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.06...\n",
            "NaiveBayes Accuracy with split 0.06: 0.7543859649122807\n",
            "NaiveBayes Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.85      0.83       125\n",
            "         1.0       0.55      0.50      0.52        46\n",
            "\n",
            "    accuracy                           0.75       171\n",
            "   macro avg       0.68      0.67      0.68       171\n",
            "weighted avg       0.75      0.75      0.75       171\n",
            "\n",
            "\n",
            "Training SVM with split 0.06...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.06: 0.9649122807017544\n",
            "SVM Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.06...\n",
            "ZeroR Accuracy with split 0.06: 0.7309941520467836\n",
            "ZeroR Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      1.00      0.84       125\n",
            "         1.0       0.00      0.00      0.00        46\n",
            "\n",
            "    accuracy                           0.73       171\n",
            "   macro avg       0.37      0.50      0.42       171\n",
            "weighted avg       0.53      0.73      0.62       171\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9299999999999999 train, 0.07 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.07...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.07: 0.97\n",
            "DecisionTree Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.07...\n",
            "NaiveBayes Accuracy with split 0.07: 0.765\n",
            "NaiveBayes Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.86      0.84       145\n",
            "         1.0       0.58      0.53      0.55        55\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.70      0.69      0.70       200\n",
            "weighted avg       0.76      0.77      0.76       200\n",
            "\n",
            "\n",
            "Training SVM with split 0.07...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.07: 0.97\n",
            "SVM Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.07...\n",
            "ZeroR Accuracy with split 0.07: 0.725\n",
            "ZeroR Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      1.00      0.84       145\n",
            "         1.0       0.00      0.00      0.00        55\n",
            "\n",
            "    accuracy                           0.72       200\n",
            "   macro avg       0.36      0.50      0.42       200\n",
            "weighted avg       0.53      0.72      0.61       200\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.92 train, 0.08 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.08...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.08: 0.9692982456140351\n",
            "DecisionTree Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       162\n",
            "         1.0       0.94      0.95      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.96      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.08...\n",
            "NaiveBayes Accuracy with split 0.08: 0.75\n",
            "NaiveBayes Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.85      0.83       162\n",
            "         1.0       0.58      0.50      0.54        66\n",
            "\n",
            "    accuracy                           0.75       228\n",
            "   macro avg       0.69      0.68      0.68       228\n",
            "weighted avg       0.74      0.75      0.74       228\n",
            "\n",
            "\n",
            "Training SVM with split 0.08...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.08: 0.9692982456140351\n",
            "SVM Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       162\n",
            "         1.0       0.93      0.97      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.97      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.08...\n",
            "ZeroR Accuracy with split 0.08: 0.7105263157894737\n",
            "ZeroR Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       162\n",
            "         1.0       0.00      0.00      0.00        66\n",
            "\n",
            "    accuracy                           0.71       228\n",
            "   macro avg       0.36      0.50      0.42       228\n",
            "weighted avg       0.50      0.71      0.59       228\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.91 train, 0.09 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.09...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.09: 0.9727626459143969\n",
            "DecisionTree Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       182\n",
            "         1.0       0.95      0.96      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.97      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.09...\n",
            "NaiveBayes Accuracy with split 0.09: 0.7587548638132295\n",
            "NaiveBayes Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84       182\n",
            "         1.0       0.61      0.49      0.54        75\n",
            "\n",
            "    accuracy                           0.76       257\n",
            "   macro avg       0.71      0.68      0.69       257\n",
            "weighted avg       0.75      0.76      0.75       257\n",
            "\n",
            "\n",
            "Training SVM with split 0.09...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.09: 0.9727626459143969\n",
            "SVM Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       182\n",
            "         1.0       0.94      0.97      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.96      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.09...\n",
            "ZeroR Accuracy with split 0.09: 0.708171206225681\n",
            "ZeroR Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       182\n",
            "         1.0       0.00      0.00      0.00        75\n",
            "\n",
            "    accuracy                           0.71       257\n",
            "   macro avg       0.35      0.50      0.41       257\n",
            "weighted avg       0.50      0.71      0.59       257\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9 train, 0.1 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.1...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.1: 0.9719298245614035\n",
            "DecisionTree Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       199\n",
            "         1.0       0.94      0.97      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.1...\n",
            "NaiveBayes Accuracy with split 0.1: 0.7508771929824561\n",
            "NaiveBayes Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.83       199\n",
            "         1.0       0.61      0.48      0.54        86\n",
            "\n",
            "    accuracy                           0.75       285\n",
            "   macro avg       0.70      0.67      0.68       285\n",
            "weighted avg       0.74      0.75      0.74       285\n",
            "\n",
            "\n",
            "Training SVM with split 0.1...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.1: 0.9719298245614035\n",
            "SVM Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       199\n",
            "         1.0       0.93      0.98      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.1...\n",
            "ZeroR Accuracy with split 0.1: 0.6982456140350877\n",
            "ZeroR Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       199\n",
            "         1.0       0.00      0.00      0.00        86\n",
            "\n",
            "    accuracy                           0.70       285\n",
            "   macro avg       0.35      0.50      0.41       285\n",
            "weighted avg       0.49      0.70      0.57       285\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.89 train, 0.11 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.11...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.11: 0.9745222929936306\n",
            "DecisionTree Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       220\n",
            "         1.0       0.95      0.97      0.96        94\n",
            "\n",
            "    accuracy                           0.97       314\n",
            "   macro avg       0.97      0.97      0.97       314\n",
            "weighted avg       0.97      0.97      0.97       314\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.11...\n",
            "NaiveBayes Accuracy with split 0.11: 0.7484076433121019\n",
            "NaiveBayes Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.86      0.83       220\n",
            "         1.0       0.60      0.49      0.54        94\n",
            "\n",
            "    accuracy                           0.75       314\n",
            "   macro avg       0.70      0.67      0.68       314\n",
            "weighted avg       0.74      0.75      0.74       314\n",
            "\n",
            "\n",
            "Training SVM with split 0.11...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.11: 0.9745222929936306\n",
            "SVM Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       220\n",
            "         1.0       0.94      0.98      0.96        94\n",
            "\n",
            "    accuracy                           0.97       314\n",
            "   macro avg       0.96      0.98      0.97       314\n",
            "weighted avg       0.98      0.97      0.97       314\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.11...\n",
            "ZeroR Accuracy with split 0.11: 0.7006369426751592\n",
            "ZeroR Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       220\n",
            "         1.0       0.00      0.00      0.00        94\n",
            "\n",
            "    accuracy                           0.70       314\n",
            "   macro avg       0.35      0.50      0.41       314\n",
            "weighted avg       0.49      0.70      0.58       314\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.88 train, 0.12 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.12...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.12: 0.9678362573099415\n",
            "DecisionTree Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       235\n",
            "         1.0       0.94      0.95      0.95       107\n",
            "\n",
            "    accuracy                           0.97       342\n",
            "   macro avg       0.96      0.96      0.96       342\n",
            "weighted avg       0.97      0.97      0.97       342\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.12...\n",
            "NaiveBayes Accuracy with split 0.12: 0.7397660818713451\n",
            "NaiveBayes Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       235\n",
            "         1.0       0.61      0.48      0.53       107\n",
            "\n",
            "    accuracy                           0.74       342\n",
            "   macro avg       0.70      0.67      0.68       342\n",
            "weighted avg       0.73      0.74      0.73       342\n",
            "\n",
            "\n",
            "Training SVM with split 0.12...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.12: 0.9678362573099415\n",
            "SVM Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       235\n",
            "         1.0       0.94      0.95      0.95       107\n",
            "\n",
            "    accuracy                           0.97       342\n",
            "   macro avg       0.96      0.96      0.96       342\n",
            "weighted avg       0.97      0.97      0.97       342\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.12...\n",
            "ZeroR Accuracy with split 0.12: 0.6871345029239766\n",
            "ZeroR Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       235\n",
            "         1.0       0.00      0.00      0.00       107\n",
            "\n",
            "    accuracy                           0.69       342\n",
            "   macro avg       0.34      0.50      0.41       342\n",
            "weighted avg       0.47      0.69      0.56       342\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.87 train, 0.13 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.13...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.13: 0.9703504043126685\n",
            "DecisionTree Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       251\n",
            "         1.0       0.95      0.96      0.95       120\n",
            "\n",
            "    accuracy                           0.97       371\n",
            "   macro avg       0.97      0.97      0.97       371\n",
            "weighted avg       0.97      0.97      0.97       371\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.13...\n",
            "NaiveBayes Accuracy with split 0.13: 0.738544474393531\n",
            "NaiveBayes Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       251\n",
            "         1.0       0.63      0.47      0.54       120\n",
            "\n",
            "    accuracy                           0.74       371\n",
            "   macro avg       0.70      0.67      0.68       371\n",
            "weighted avg       0.73      0.74      0.73       371\n",
            "\n",
            "\n",
            "Training SVM with split 0.13...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.13: 0.9703504043126685\n",
            "SVM Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       251\n",
            "         1.0       0.95      0.96      0.95       120\n",
            "\n",
            "    accuracy                           0.97       371\n",
            "   macro avg       0.97      0.97      0.97       371\n",
            "weighted avg       0.97      0.97      0.97       371\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.13...\n",
            "ZeroR Accuracy with split 0.13: 0.6765498652291105\n",
            "ZeroR Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       251\n",
            "         1.0       0.00      0.00      0.00       120\n",
            "\n",
            "    accuracy                           0.68       371\n",
            "   macro avg       0.34      0.50      0.40       371\n",
            "weighted avg       0.46      0.68      0.55       371\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.86 train, 0.14 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.14...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.14: 0.9725\n",
            "DecisionTree Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       274\n",
            "         1.0       0.95      0.96      0.96       126\n",
            "\n",
            "    accuracy                           0.97       400\n",
            "   macro avg       0.97      0.97      0.97       400\n",
            "weighted avg       0.97      0.97      0.97       400\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.14...\n",
            "NaiveBayes Accuracy with split 0.14: 0.735\n",
            "NaiveBayes Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       274\n",
            "         1.0       0.60      0.47      0.53       126\n",
            "\n",
            "    accuracy                           0.73       400\n",
            "   macro avg       0.69      0.66      0.67       400\n",
            "weighted avg       0.72      0.73      0.72       400\n",
            "\n",
            "\n",
            "Training SVM with split 0.14...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.14: 0.9725\n",
            "SVM Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       274\n",
            "         1.0       0.95      0.96      0.96       126\n",
            "\n",
            "    accuracy                           0.97       400\n",
            "   macro avg       0.97      0.97      0.97       400\n",
            "weighted avg       0.97      0.97      0.97       400\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.14...\n",
            "ZeroR Accuracy with split 0.14: 0.685\n",
            "ZeroR Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       274\n",
            "         1.0       0.00      0.00      0.00       126\n",
            "\n",
            "    accuracy                           0.69       400\n",
            "   macro avg       0.34      0.50      0.41       400\n",
            "weighted avg       0.47      0.69      0.56       400\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.85 train, 0.15 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.15...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.15: 0.969626168224299\n",
            "DecisionTree Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       293\n",
            "         1.0       0.94      0.96      0.95       135\n",
            "\n",
            "    accuracy                           0.97       428\n",
            "   macro avg       0.96      0.97      0.97       428\n",
            "weighted avg       0.97      0.97      0.97       428\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.15...\n",
            "NaiveBayes Accuracy with split 0.15: 0.735981308411215\n",
            "NaiveBayes Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       293\n",
            "         1.0       0.60      0.47      0.53       135\n",
            "\n",
            "    accuracy                           0.74       428\n",
            "   macro avg       0.69      0.67      0.67       428\n",
            "weighted avg       0.72      0.74      0.73       428\n",
            "\n",
            "\n",
            "Training SVM with split 0.15...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.15: 0.969626168224299\n",
            "SVM Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       293\n",
            "         1.0       0.94      0.96      0.95       135\n",
            "\n",
            "    accuracy                           0.97       428\n",
            "   macro avg       0.96      0.97      0.97       428\n",
            "weighted avg       0.97      0.97      0.97       428\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.15...\n",
            "ZeroR Accuracy with split 0.15: 0.6845794392523364\n",
            "ZeroR Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       293\n",
            "         1.0       0.00      0.00      0.00       135\n",
            "\n",
            "    accuracy                           0.68       428\n",
            "   macro avg       0.34      0.50      0.41       428\n",
            "weighted avg       0.47      0.68      0.56       428\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.84 train, 0.16 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.16...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.16: 0.9671052631578947\n",
            "DecisionTree Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       315\n",
            "         1.0       0.93      0.96      0.95       141\n",
            "\n",
            "    accuracy                           0.97       456\n",
            "   macro avg       0.96      0.97      0.96       456\n",
            "weighted avg       0.97      0.97      0.97       456\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.16...\n",
            "NaiveBayes Accuracy with split 0.16: 0.7412280701754386\n",
            "NaiveBayes Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       315\n",
            "         1.0       0.61      0.47      0.53       141\n",
            "\n",
            "    accuracy                           0.74       456\n",
            "   macro avg       0.69      0.67      0.67       456\n",
            "weighted avg       0.73      0.74      0.73       456\n",
            "\n",
            "\n",
            "Training SVM with split 0.16...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.16: 0.9671052631578947\n",
            "SVM Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       315\n",
            "         1.0       0.93      0.96      0.95       141\n",
            "\n",
            "    accuracy                           0.97       456\n",
            "   macro avg       0.96      0.97      0.96       456\n",
            "weighted avg       0.97      0.97      0.97       456\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.16...\n",
            "ZeroR Accuracy with split 0.16: 0.6907894736842105\n",
            "ZeroR Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       315\n",
            "         1.0       0.00      0.00      0.00       141\n",
            "\n",
            "    accuracy                           0.69       456\n",
            "   macro avg       0.35      0.50      0.41       456\n",
            "weighted avg       0.48      0.69      0.56       456\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.83 train, 0.17 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.17...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.17: 0.9670103092783505\n",
            "DecisionTree Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       335\n",
            "         1.0       0.94      0.95      0.95       150\n",
            "\n",
            "    accuracy                           0.97       485\n",
            "   macro avg       0.96      0.96      0.96       485\n",
            "weighted avg       0.97      0.97      0.97       485\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.17...\n",
            "NaiveBayes Accuracy with split 0.17: 0.7422680412371134\n",
            "NaiveBayes Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       335\n",
            "         1.0       0.61      0.47      0.53       150\n",
            "\n",
            "    accuracy                           0.74       485\n",
            "   macro avg       0.70      0.67      0.68       485\n",
            "weighted avg       0.73      0.74      0.73       485\n",
            "\n",
            "\n",
            "Training SVM with split 0.17...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.17: 0.9670103092783505\n",
            "SVM Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       335\n",
            "         1.0       0.94      0.95      0.95       150\n",
            "\n",
            "    accuracy                           0.97       485\n",
            "   macro avg       0.96      0.96      0.96       485\n",
            "weighted avg       0.97      0.97      0.97       485\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.17...\n",
            "ZeroR Accuracy with split 0.17: 0.6907216494845361\n",
            "ZeroR Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       335\n",
            "         1.0       0.00      0.00      0.00       150\n",
            "\n",
            "    accuracy                           0.69       485\n",
            "   macro avg       0.35      0.50      0.41       485\n",
            "weighted avg       0.48      0.69      0.56       485\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8200000000000001 train, 0.18 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.18...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.18: 0.9688109161793372\n",
            "DecisionTree Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       352\n",
            "         1.0       0.94      0.96      0.95       161\n",
            "\n",
            "    accuracy                           0.97       513\n",
            "   macro avg       0.96      0.97      0.96       513\n",
            "weighted avg       0.97      0.97      0.97       513\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.18...\n",
            "NaiveBayes Accuracy with split 0.18: 0.7309941520467836\n",
            "NaiveBayes Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.86      0.82       352\n",
            "         1.0       0.60      0.44      0.51       161\n",
            "\n",
            "    accuracy                           0.73       513\n",
            "   macro avg       0.68      0.65      0.66       513\n",
            "weighted avg       0.72      0.73      0.72       513\n",
            "\n",
            "\n",
            "Training SVM with split 0.18...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.18: 0.9688109161793372\n",
            "SVM Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       352\n",
            "         1.0       0.94      0.96      0.95       161\n",
            "\n",
            "    accuracy                           0.97       513\n",
            "   macro avg       0.96      0.97      0.96       513\n",
            "weighted avg       0.97      0.97      0.97       513\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.18...\n",
            "ZeroR Accuracy with split 0.18: 0.6861598440545809\n",
            "ZeroR Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       352\n",
            "         1.0       0.00      0.00      0.00       161\n",
            "\n",
            "    accuracy                           0.69       513\n",
            "   macro avg       0.34      0.50      0.41       513\n",
            "weighted avg       0.47      0.69      0.56       513\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.81 train, 0.19 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.19...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.19: 0.9704797047970479\n",
            "DecisionTree Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       370\n",
            "         1.0       0.95      0.96      0.95       172\n",
            "\n",
            "    accuracy                           0.97       542\n",
            "   macro avg       0.96      0.97      0.97       542\n",
            "weighted avg       0.97      0.97      0.97       542\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.19...\n",
            "NaiveBayes Accuracy with split 0.19: 0.7380073800738007\n",
            "NaiveBayes Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       370\n",
            "         1.0       0.61      0.47      0.53       172\n",
            "\n",
            "    accuracy                           0.74       542\n",
            "   macro avg       0.70      0.67      0.68       542\n",
            "weighted avg       0.73      0.74      0.73       542\n",
            "\n",
            "\n",
            "Training SVM with split 0.19...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.19: 0.9704797047970479\n",
            "SVM Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       370\n",
            "         1.0       0.95      0.96      0.95       172\n",
            "\n",
            "    accuracy                           0.97       542\n",
            "   macro avg       0.96      0.97      0.97       542\n",
            "weighted avg       0.97      0.97      0.97       542\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.19...\n",
            "ZeroR Accuracy with split 0.19: 0.6826568265682657\n",
            "ZeroR Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       370\n",
            "         1.0       0.00      0.00      0.00       172\n",
            "\n",
            "    accuracy                           0.68       542\n",
            "   macro avg       0.34      0.50      0.41       542\n",
            "weighted avg       0.47      0.68      0.55       542\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8 train, 0.2 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.2...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.2: 0.9701754385964912\n",
            "DecisionTree Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       394\n",
            "         1.0       0.93      0.97      0.95       176\n",
            "\n",
            "    accuracy                           0.97       570\n",
            "   macro avg       0.96      0.97      0.97       570\n",
            "weighted avg       0.97      0.97      0.97       570\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.2...\n",
            "NaiveBayes Accuracy with split 0.2: 0.7421052631578947\n",
            "NaiveBayes Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       394\n",
            "         1.0       0.61      0.47      0.53       176\n",
            "\n",
            "    accuracy                           0.74       570\n",
            "   macro avg       0.70      0.67      0.68       570\n",
            "weighted avg       0.73      0.74      0.73       570\n",
            "\n",
            "\n",
            "Training SVM with split 0.2...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.2: 0.9701754385964912\n",
            "SVM Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       394\n",
            "         1.0       0.93      0.97      0.95       176\n",
            "\n",
            "    accuracy                           0.97       570\n",
            "   macro avg       0.96      0.97      0.97       570\n",
            "weighted avg       0.97      0.97      0.97       570\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.2...\n",
            "ZeroR Accuracy with split 0.2: 0.6912280701754386\n",
            "ZeroR Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       394\n",
            "         1.0       0.00      0.00      0.00       176\n",
            "\n",
            "    accuracy                           0.69       570\n",
            "   macro avg       0.35      0.50      0.41       570\n",
            "weighted avg       0.48      0.69      0.57       570\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.79 train, 0.21 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.21...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.21: 0.9699499165275459\n",
            "DecisionTree Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       416\n",
            "         1.0       0.94      0.97      0.95       183\n",
            "\n",
            "    accuracy                           0.97       599\n",
            "   macro avg       0.96      0.97      0.96       599\n",
            "weighted avg       0.97      0.97      0.97       599\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.21...\n",
            "NaiveBayes Accuracy with split 0.21: 0.7378964941569283\n",
            "NaiveBayes Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       416\n",
            "         1.0       0.59      0.45      0.51       183\n",
            "\n",
            "    accuracy                           0.74       599\n",
            "   macro avg       0.69      0.66      0.67       599\n",
            "weighted avg       0.72      0.74      0.73       599\n",
            "\n",
            "\n",
            "Training SVM with split 0.21...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.21: 0.9699499165275459\n",
            "SVM Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       416\n",
            "         1.0       0.94      0.97      0.95       183\n",
            "\n",
            "    accuracy                           0.97       599\n",
            "   macro avg       0.96      0.97      0.96       599\n",
            "weighted avg       0.97      0.97      0.97       599\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.21...\n",
            "ZeroR Accuracy with split 0.21: 0.6944908180300501\n",
            "ZeroR Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       416\n",
            "         1.0       0.00      0.00      0.00       183\n",
            "\n",
            "    accuracy                           0.69       599\n",
            "   macro avg       0.35      0.50      0.41       599\n",
            "weighted avg       0.48      0.69      0.57       599\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.78 train, 0.22 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.22...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.22: 0.9681020733652312\n",
            "DecisionTree Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       438\n",
            "         1.0       0.93      0.96      0.95       189\n",
            "\n",
            "    accuracy                           0.97       627\n",
            "   macro avg       0.96      0.97      0.96       627\n",
            "weighted avg       0.97      0.97      0.97       627\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.22...\n",
            "NaiveBayes Accuracy with split 0.22: 0.7384370015948963\n",
            "NaiveBayes Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       438\n",
            "         1.0       0.58      0.47      0.52       189\n",
            "\n",
            "    accuracy                           0.74       627\n",
            "   macro avg       0.69      0.66      0.67       627\n",
            "weighted avg       0.73      0.74      0.73       627\n",
            "\n",
            "\n",
            "Training SVM with split 0.22...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.22: 0.9681020733652312\n",
            "SVM Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       438\n",
            "         1.0       0.93      0.96      0.95       189\n",
            "\n",
            "    accuracy                           0.97       627\n",
            "   macro avg       0.96      0.97      0.96       627\n",
            "weighted avg       0.97      0.97      0.97       627\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.22...\n",
            "ZeroR Accuracy with split 0.22: 0.6985645933014354\n",
            "ZeroR Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       438\n",
            "         1.0       0.00      0.00      0.00       189\n",
            "\n",
            "    accuracy                           0.70       627\n",
            "   macro avg       0.35      0.50      0.41       627\n",
            "weighted avg       0.49      0.70      0.57       627\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.77 train, 0.23 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.23...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.23: 0.9679878048780488\n",
            "DecisionTree Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       459\n",
            "         1.0       0.94      0.96      0.95       197\n",
            "\n",
            "    accuracy                           0.97       656\n",
            "   macro avg       0.96      0.97      0.96       656\n",
            "weighted avg       0.97      0.97      0.97       656\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.23...\n",
            "NaiveBayes Accuracy with split 0.23: 0.7423780487804879\n",
            "NaiveBayes Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.82       459\n",
            "         1.0       0.59      0.45      0.51       197\n",
            "\n",
            "    accuracy                           0.74       656\n",
            "   macro avg       0.69      0.66      0.67       656\n",
            "weighted avg       0.73      0.74      0.73       656\n",
            "\n",
            "\n",
            "Training SVM with split 0.23...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.23: 0.9679878048780488\n",
            "SVM Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       459\n",
            "         1.0       0.94      0.96      0.95       197\n",
            "\n",
            "    accuracy                           0.97       656\n",
            "   macro avg       0.96      0.97      0.96       656\n",
            "weighted avg       0.97      0.97      0.97       656\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.23...\n",
            "ZeroR Accuracy with split 0.23: 0.6996951219512195\n",
            "ZeroR Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       459\n",
            "         1.0       0.00      0.00      0.00       197\n",
            "\n",
            "    accuracy                           0.70       656\n",
            "   macro avg       0.35      0.50      0.41       656\n",
            "weighted avg       0.49      0.70      0.58       656\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.76 train, 0.24 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.24...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.24: 0.9692982456140351\n",
            "DecisionTree Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       477\n",
            "         1.0       0.94      0.96      0.95       207\n",
            "\n",
            "    accuracy                           0.97       684\n",
            "   macro avg       0.96      0.97      0.96       684\n",
            "weighted avg       0.97      0.97      0.97       684\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.24...\n",
            "NaiveBayes Accuracy with split 0.24: 0.7412280701754386\n",
            "NaiveBayes Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.82       477\n",
            "         1.0       0.59      0.45      0.52       207\n",
            "\n",
            "    accuracy                           0.74       684\n",
            "   macro avg       0.69      0.66      0.67       684\n",
            "weighted avg       0.73      0.74      0.73       684\n",
            "\n",
            "\n",
            "Training SVM with split 0.24...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.24: 0.9692982456140351\n",
            "SVM Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       477\n",
            "         1.0       0.94      0.96      0.95       207\n",
            "\n",
            "    accuracy                           0.97       684\n",
            "   macro avg       0.96      0.97      0.96       684\n",
            "weighted avg       0.97      0.97      0.97       684\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.24...\n",
            "ZeroR Accuracy with split 0.24: 0.6973684210526315\n",
            "ZeroR Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       477\n",
            "         1.0       0.00      0.00      0.00       207\n",
            "\n",
            "    accuracy                           0.70       684\n",
            "   macro avg       0.35      0.50      0.41       684\n",
            "weighted avg       0.49      0.70      0.57       684\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.75 train, 0.25 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.25...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.25: 0.97054698457223\n",
            "DecisionTree Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       495\n",
            "         1.0       0.94      0.96      0.95       218\n",
            "\n",
            "    accuracy                           0.97       713\n",
            "   macro avg       0.96      0.97      0.97       713\n",
            "weighted avg       0.97      0.97      0.97       713\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.25...\n",
            "NaiveBayes Accuracy with split 0.25: 0.7349228611500701\n",
            "NaiveBayes Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       495\n",
            "         1.0       0.59      0.44      0.51       218\n",
            "\n",
            "    accuracy                           0.73       713\n",
            "   macro avg       0.68      0.65      0.66       713\n",
            "weighted avg       0.72      0.73      0.72       713\n",
            "\n",
            "\n",
            "Training SVM with split 0.25...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.25: 0.97054698457223\n",
            "SVM Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       495\n",
            "         1.0       0.94      0.96      0.95       218\n",
            "\n",
            "    accuracy                           0.97       713\n",
            "   macro avg       0.96      0.97      0.97       713\n",
            "weighted avg       0.97      0.97      0.97       713\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.25...\n",
            "ZeroR Accuracy with split 0.25: 0.6942496493688639\n",
            "ZeroR Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       495\n",
            "         1.0       0.00      0.00      0.00       218\n",
            "\n",
            "    accuracy                           0.69       713\n",
            "   macro avg       0.35      0.50      0.41       713\n",
            "weighted avg       0.48      0.69      0.57       713\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.74 train, 0.26 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.26...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.26: 0.970310391363023\n",
            "DecisionTree Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       518\n",
            "         1.0       0.94      0.96      0.95       223\n",
            "\n",
            "    accuracy                           0.97       741\n",
            "   macro avg       0.96      0.97      0.96       741\n",
            "weighted avg       0.97      0.97      0.97       741\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.26...\n",
            "NaiveBayes Accuracy with split 0.26: 0.7368421052631579\n",
            "NaiveBayes Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       518\n",
            "         1.0       0.58      0.44      0.50       223\n",
            "\n",
            "    accuracy                           0.74       741\n",
            "   macro avg       0.68      0.65      0.66       741\n",
            "weighted avg       0.72      0.74      0.73       741\n",
            "\n",
            "\n",
            "Training SVM with split 0.26...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.26: 0.970310391363023\n",
            "SVM Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       518\n",
            "         1.0       0.94      0.96      0.95       223\n",
            "\n",
            "    accuracy                           0.97       741\n",
            "   macro avg       0.96      0.97      0.96       741\n",
            "weighted avg       0.97      0.97      0.97       741\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.26...\n",
            "ZeroR Accuracy with split 0.26: 0.699055330634278\n",
            "ZeroR Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       518\n",
            "         1.0       0.00      0.00      0.00       223\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.35      0.50      0.41       741\n",
            "weighted avg       0.49      0.70      0.58       741\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.73 train, 0.27 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.27...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.27: 0.9714285714285714\n",
            "DecisionTree Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       535\n",
            "         1.0       0.95      0.95      0.95       235\n",
            "\n",
            "    accuracy                           0.97       770\n",
            "   macro avg       0.97      0.97      0.97       770\n",
            "weighted avg       0.97      0.97      0.97       770\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.27...\n",
            "NaiveBayes Accuracy with split 0.27: 0.7363636363636363\n",
            "NaiveBayes Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       535\n",
            "         1.0       0.59      0.44      0.51       235\n",
            "\n",
            "    accuracy                           0.74       770\n",
            "   macro avg       0.69      0.65      0.66       770\n",
            "weighted avg       0.72      0.74      0.72       770\n",
            "\n",
            "\n",
            "Training SVM with split 0.27...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.27: 0.9714285714285714\n",
            "SVM Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       535\n",
            "         1.0       0.95      0.95      0.95       235\n",
            "\n",
            "    accuracy                           0.97       770\n",
            "   macro avg       0.97      0.97      0.97       770\n",
            "weighted avg       0.97      0.97      0.97       770\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.27...\n",
            "ZeroR Accuracy with split 0.27: 0.6948051948051948\n",
            "ZeroR Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       535\n",
            "         1.0       0.00      0.00      0.00       235\n",
            "\n",
            "    accuracy                           0.69       770\n",
            "   macro avg       0.35      0.50      0.41       770\n",
            "weighted avg       0.48      0.69      0.57       770\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.72 train, 0.28 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.28...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.28: 0.9724655819774718\n",
            "DecisionTree Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       556\n",
            "         1.0       0.95      0.95      0.95       243\n",
            "\n",
            "    accuracy                           0.97       799\n",
            "   macro avg       0.97      0.97      0.97       799\n",
            "weighted avg       0.97      0.97      0.97       799\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.28...\n",
            "NaiveBayes Accuracy with split 0.28: 0.7396745932415519\n",
            "NaiveBayes Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       556\n",
            "         1.0       0.60      0.45      0.51       243\n",
            "\n",
            "    accuracy                           0.74       799\n",
            "   macro avg       0.69      0.66      0.67       799\n",
            "weighted avg       0.73      0.74      0.73       799\n",
            "\n",
            "\n",
            "Training SVM with split 0.28...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.28: 0.9724655819774718\n",
            "SVM Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       556\n",
            "         1.0       0.95      0.95      0.95       243\n",
            "\n",
            "    accuracy                           0.97       799\n",
            "   macro avg       0.97      0.97      0.97       799\n",
            "weighted avg       0.97      0.97      0.97       799\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.28...\n",
            "ZeroR Accuracy with split 0.28: 0.6958698372966208\n",
            "ZeroR Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       556\n",
            "         1.0       0.00      0.00      0.00       243\n",
            "\n",
            "    accuracy                           0.70       799\n",
            "   macro avg       0.35      0.50      0.41       799\n",
            "weighted avg       0.48      0.70      0.57       799\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.71 train, 0.29 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.29...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.29: 0.973397823458283\n",
            "DecisionTree Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       574\n",
            "         1.0       0.96      0.96      0.96       253\n",
            "\n",
            "    accuracy                           0.97       827\n",
            "   macro avg       0.97      0.97      0.97       827\n",
            "weighted avg       0.97      0.97      0.97       827\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.29...\n",
            "NaiveBayes Accuracy with split 0.29: 0.7424425634824667\n",
            "NaiveBayes Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       574\n",
            "         1.0       0.61      0.45      0.52       253\n",
            "\n",
            "    accuracy                           0.74       827\n",
            "   macro avg       0.69      0.66      0.67       827\n",
            "weighted avg       0.73      0.74      0.73       827\n",
            "\n",
            "\n",
            "Training SVM with split 0.29...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.29: 0.973397823458283\n",
            "SVM Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       574\n",
            "         1.0       0.96      0.96      0.96       253\n",
            "\n",
            "    accuracy                           0.97       827\n",
            "   macro avg       0.97      0.97      0.97       827\n",
            "weighted avg       0.97      0.97      0.97       827\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.29...\n",
            "ZeroR Accuracy with split 0.29: 0.694074969770254\n",
            "ZeroR Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       574\n",
            "         1.0       0.00      0.00      0.00       253\n",
            "\n",
            "    accuracy                           0.69       827\n",
            "   macro avg       0.35      0.50      0.41       827\n",
            "weighted avg       0.48      0.69      0.57       827\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.7 train, 0.3 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.3...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.3: 0.9742690058479532\n",
            "DecisionTree Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       592\n",
            "         1.0       0.96      0.96      0.96       263\n",
            "\n",
            "    accuracy                           0.97       855\n",
            "   macro avg       0.97      0.97      0.97       855\n",
            "weighted avg       0.97      0.97      0.97       855\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.3...\n",
            "NaiveBayes Accuracy with split 0.3: 0.7380116959064328\n",
            "NaiveBayes Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       592\n",
            "         1.0       0.60      0.45      0.52       263\n",
            "\n",
            "    accuracy                           0.74       855\n",
            "   macro avg       0.69      0.66      0.67       855\n",
            "weighted avg       0.72      0.74      0.73       855\n",
            "\n",
            "\n",
            "Training SVM with split 0.3...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.3: 0.9742690058479532\n",
            "SVM Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       592\n",
            "         1.0       0.96      0.96      0.96       263\n",
            "\n",
            "    accuracy                           0.97       855\n",
            "   macro avg       0.97      0.97      0.97       855\n",
            "weighted avg       0.97      0.97      0.97       855\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.3...\n",
            "ZeroR Accuracy with split 0.3: 0.6923976608187135\n",
            "ZeroR Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       592\n",
            "         1.0       0.00      0.00      0.00       263\n",
            "\n",
            "    accuracy                           0.69       855\n",
            "   macro avg       0.35      0.50      0.41       855\n",
            "weighted avg       0.48      0.69      0.57       855\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.69 train, 0.31 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.31...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.31: 0.9728506787330317\n",
            "DecisionTree Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       611\n",
            "         1.0       0.96      0.96      0.96       273\n",
            "\n",
            "    accuracy                           0.97       884\n",
            "   macro avg       0.97      0.97      0.97       884\n",
            "weighted avg       0.97      0.97      0.97       884\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.31...\n",
            "NaiveBayes Accuracy with split 0.31: 0.7375565610859729\n",
            "NaiveBayes Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       611\n",
            "         1.0       0.60      0.45      0.52       273\n",
            "\n",
            "    accuracy                           0.74       884\n",
            "   macro avg       0.69      0.66      0.67       884\n",
            "weighted avg       0.72      0.74      0.73       884\n",
            "\n",
            "\n",
            "Training SVM with split 0.31...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.31: 0.9728506787330317\n",
            "SVM Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       611\n",
            "         1.0       0.96      0.96      0.96       273\n",
            "\n",
            "    accuracy                           0.97       884\n",
            "   macro avg       0.97      0.97      0.97       884\n",
            "weighted avg       0.97      0.97      0.97       884\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.31...\n",
            "ZeroR Accuracy with split 0.31: 0.6911764705882353\n",
            "ZeroR Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       611\n",
            "         1.0       0.00      0.00      0.00       273\n",
            "\n",
            "    accuracy                           0.69       884\n",
            "   macro avg       0.35      0.50      0.41       884\n",
            "weighted avg       0.48      0.69      0.56       884\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6799999999999999 train, 0.32 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.32...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.32: 0.9725877192982456\n",
            "DecisionTree Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       629\n",
            "         1.0       0.96      0.95      0.96       283\n",
            "\n",
            "    accuracy                           0.97       912\n",
            "   macro avg       0.97      0.97      0.97       912\n",
            "weighted avg       0.97      0.97      0.97       912\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.32...\n",
            "NaiveBayes Accuracy with split 0.32: 0.743421052631579\n",
            "NaiveBayes Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       629\n",
            "         1.0       0.61      0.48      0.54       283\n",
            "\n",
            "    accuracy                           0.74       912\n",
            "   macro avg       0.70      0.67      0.68       912\n",
            "weighted avg       0.73      0.74      0.73       912\n",
            "\n",
            "\n",
            "Training SVM with split 0.32...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.32: 0.9725877192982456\n",
            "SVM Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       629\n",
            "         1.0       0.96      0.95      0.96       283\n",
            "\n",
            "    accuracy                           0.97       912\n",
            "   macro avg       0.97      0.97      0.97       912\n",
            "weighted avg       0.97      0.97      0.97       912\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.32...\n",
            "ZeroR Accuracy with split 0.32: 0.6896929824561403\n",
            "ZeroR Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       629\n",
            "         1.0       0.00      0.00      0.00       283\n",
            "\n",
            "    accuracy                           0.69       912\n",
            "   macro avg       0.34      0.50      0.41       912\n",
            "weighted avg       0.48      0.69      0.56       912\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6699999999999999 train, 0.33 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.33...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.33: 0.973432518597237\n",
            "DecisionTree Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       649\n",
            "         1.0       0.96      0.96      0.96       292\n",
            "\n",
            "    accuracy                           0.97       941\n",
            "   macro avg       0.97      0.97      0.97       941\n",
            "weighted avg       0.97      0.97      0.97       941\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.33...\n",
            "NaiveBayes Accuracy with split 0.33: 0.7523910733262487\n",
            "NaiveBayes Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.83       649\n",
            "         1.0       0.63      0.50      0.55       292\n",
            "\n",
            "    accuracy                           0.75       941\n",
            "   macro avg       0.71      0.68      0.69       941\n",
            "weighted avg       0.74      0.75      0.74       941\n",
            "\n",
            "\n",
            "Training SVM with split 0.33...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.33: 0.973432518597237\n",
            "SVM Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       649\n",
            "         1.0       0.96      0.96      0.96       292\n",
            "\n",
            "    accuracy                           0.97       941\n",
            "   macro avg       0.97      0.97      0.97       941\n",
            "weighted avg       0.97      0.97      0.97       941\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.33...\n",
            "ZeroR Accuracy with split 0.33: 0.6896918172157279\n",
            "ZeroR Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       649\n",
            "         1.0       0.00      0.00      0.00       292\n",
            "\n",
            "    accuracy                           0.69       941\n",
            "   macro avg       0.34      0.50      0.41       941\n",
            "weighted avg       0.48      0.69      0.56       941\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6599999999999999 train, 0.34 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.34...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.34: 0.9742268041237113\n",
            "DecisionTree Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       669\n",
            "         1.0       0.96      0.96      0.96       301\n",
            "\n",
            "    accuracy                           0.97       970\n",
            "   macro avg       0.97      0.97      0.97       970\n",
            "weighted avg       0.97      0.97      0.97       970\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.34...\n",
            "NaiveBayes Accuracy with split 0.34: 0.7422680412371134\n",
            "NaiveBayes Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       669\n",
            "         1.0       0.61      0.49      0.54       301\n",
            "\n",
            "    accuracy                           0.74       970\n",
            "   macro avg       0.70      0.67      0.68       970\n",
            "weighted avg       0.73      0.74      0.73       970\n",
            "\n",
            "\n",
            "Training SVM with split 0.34...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.34: 0.9742268041237113\n",
            "SVM Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       669\n",
            "         1.0       0.96      0.96      0.96       301\n",
            "\n",
            "    accuracy                           0.97       970\n",
            "   macro avg       0.97      0.97      0.97       970\n",
            "weighted avg       0.97      0.97      0.97       970\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.34...\n",
            "ZeroR Accuracy with split 0.34: 0.6896907216494845\n",
            "ZeroR Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       669\n",
            "         1.0       0.00      0.00      0.00       301\n",
            "\n",
            "    accuracy                           0.69       970\n",
            "   macro avg       0.34      0.50      0.41       970\n",
            "weighted avg       0.48      0.69      0.56       970\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.65 train, 0.35 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.35...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.35: 0.9749498997995992\n",
            "DecisionTree Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       683\n",
            "         1.0       0.96      0.96      0.96       315\n",
            "\n",
            "    accuracy                           0.97       998\n",
            "   macro avg       0.97      0.97      0.97       998\n",
            "weighted avg       0.97      0.97      0.97       998\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.35...\n",
            "NaiveBayes Accuracy with split 0.35: 0.7404809619238477\n",
            "NaiveBayes Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       683\n",
            "         1.0       0.61      0.48      0.54       315\n",
            "\n",
            "    accuracy                           0.74       998\n",
            "   macro avg       0.70      0.67      0.68       998\n",
            "weighted avg       0.73      0.74      0.73       998\n",
            "\n",
            "\n",
            "Training SVM with split 0.35...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.35: 0.9749498997995992\n",
            "SVM Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       683\n",
            "         1.0       0.96      0.96      0.96       315\n",
            "\n",
            "    accuracy                           0.97       998\n",
            "   macro avg       0.97      0.97      0.97       998\n",
            "weighted avg       0.97      0.97      0.97       998\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.35...\n",
            "ZeroR Accuracy with split 0.35: 0.6843687374749499\n",
            "ZeroR Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       683\n",
            "         1.0       0.00      0.00      0.00       315\n",
            "\n",
            "    accuracy                           0.68       998\n",
            "   macro avg       0.34      0.50      0.41       998\n",
            "weighted avg       0.47      0.68      0.56       998\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.64 train, 0.36 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.36...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.36: 0.9756335282651072\n",
            "DecisionTree Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       701\n",
            "         1.0       0.96      0.96      0.96       325\n",
            "\n",
            "    accuracy                           0.98      1026\n",
            "   macro avg       0.97      0.97      0.97      1026\n",
            "weighted avg       0.98      0.98      0.98      1026\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.36...\n",
            "NaiveBayes Accuracy with split 0.36: 0.7456140350877193\n",
            "NaiveBayes Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       701\n",
            "         1.0       0.62      0.49      0.55       325\n",
            "\n",
            "    accuracy                           0.75      1026\n",
            "   macro avg       0.71      0.68      0.69      1026\n",
            "weighted avg       0.73      0.75      0.74      1026\n",
            "\n",
            "\n",
            "Training SVM with split 0.36...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.36: 0.9756335282651072\n",
            "SVM Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       701\n",
            "         1.0       0.96      0.96      0.96       325\n",
            "\n",
            "    accuracy                           0.98      1026\n",
            "   macro avg       0.97      0.97      0.97      1026\n",
            "weighted avg       0.98      0.98      0.98      1026\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.36...\n",
            "ZeroR Accuracy with split 0.36: 0.6832358674463938\n",
            "ZeroR Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       701\n",
            "         1.0       0.00      0.00      0.00       325\n",
            "\n",
            "    accuracy                           0.68      1026\n",
            "   macro avg       0.34      0.50      0.41      1026\n",
            "weighted avg       0.47      0.68      0.55      1026\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.63 train, 0.37 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.37...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.37: 0.9753554502369668\n",
            "DecisionTree Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       720\n",
            "         1.0       0.96      0.96      0.96       335\n",
            "\n",
            "    accuracy                           0.98      1055\n",
            "   macro avg       0.97      0.97      0.97      1055\n",
            "weighted avg       0.98      0.98      0.98      1055\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.37...\n",
            "NaiveBayes Accuracy with split 0.37: 0.7450236966824645\n",
            "NaiveBayes Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       720\n",
            "         1.0       0.63      0.49      0.55       335\n",
            "\n",
            "    accuracy                           0.75      1055\n",
            "   macro avg       0.71      0.68      0.69      1055\n",
            "weighted avg       0.73      0.75      0.74      1055\n",
            "\n",
            "\n",
            "Training SVM with split 0.37...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.37: 0.9753554502369668\n",
            "SVM Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       720\n",
            "         1.0       0.95      0.97      0.96       335\n",
            "\n",
            "    accuracy                           0.98      1055\n",
            "   macro avg       0.97      0.97      0.97      1055\n",
            "weighted avg       0.98      0.98      0.98      1055\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.37...\n",
            "ZeroR Accuracy with split 0.37: 0.6824644549763034\n",
            "ZeroR Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       720\n",
            "         1.0       0.00      0.00      0.00       335\n",
            "\n",
            "    accuracy                           0.68      1055\n",
            "   macro avg       0.34      0.50      0.41      1055\n",
            "weighted avg       0.47      0.68      0.55      1055\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.62 train, 0.38 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.38...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.38: 0.9759926131117267\n",
            "DecisionTree Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       739\n",
            "         1.0       0.96      0.96      0.96       344\n",
            "\n",
            "    accuracy                           0.98      1083\n",
            "   macro avg       0.97      0.97      0.97      1083\n",
            "weighted avg       0.98      0.98      0.98      1083\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.38...\n",
            "NaiveBayes Accuracy with split 0.38: 0.7460757156048015\n",
            "NaiveBayes Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       739\n",
            "         1.0       0.63      0.49      0.55       344\n",
            "\n",
            "    accuracy                           0.75      1083\n",
            "   macro avg       0.71      0.68      0.69      1083\n",
            "weighted avg       0.74      0.75      0.74      1083\n",
            "\n",
            "\n",
            "Training SVM with split 0.38...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.38: 0.9695290858725761\n",
            "SVM Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       739\n",
            "         1.0       0.95      0.95      0.95       344\n",
            "\n",
            "    accuracy                           0.97      1083\n",
            "   macro avg       0.96      0.97      0.96      1083\n",
            "weighted avg       0.97      0.97      0.97      1083\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.38...\n",
            "ZeroR Accuracy with split 0.38: 0.6823638042474608\n",
            "ZeroR Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       739\n",
            "         1.0       0.00      0.00      0.00       344\n",
            "\n",
            "    accuracy                           0.68      1083\n",
            "   macro avg       0.34      0.50      0.41      1083\n",
            "weighted avg       0.47      0.68      0.55      1083\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.61 train, 0.39 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.39...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.39: 0.9766187050359713\n",
            "DecisionTree Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       759\n",
            "         1.0       0.96      0.96      0.96       353\n",
            "\n",
            "    accuracy                           0.98      1112\n",
            "   macro avg       0.97      0.97      0.97      1112\n",
            "weighted avg       0.98      0.98      0.98      1112\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.39...\n",
            "NaiveBayes Accuracy with split 0.39: 0.7455035971223022\n",
            "NaiveBayes Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       759\n",
            "         1.0       0.63      0.49      0.55       353\n",
            "\n",
            "    accuracy                           0.75      1112\n",
            "   macro avg       0.71      0.68      0.69      1112\n",
            "weighted avg       0.73      0.75      0.74      1112\n",
            "\n",
            "\n",
            "Training SVM with split 0.39...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.39: 0.9703237410071942\n",
            "SVM Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       759\n",
            "         1.0       0.95      0.95      0.95       353\n",
            "\n",
            "    accuracy                           0.97      1112\n",
            "   macro avg       0.97      0.97      0.97      1112\n",
            "weighted avg       0.97      0.97      0.97      1112\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.39...\n",
            "ZeroR Accuracy with split 0.39: 0.6825539568345323\n",
            "ZeroR Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       759\n",
            "         1.0       0.00      0.00      0.00       353\n",
            "\n",
            "    accuracy                           0.68      1112\n",
            "   macro avg       0.34      0.50      0.41      1112\n",
            "weighted avg       0.47      0.68      0.55      1112\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6 train, 0.4 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.4...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.4: 0.9771929824561404\n",
            "DecisionTree Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       775\n",
            "         1.0       0.96      0.96      0.96       365\n",
            "\n",
            "    accuracy                           0.98      1140\n",
            "   macro avg       0.97      0.97      0.97      1140\n",
            "weighted avg       0.98      0.98      0.98      1140\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.4...\n",
            "NaiveBayes Accuracy with split 0.4: 0.7491228070175439\n",
            "NaiveBayes Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.83       775\n",
            "         1.0       0.64      0.49      0.55       365\n",
            "\n",
            "    accuracy                           0.75      1140\n",
            "   macro avg       0.71      0.68      0.69      1140\n",
            "weighted avg       0.74      0.75      0.74      1140\n",
            "\n",
            "\n",
            "Training SVM with split 0.4...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.4: 0.9710526315789474\n",
            "SVM Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       775\n",
            "         1.0       0.95      0.96      0.95       365\n",
            "\n",
            "    accuracy                           0.97      1140\n",
            "   macro avg       0.97      0.97      0.97      1140\n",
            "weighted avg       0.97      0.97      0.97      1140\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.4...\n",
            "ZeroR Accuracy with split 0.4: 0.6798245614035088\n",
            "ZeroR Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       775\n",
            "         1.0       0.00      0.00      0.00       365\n",
            "\n",
            "    accuracy                           0.68      1140\n",
            "   macro avg       0.34      0.50      0.40      1140\n",
            "weighted avg       0.46      0.68      0.55      1140\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5900000000000001 train, 0.41 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.41...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.41: 0.9769033361847733\n",
            "DecisionTree Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       795\n",
            "         1.0       0.97      0.96      0.96       374\n",
            "\n",
            "    accuracy                           0.98      1169\n",
            "   macro avg       0.97      0.97      0.97      1169\n",
            "weighted avg       0.98      0.98      0.98      1169\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.41...\n",
            "NaiveBayes Accuracy with split 0.41: 0.7442258340461934\n",
            "NaiveBayes Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       795\n",
            "         1.0       0.63      0.48      0.54       374\n",
            "\n",
            "    accuracy                           0.74      1169\n",
            "   macro avg       0.71      0.67      0.68      1169\n",
            "weighted avg       0.73      0.74      0.73      1169\n",
            "\n",
            "\n",
            "Training SVM with split 0.41...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.41: 0.9717707442258341\n",
            "SVM Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       795\n",
            "         1.0       0.97      0.94      0.96       374\n",
            "\n",
            "    accuracy                           0.97      1169\n",
            "   macro avg       0.97      0.96      0.97      1169\n",
            "weighted avg       0.97      0.97      0.97      1169\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.41...\n",
            "ZeroR Accuracy with split 0.41: 0.6800684345594525\n",
            "ZeroR Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       795\n",
            "         1.0       0.00      0.00      0.00       374\n",
            "\n",
            "    accuracy                           0.68      1169\n",
            "   macro avg       0.34      0.50      0.40      1169\n",
            "weighted avg       0.46      0.68      0.55      1169\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5800000000000001 train, 0.42 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.42...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.42: 0.9774436090225563\n",
            "DecisionTree Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       813\n",
            "         1.0       0.97      0.96      0.96       384\n",
            "\n",
            "    accuracy                           0.98      1197\n",
            "   macro avg       0.97      0.97      0.97      1197\n",
            "weighted avg       0.98      0.98      0.98      1197\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.42...\n",
            "NaiveBayes Accuracy with split 0.42: 0.7410192147034252\n",
            "NaiveBayes Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       813\n",
            "         1.0       0.63      0.47      0.54       384\n",
            "\n",
            "    accuracy                           0.74      1197\n",
            "   macro avg       0.70      0.67      0.68      1197\n",
            "weighted avg       0.73      0.74      0.73      1197\n",
            "\n",
            "\n",
            "Training SVM with split 0.42...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.42: 0.9724310776942355\n",
            "SVM Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       813\n",
            "         1.0       0.97      0.94      0.96       384\n",
            "\n",
            "    accuracy                           0.97      1197\n",
            "   macro avg       0.97      0.96      0.97      1197\n",
            "weighted avg       0.97      0.97      0.97      1197\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.42...\n",
            "ZeroR Accuracy with split 0.42: 0.6791979949874687\n",
            "ZeroR Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       813\n",
            "         1.0       0.00      0.00      0.00       384\n",
            "\n",
            "    accuracy                           0.68      1197\n",
            "   macro avg       0.34      0.50      0.40      1197\n",
            "weighted avg       0.46      0.68      0.55      1197\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5700000000000001 train, 0.43 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.43...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.43: 0.9771615008156607\n",
            "DecisionTree Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       829\n",
            "         1.0       0.97      0.96      0.96       397\n",
            "\n",
            "    accuracy                           0.98      1226\n",
            "   macro avg       0.97      0.97      0.97      1226\n",
            "weighted avg       0.98      0.98      0.98      1226\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.43...\n",
            "NaiveBayes Accuracy with split 0.43: 0.736541598694943\n",
            "NaiveBayes Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       829\n",
            "         1.0       0.62      0.47      0.53       397\n",
            "\n",
            "    accuracy                           0.74      1226\n",
            "   macro avg       0.70      0.67      0.68      1226\n",
            "weighted avg       0.72      0.74      0.72      1226\n",
            "\n",
            "\n",
            "Training SVM with split 0.43...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.43: 0.9722675367047309\n",
            "SVM Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       829\n",
            "         1.0       0.97      0.94      0.96       397\n",
            "\n",
            "    accuracy                           0.97      1226\n",
            "   macro avg       0.97      0.96      0.97      1226\n",
            "weighted avg       0.97      0.97      0.97      1226\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.43...\n",
            "ZeroR Accuracy with split 0.43: 0.6761827079934747\n",
            "ZeroR Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       829\n",
            "         1.0       0.00      0.00      0.00       397\n",
            "\n",
            "    accuracy                           0.68      1226\n",
            "   macro avg       0.34      0.50      0.40      1226\n",
            "weighted avg       0.46      0.68      0.55      1226\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.56 train, 0.44 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.44...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.44: 0.9776714513556619\n",
            "DecisionTree Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       852\n",
            "         1.0       0.98      0.95      0.96       402\n",
            "\n",
            "    accuracy                           0.98      1254\n",
            "   macro avg       0.98      0.97      0.97      1254\n",
            "weighted avg       0.98      0.98      0.98      1254\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.44...\n",
            "NaiveBayes Accuracy with split 0.44: 0.7320574162679426\n",
            "NaiveBayes Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.81       852\n",
            "         1.0       0.61      0.45      0.52       402\n",
            "\n",
            "    accuracy                           0.73      1254\n",
            "   macro avg       0.69      0.66      0.67      1254\n",
            "weighted avg       0.72      0.73      0.72      1254\n",
            "\n",
            "\n",
            "Training SVM with split 0.44...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.44: 0.9728867623604466\n",
            "SVM Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       852\n",
            "         1.0       0.98      0.93      0.96       402\n",
            "\n",
            "    accuracy                           0.97      1254\n",
            "   macro avg       0.98      0.96      0.97      1254\n",
            "weighted avg       0.97      0.97      0.97      1254\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.44...\n",
            "ZeroR Accuracy with split 0.44: 0.6794258373205742\n",
            "ZeroR Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       852\n",
            "         1.0       0.00      0.00      0.00       402\n",
            "\n",
            "    accuracy                           0.68      1254\n",
            "   macro avg       0.34      0.50      0.40      1254\n",
            "weighted avg       0.46      0.68      0.55      1254\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.55 train, 0.45 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.45...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.45: 0.9781761496492596\n",
            "DecisionTree Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       876\n",
            "         1.0       0.98      0.95      0.97       407\n",
            "\n",
            "    accuracy                           0.98      1283\n",
            "   macro avg       0.98      0.97      0.97      1283\n",
            "weighted avg       0.98      0.98      0.98      1283\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.45...\n",
            "NaiveBayes Accuracy with split 0.45: 0.7357755261106781\n",
            "NaiveBayes Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       876\n",
            "         1.0       0.62      0.44      0.52       407\n",
            "\n",
            "    accuracy                           0.74      1283\n",
            "   macro avg       0.69      0.66      0.67      1283\n",
            "weighted avg       0.72      0.74      0.72      1283\n",
            "\n",
            "\n",
            "Training SVM with split 0.45...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.45: 0.9734996102883866\n",
            "SVM Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       876\n",
            "         1.0       0.98      0.93      0.96       407\n",
            "\n",
            "    accuracy                           0.97      1283\n",
            "   macro avg       0.98      0.96      0.97      1283\n",
            "weighted avg       0.97      0.97      0.97      1283\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.45...\n",
            "ZeroR Accuracy with split 0.45: 0.6827747466874513\n",
            "ZeroR Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       876\n",
            "         1.0       0.00      0.00      0.00       407\n",
            "\n",
            "    accuracy                           0.68      1283\n",
            "   macro avg       0.34      0.50      0.41      1283\n",
            "weighted avg       0.47      0.68      0.55      1283\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.54 train, 0.46 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.46...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.46: 0.977116704805492\n",
            "DecisionTree Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       900\n",
            "         1.0       0.96      0.96      0.96       411\n",
            "\n",
            "    accuracy                           0.98      1311\n",
            "   macro avg       0.97      0.97      0.97      1311\n",
            "weighted avg       0.98      0.98      0.98      1311\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.46...\n",
            "NaiveBayes Accuracy with split 0.46: 0.738367658276125\n",
            "NaiveBayes Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       900\n",
            "         1.0       0.61      0.45      0.52       411\n",
            "\n",
            "    accuracy                           0.74      1311\n",
            "   macro avg       0.69      0.66      0.67      1311\n",
            "weighted avg       0.72      0.74      0.73      1311\n",
            "\n",
            "\n",
            "Training SVM with split 0.46...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.46: 0.9725400457665904\n",
            "SVM Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       900\n",
            "         1.0       0.98      0.93      0.96       411\n",
            "\n",
            "    accuracy                           0.97      1311\n",
            "   macro avg       0.97      0.96      0.97      1311\n",
            "weighted avg       0.97      0.97      0.97      1311\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.46...\n",
            "ZeroR Accuracy with split 0.46: 0.6864988558352403\n",
            "ZeroR Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       900\n",
            "         1.0       0.00      0.00      0.00       411\n",
            "\n",
            "    accuracy                           0.69      1311\n",
            "   macro avg       0.34      0.50      0.41      1311\n",
            "weighted avg       0.47      0.69      0.56      1311\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.53 train, 0.47 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.47...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.47: 0.9776119402985075\n",
            "DecisionTree Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       918\n",
            "         1.0       0.98      0.95      0.96       422\n",
            "\n",
            "    accuracy                           0.98      1340\n",
            "   macro avg       0.98      0.97      0.97      1340\n",
            "weighted avg       0.98      0.98      0.98      1340\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.47...\n",
            "NaiveBayes Accuracy with split 0.47: 0.7373134328358208\n",
            "NaiveBayes Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       918\n",
            "         1.0       0.62      0.44      0.51       422\n",
            "\n",
            "    accuracy                           0.74      1340\n",
            "   macro avg       0.69      0.66      0.67      1340\n",
            "weighted avg       0.72      0.74      0.72      1340\n",
            "\n",
            "\n",
            "Training SVM with split 0.47...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.47: 0.9731343283582089\n",
            "SVM Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       918\n",
            "         1.0       0.98      0.94      0.96       422\n",
            "\n",
            "    accuracy                           0.97      1340\n",
            "   macro avg       0.97      0.96      0.97      1340\n",
            "weighted avg       0.97      0.97      0.97      1340\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.47...\n",
            "ZeroR Accuracy with split 0.47: 0.6850746268656717\n",
            "ZeroR Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       918\n",
            "         1.0       0.00      0.00      0.00       422\n",
            "\n",
            "    accuracy                           0.69      1340\n",
            "   macro avg       0.34      0.50      0.41      1340\n",
            "weighted avg       0.47      0.69      0.56      1340\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.52 train, 0.48 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.48...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.48: 0.9780701754385965\n",
            "DecisionTree Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       937\n",
            "         1.0       0.98      0.95      0.96       431\n",
            "\n",
            "    accuracy                           0.98      1368\n",
            "   macro avg       0.98      0.97      0.97      1368\n",
            "weighted avg       0.98      0.98      0.98      1368\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.48...\n",
            "NaiveBayes Accuracy with split 0.48: 0.72953216374269\n",
            "NaiveBayes Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       937\n",
            "         1.0       0.60      0.42      0.49       431\n",
            "\n",
            "    accuracy                           0.73      1368\n",
            "   macro avg       0.68      0.65      0.65      1368\n",
            "weighted avg       0.71      0.73      0.71      1368\n",
            "\n",
            "\n",
            "Training SVM with split 0.48...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.48: 0.9736842105263158\n",
            "SVM Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       937\n",
            "         1.0       0.98      0.94      0.96       431\n",
            "\n",
            "    accuracy                           0.97      1368\n",
            "   macro avg       0.97      0.96      0.97      1368\n",
            "weighted avg       0.97      0.97      0.97      1368\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.48...\n",
            "ZeroR Accuracy with split 0.48: 0.6849415204678363\n",
            "ZeroR Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       937\n",
            "         1.0       0.00      0.00      0.00       431\n",
            "\n",
            "    accuracy                           0.68      1368\n",
            "   macro avg       0.34      0.50      0.41      1368\n",
            "weighted avg       0.47      0.68      0.56      1368\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.51 train, 0.49 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.49...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.49: 0.9778095919828204\n",
            "DecisionTree Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       959\n",
            "         1.0       0.98      0.95      0.96       438\n",
            "\n",
            "    accuracy                           0.98      1397\n",
            "   macro avg       0.98      0.97      0.97      1397\n",
            "weighted avg       0.98      0.98      0.98      1397\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.49...\n",
            "NaiveBayes Accuracy with split 0.49: 0.737294201861131\n",
            "NaiveBayes Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82       959\n",
            "         1.0       0.62      0.42      0.50       438\n",
            "\n",
            "    accuracy                           0.74      1397\n",
            "   macro avg       0.69      0.65      0.66      1397\n",
            "weighted avg       0.72      0.74      0.72      1397\n",
            "\n",
            "\n",
            "Training SVM with split 0.49...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.49: 0.9735146743020758\n",
            "SVM Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       959\n",
            "         1.0       0.98      0.94      0.96       438\n",
            "\n",
            "    accuracy                           0.97      1397\n",
            "   macro avg       0.97      0.96      0.97      1397\n",
            "weighted avg       0.97      0.97      0.97      1397\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.49...\n",
            "ZeroR Accuracy with split 0.49: 0.686471009305655\n",
            "ZeroR Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       959\n",
            "         1.0       0.00      0.00      0.00       438\n",
            "\n",
            "    accuracy                           0.69      1397\n",
            "   macro avg       0.34      0.50      0.41      1397\n",
            "weighted avg       0.47      0.69      0.56      1397\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5 train, 0.5 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.5...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.5: 0.9775438596491228\n",
            "DecisionTree Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       979\n",
            "         1.0       0.98      0.95      0.96       446\n",
            "\n",
            "    accuracy                           0.98      1425\n",
            "   macro avg       0.98      0.97      0.97      1425\n",
            "weighted avg       0.98      0.98      0.98      1425\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.5...\n",
            "NaiveBayes Accuracy with split 0.5: 0.7452631578947368\n",
            "NaiveBayes Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.83       979\n",
            "         1.0       0.63      0.44      0.52       446\n",
            "\n",
            "    accuracy                           0.75      1425\n",
            "   macro avg       0.71      0.66      0.67      1425\n",
            "weighted avg       0.73      0.75      0.73      1425\n",
            "\n",
            "\n",
            "Training SVM with split 0.5...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.5: 0.9733333333333334\n",
            "SVM Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       979\n",
            "         1.0       0.98      0.93      0.96       446\n",
            "\n",
            "    accuracy                           0.97      1425\n",
            "   macro avg       0.97      0.96      0.97      1425\n",
            "weighted avg       0.97      0.97      0.97      1425\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.5...\n",
            "ZeroR Accuracy with split 0.5: 0.6870175438596491\n",
            "ZeroR Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       979\n",
            "         1.0       0.00      0.00      0.00       446\n",
            "\n",
            "    accuracy                           0.69      1425\n",
            "   macro avg       0.34      0.50      0.41      1425\n",
            "weighted avg       0.47      0.69      0.56      1425\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.49 train, 0.51 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.51...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.51: 0.9766162310866575\n",
            "DecisionTree Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       999\n",
            "         1.0       0.98      0.95      0.96       455\n",
            "\n",
            "    accuracy                           0.98      1454\n",
            "   macro avg       0.98      0.97      0.97      1454\n",
            "weighted avg       0.98      0.98      0.98      1454\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.51...\n",
            "NaiveBayes Accuracy with split 0.51: 0.7427785419532325\n",
            "NaiveBayes Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.83       999\n",
            "         1.0       0.63      0.44      0.51       455\n",
            "\n",
            "    accuracy                           0.74      1454\n",
            "   macro avg       0.70      0.66      0.67      1454\n",
            "weighted avg       0.73      0.74      0.73      1454\n",
            "\n",
            "\n",
            "Training SVM with split 0.51...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.51: 0.9724896836313618\n",
            "SVM Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       999\n",
            "         1.0       0.99      0.92      0.95       455\n",
            "\n",
            "    accuracy                           0.97      1454\n",
            "   macro avg       0.98      0.96      0.97      1454\n",
            "weighted avg       0.97      0.97      0.97      1454\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.51...\n",
            "ZeroR Accuracy with split 0.51: 0.68707015130674\n",
            "ZeroR Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       999\n",
            "         1.0       0.00      0.00      0.00       455\n",
            "\n",
            "    accuracy                           0.69      1454\n",
            "   macro avg       0.34      0.50      0.41      1454\n",
            "weighted avg       0.47      0.69      0.56      1454\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.48 train, 0.52 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.52...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.52: 0.9763832658569501\n",
            "DecisionTree Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1019\n",
            "         1.0       0.99      0.94      0.96       463\n",
            "\n",
            "    accuracy                           0.98      1482\n",
            "   macro avg       0.98      0.97      0.97      1482\n",
            "weighted avg       0.98      0.98      0.98      1482\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.52...\n",
            "NaiveBayes Accuracy with split 0.52: 0.7415654520917678\n",
            "NaiveBayes Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82      1019\n",
            "         1.0       0.62      0.43      0.51       463\n",
            "\n",
            "    accuracy                           0.74      1482\n",
            "   macro avg       0.70      0.66      0.67      1482\n",
            "weighted avg       0.73      0.74      0.73      1482\n",
            "\n",
            "\n",
            "Training SVM with split 0.52...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.52: 0.97165991902834\n",
            "SVM Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      1.00      0.98      1019\n",
            "         1.0       0.99      0.92      0.95       463\n",
            "\n",
            "    accuracy                           0.97      1482\n",
            "   macro avg       0.98      0.96      0.97      1482\n",
            "weighted avg       0.97      0.97      0.97      1482\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.52...\n",
            "ZeroR Accuracy with split 0.52: 0.6875843454790823\n",
            "ZeroR Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81      1019\n",
            "         1.0       0.00      0.00      0.00       463\n",
            "\n",
            "    accuracy                           0.69      1482\n",
            "   macro avg       0.34      0.50      0.41      1482\n",
            "weighted avg       0.47      0.69      0.56      1482\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.47 train, 0.53 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.53...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.53: 0.9761747187293184\n",
            "DecisionTree Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1045\n",
            "         1.0       0.97      0.95      0.96       466\n",
            "\n",
            "    accuracy                           0.98      1511\n",
            "   macro avg       0.98      0.97      0.97      1511\n",
            "weighted avg       0.98      0.98      0.98      1511\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.53...\n",
            "NaiveBayes Accuracy with split 0.53: 0.7405691594970218\n",
            "NaiveBayes Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.82      1045\n",
            "         1.0       0.61      0.43      0.51       466\n",
            "\n",
            "    accuracy                           0.74      1511\n",
            "   macro avg       0.69      0.66      0.67      1511\n",
            "weighted avg       0.73      0.74      0.73      1511\n",
            "\n",
            "\n",
            "Training SVM with split 0.53...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.53: 0.972203838517538\n",
            "SVM Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1045\n",
            "         1.0       0.99      0.92      0.95       466\n",
            "\n",
            "    accuracy                           0.97      1511\n",
            "   macro avg       0.98      0.96      0.97      1511\n",
            "weighted avg       0.97      0.97      0.97      1511\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.53...\n",
            "ZeroR Accuracy with split 0.53: 0.6915949702183984\n",
            "ZeroR Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1045\n",
            "         1.0       0.00      0.00      0.00       466\n",
            "\n",
            "    accuracy                           0.69      1511\n",
            "   macro avg       0.35      0.50      0.41      1511\n",
            "weighted avg       0.48      0.69      0.57      1511\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.45999999999999996 train, 0.54 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.54...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.54: 0.9766081871345029\n",
            "DecisionTree Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1070\n",
            "         1.0       0.97      0.95      0.96       469\n",
            "\n",
            "    accuracy                           0.98      1539\n",
            "   macro avg       0.98      0.97      0.97      1539\n",
            "weighted avg       0.98      0.98      0.98      1539\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.54...\n",
            "NaiveBayes Accuracy with split 0.54: 0.7439896036387265\n",
            "NaiveBayes Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.83      1070\n",
            "         1.0       0.61      0.45      0.52       469\n",
            "\n",
            "    accuracy                           0.74      1539\n",
            "   macro avg       0.70      0.66      0.67      1539\n",
            "weighted avg       0.73      0.74      0.73      1539\n",
            "\n",
            "\n",
            "Training SVM with split 0.54...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.54: 0.9727095516569201\n",
            "SVM Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1070\n",
            "         1.0       0.99      0.92      0.95       469\n",
            "\n",
            "    accuracy                           0.97      1539\n",
            "   macro avg       0.98      0.96      0.97      1539\n",
            "weighted avg       0.97      0.97      0.97      1539\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.54...\n",
            "ZeroR Accuracy with split 0.54: 0.6952566601689408\n",
            "ZeroR Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82      1070\n",
            "         1.0       0.00      0.00      0.00       469\n",
            "\n",
            "    accuracy                           0.70      1539\n",
            "   macro avg       0.35      0.50      0.41      1539\n",
            "weighted avg       0.48      0.70      0.57      1539\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.44999999999999996 train, 0.55 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.55...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.55: 0.9770408163265306\n",
            "DecisionTree Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1089\n",
            "         1.0       0.97      0.95      0.96       479\n",
            "\n",
            "    accuracy                           0.98      1568\n",
            "   macro avg       0.98      0.97      0.97      1568\n",
            "weighted avg       0.98      0.98      0.98      1568\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.55...\n",
            "NaiveBayes Accuracy with split 0.55: 0.7429846938775511\n",
            "NaiveBayes Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.83      1089\n",
            "         1.0       0.61      0.44      0.51       479\n",
            "\n",
            "    accuracy                           0.74      1568\n",
            "   macro avg       0.70      0.66      0.67      1568\n",
            "weighted avg       0.73      0.74      0.73      1568\n",
            "\n",
            "\n",
            "Training SVM with split 0.55...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.55: 0.9732142857142857\n",
            "SVM Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1089\n",
            "         1.0       0.99      0.92      0.95       479\n",
            "\n",
            "    accuracy                           0.97      1568\n",
            "   macro avg       0.98      0.96      0.97      1568\n",
            "weighted avg       0.97      0.97      0.97      1568\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.55...\n",
            "ZeroR Accuracy with split 0.55: 0.6945153061224489\n",
            "ZeroR Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1089\n",
            "         1.0       0.00      0.00      0.00       479\n",
            "\n",
            "    accuracy                           0.69      1568\n",
            "   macro avg       0.35      0.50      0.41      1568\n",
            "weighted avg       0.48      0.69      0.57      1568\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43999999999999995 train, 0.56 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.56...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.56: 0.9774577332498434\n",
            "DecisionTree Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1110\n",
            "         1.0       0.97      0.95      0.96       487\n",
            "\n",
            "    accuracy                           0.98      1597\n",
            "   macro avg       0.98      0.97      0.97      1597\n",
            "weighted avg       0.98      0.98      0.98      1597\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.56...\n",
            "NaiveBayes Accuracy with split 0.56: 0.7470256731371321\n",
            "NaiveBayes Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.83      1110\n",
            "         1.0       0.62      0.44      0.51       487\n",
            "\n",
            "    accuracy                           0.75      1597\n",
            "   macro avg       0.70      0.66      0.67      1597\n",
            "weighted avg       0.73      0.75      0.73      1597\n",
            "\n",
            "\n",
            "Training SVM with split 0.56...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.56: 0.9737006887914841\n",
            "SVM Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1110\n",
            "         1.0       0.99      0.92      0.96       487\n",
            "\n",
            "    accuracy                           0.97      1597\n",
            "   macro avg       0.98      0.96      0.97      1597\n",
            "weighted avg       0.97      0.97      0.97      1597\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.56...\n",
            "ZeroR Accuracy with split 0.56: 0.6950532247964935\n",
            "ZeroR Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82      1110\n",
            "         1.0       0.00      0.00      0.00       487\n",
            "\n",
            "    accuracy                           0.70      1597\n",
            "   macro avg       0.35      0.50      0.41      1597\n",
            "weighted avg       0.48      0.70      0.57      1597\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43000000000000005 train, 0.57 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.57...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.57: 0.9778461538461538\n",
            "DecisionTree Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1128\n",
            "         1.0       0.98      0.95      0.96       497\n",
            "\n",
            "    accuracy                           0.98      1625\n",
            "   macro avg       0.98      0.97      0.97      1625\n",
            "weighted avg       0.98      0.98      0.98      1625\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.57...\n",
            "NaiveBayes Accuracy with split 0.57: 0.7415384615384616\n",
            "NaiveBayes Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.82      1128\n",
            "         1.0       0.61      0.44      0.51       497\n",
            "\n",
            "    accuracy                           0.74      1625\n",
            "   macro avg       0.69      0.66      0.67      1625\n",
            "weighted avg       0.73      0.74      0.73      1625\n",
            "\n",
            "\n",
            "Training SVM with split 0.57...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.57: 0.9741538461538461\n",
            "SVM Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1128\n",
            "         1.0       0.99      0.93      0.96       497\n",
            "\n",
            "    accuracy                           0.97      1625\n",
            "   macro avg       0.98      0.96      0.97      1625\n",
            "weighted avg       0.97      0.97      0.97      1625\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.57...\n",
            "ZeroR Accuracy with split 0.57: 0.6941538461538461\n",
            "ZeroR Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1128\n",
            "         1.0       0.00      0.00      0.00       497\n",
            "\n",
            "    accuracy                           0.69      1625\n",
            "   macro avg       0.35      0.50      0.41      1625\n",
            "weighted avg       0.48      0.69      0.57      1625\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.42000000000000004 train, 0.58 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.58...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.58: 0.9776164549304295\n",
            "DecisionTree Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1147\n",
            "         1.0       0.98      0.95      0.96       506\n",
            "\n",
            "    accuracy                           0.98      1653\n",
            "   macro avg       0.98      0.97      0.97      1653\n",
            "weighted avg       0.98      0.98      0.98      1653\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.58...\n",
            "NaiveBayes Accuracy with split 0.58: 0.736237144585602\n",
            "NaiveBayes Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82      1147\n",
            "         1.0       0.59      0.44      0.50       506\n",
            "\n",
            "    accuracy                           0.74      1653\n",
            "   macro avg       0.69      0.65      0.66      1653\n",
            "weighted avg       0.72      0.74      0.72      1653\n",
            "\n",
            "\n",
            "Training SVM with split 0.58...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.58: 0.9739866908650938\n",
            "SVM Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98      1147\n",
            "         1.0       0.98      0.94      0.96       506\n",
            "\n",
            "    accuracy                           0.97      1653\n",
            "   macro avg       0.97      0.96      0.97      1653\n",
            "weighted avg       0.97      0.97      0.97      1653\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.58...\n",
            "ZeroR Accuracy with split 0.58: 0.6938898971566848\n",
            "ZeroR Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1147\n",
            "         1.0       0.00      0.00      0.00       506\n",
            "\n",
            "    accuracy                           0.69      1653\n",
            "   macro avg       0.35      0.50      0.41      1653\n",
            "weighted avg       0.48      0.69      0.57      1653\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.41000000000000003 train, 0.59 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.59...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.59: 0.9774078478002378\n",
            "DecisionTree Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1167\n",
            "         1.0       0.97      0.95      0.96       515\n",
            "\n",
            "    accuracy                           0.98      1682\n",
            "   macro avg       0.98      0.97      0.97      1682\n",
            "weighted avg       0.98      0.98      0.98      1682\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.59...\n",
            "NaiveBayes Accuracy with split 0.59: 0.7360285374554102\n",
            "NaiveBayes Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82      1167\n",
            "         1.0       0.59      0.43      0.50       515\n",
            "\n",
            "    accuracy                           0.74      1682\n",
            "   macro avg       0.69      0.65      0.66      1682\n",
            "weighted avg       0.72      0.74      0.72      1682\n",
            "\n",
            "\n",
            "Training SVM with split 0.59...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.59: 0.9738406658739596\n",
            "SVM Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98      1167\n",
            "         1.0       0.98      0.94      0.96       515\n",
            "\n",
            "    accuracy                           0.97      1682\n",
            "   macro avg       0.97      0.96      0.97      1682\n",
            "weighted avg       0.97      0.97      0.97      1682\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.59...\n",
            "ZeroR Accuracy with split 0.59: 0.6938168846611177\n",
            "ZeroR Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1167\n",
            "         1.0       0.00      0.00      0.00       515\n",
            "\n",
            "    accuracy                           0.69      1682\n",
            "   macro avg       0.35      0.50      0.41      1682\n",
            "weighted avg       0.48      0.69      0.57      1682\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.4 train, 0.6 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.6...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.6: 0.9777777777777777\n",
            "DecisionTree Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1184\n",
            "         1.0       0.97      0.95      0.96       526\n",
            "\n",
            "    accuracy                           0.98      1710\n",
            "   macro avg       0.98      0.97      0.97      1710\n",
            "weighted avg       0.98      0.98      0.98      1710\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.6...\n",
            "NaiveBayes Accuracy with split 0.6: 0.7345029239766082\n",
            "NaiveBayes Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82      1184\n",
            "         1.0       0.59      0.43      0.50       526\n",
            "\n",
            "    accuracy                           0.73      1710\n",
            "   macro avg       0.68      0.65      0.66      1710\n",
            "weighted avg       0.72      0.73      0.72      1710\n",
            "\n",
            "\n",
            "Training SVM with split 0.6...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.6: 0.9742690058479532\n",
            "SVM Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98      1184\n",
            "         1.0       0.98      0.94      0.96       526\n",
            "\n",
            "    accuracy                           0.97      1710\n",
            "   macro avg       0.97      0.96      0.97      1710\n",
            "weighted avg       0.97      0.97      0.97      1710\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.6...\n",
            "ZeroR Accuracy with split 0.6: 0.6923976608187135\n",
            "ZeroR Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1184\n",
            "         1.0       0.00      0.00      0.00       526\n",
            "\n",
            "    accuracy                           0.69      1710\n",
            "   macro avg       0.35      0.50      0.41      1710\n",
            "weighted avg       0.48      0.69      0.57      1710\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkVu69bYQBCT"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}