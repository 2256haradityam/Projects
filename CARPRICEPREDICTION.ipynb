{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZr7lCNb5o98AYoSy7c4A4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2256haradityam/Projects/blob/main/CARPRICEPREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4m3-2g621mK6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://github.com/2256haradityam/dataset/raw/refs/heads/main/bcn.csv')"
      ],
      "metadata": {
        "id": "mo5jHXYL2YWI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4tJ5fwpb2b7f",
        "outputId": "f11f4f35-c96d-4239-e80c-3896bcee8458"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   class  age  menopause  tumor-size  inv-nodes  node-caps  deg-malig  breast  \\\n",
              "0      0   39          1          34          2          0          3       1   \n",
              "1      0   49          1          24          2          0          2       2   \n",
              "2      0   49          1          24          2          0          2       1   \n",
              "3      0   69          2          19          2          0          2       2   \n",
              "4      0   49          1           4          2          0          2       2   \n",
              "\n",
              "   breast-quad  irradiat  \n",
              "0            1         0  \n",
              "1            3         0  \n",
              "2            1         0  \n",
              "3            2         0  \n",
              "4            4         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b74af911-1925-4511-91e0-35470c8c90b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>age</th>\n",
              "      <th>menopause</th>\n",
              "      <th>tumor-size</th>\n",
              "      <th>inv-nodes</th>\n",
              "      <th>node-caps</th>\n",
              "      <th>deg-malig</th>\n",
              "      <th>breast</th>\n",
              "      <th>breast-quad</th>\n",
              "      <th>irradiat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>69</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b74af911-1925-4511-91e0-35470c8c90b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b74af911-1925-4511-91e0-35470c8c90b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b74af911-1925-4511-91e0-35470c8c90b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d759cde1-e2f1-45f9-9de1-b4279bd7a461\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d759cde1-e2f1-45f9-9de1-b4279bd7a461')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d759cde1-e2f1-45f9-9de1-b4279bd7a461 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 285,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 29,\n        \"max\": 79,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          39,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"menopause\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tumor-size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 4,\n        \"max\": 54,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          54,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inv-nodes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 26,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"node-caps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deg-malig\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"breast\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"breast-quad\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"irradiat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfn = df.copy()"
      ],
      "metadata": {
        "id": "e4AmSj7UQM6L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfn['breast'].value_counts()"
      ],
      "metadata": {
        "id": "iLrvjQjbHoD0",
        "outputId": "7a1f7d3a-b7b5-415e-8798-55648d3c0874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "breast\n",
              "0    151\n",
              "1    134\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breast</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfn['breast'] = dfn['breast'].map({1: 0, 2: 1})"
      ],
      "metadata": {
        "id": "Av3CIQTsHTMl"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_encode = ['age', 'inv-nodes', 'deg-malig', 'tumor-size', 'irradiat',  'breast-quad', 'menopause']\n",
        "\n",
        "# Perform One-Hot Encoding on selected columns\n",
        "df_encoded = pd.get_dummies(dfn, columns=columns_to_encode)\n",
        "\n",
        "# Convert only the columns created from `columns_to_encode` to int\n",
        "encoded_columns = df_encoded.columns[df_encoded.columns.str.contains('_'.join(columns_to_encode))]\n",
        "df_encoded[encoded_columns] = df_encoded[encoded_columns].astype(int)\n"
      ],
      "metadata": {
        "id": "BPdWR0k62e-n"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df_encoded['new_column'] = scaler.fit_transform(dfn[['new_column']])"
      ],
      "metadata": {
        "id": "FHuCegOHDo8F"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df_encoded.corr()[['class']], annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "OJgR0GwANbmJ",
        "outputId": "63514d0c-991d-4f7a-fbb2-54aecd43f93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Define ranges for numerator and denominator power ratios\n",
        "numerator_powers = range(1, 20)\n",
        "denominator_powers = range(1, 20)\n",
        "\n",
        "# Initialize variables to store the best power ratios and correlation\n",
        "best_numerator_power = 0\n",
        "best_denominator_power = 0\n",
        "best_correlation = -1\n",
        "\n",
        "# Iterate through different power ratios\n",
        "for numerator_power in numerator_powers:\n",
        "    for denominator_power in denominator_powers:\n",
        "        dfn['new_column'] = ((dfn['tumor-size'] * dfn['inv-nodes'] * dfn['deg-malig'])**numerator_power)/((dfn['age']*dfn['menopause'])**denominator_power)\n",
        "        correlation = pearsonr(dfn['new_column'], dfn['class'])[0]\n",
        "        if correlation > best_correlation:\n",
        "            best_numerator_power = numerator_power\n",
        "            best_denominator_power = denominator_power\n",
        "            best_correlation = correlation\n",
        "\n",
        "# Apply the best power ratios to calculate the final 'new_column'\n",
        "dfn['new_column'] = ((dfn['tumor-size'] * dfn['inv-nodes'] * dfn['deg-malig'])**best_numerator_power)/((dfn['age']*dfn['menopause'])**best_denominator_power)\n",
        "\n",
        "print(\"Best numerator power:\", best_numerator_power)\n",
        "print(\"Best denominator power:\", best_denominator_power)\n",
        "print(\"Best correlation:\", best_correlation)"
      ],
      "metadata": {
        "id": "YM3sYRlMA2OD",
        "outputId": "30de6b87-5f65-4369-9b5e-aa115ed9da1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best numerator power: 1\n",
            "Best denominator power: 1\n",
            "Best correlation: 0.313585457213801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfn['new_column'] = dfn['new_column']"
      ],
      "metadata": {
        "id": "HaMeh4opBwuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dfn[(dfn['class'] == 0)].groupby('class').mean().to_string())"
      ],
      "metadata": {
        "id": "vivk9oU6mYgg",
        "outputId": "4310ae15-3501-49cf-cfd2-0ba912dbb615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             age  menopause  tumor-size  inv-nodes  node-caps  deg-malig    breast  breast-quad  irradiat\n",
            "class                                                                                                    \n",
            "0      56.114428   1.517413   27.208955   2.955224   0.124378   1.905473  1.487562     2.159204   0.18408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class\n",
        "0      56.114428   1.517413   27.208955   2.955224   0.124378   1.905473  1.487562     2.159204   0.18408\n",
        "1      54.47619   1.452381   31.202381   5.071429   0.369048   2.380952  1.428571      2.02381    0.369048"
      ],
      "metadata": {
        "id": "gmFitDrFAKur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.info()"
      ],
      "metadata": {
        "id": "KTmpSPFWHwxm",
        "outputId": "a83b2209-14bb-474c-f3ed-424c9213ac3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 285 entries, 0 to 284\n",
            "Data columns (total 41 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   class          285 non-null    int64  \n",
            " 1   node-caps      285 non-null    int64  \n",
            " 2   breast         285 non-null    int64  \n",
            " 3   new_column     285 non-null    float64\n",
            " 4   age_29         285 non-null    bool   \n",
            " 5   age_39         285 non-null    bool   \n",
            " 6   age_49         285 non-null    bool   \n",
            " 7   age_59         285 non-null    bool   \n",
            " 8   age_69         285 non-null    bool   \n",
            " 9   age_79         285 non-null    bool   \n",
            " 10  inv-nodes_2    285 non-null    bool   \n",
            " 11  inv-nodes_5    285 non-null    bool   \n",
            " 12  inv-nodes_8    285 non-null    bool   \n",
            " 13  inv-nodes_11   285 non-null    bool   \n",
            " 14  inv-nodes_14   285 non-null    bool   \n",
            " 15  inv-nodes_17   285 non-null    bool   \n",
            " 16  inv-nodes_26   285 non-null    bool   \n",
            " 17  deg-malig_1    285 non-null    bool   \n",
            " 18  deg-malig_2    285 non-null    bool   \n",
            " 19  deg-malig_3    285 non-null    bool   \n",
            " 20  tumor-size_4   285 non-null    bool   \n",
            " 21  tumor-size_9   285 non-null    bool   \n",
            " 22  tumor-size_14  285 non-null    bool   \n",
            " 23  tumor-size_19  285 non-null    bool   \n",
            " 24  tumor-size_24  285 non-null    bool   \n",
            " 25  tumor-size_29  285 non-null    bool   \n",
            " 26  tumor-size_34  285 non-null    bool   \n",
            " 27  tumor-size_39  285 non-null    bool   \n",
            " 28  tumor-size_44  285 non-null    bool   \n",
            " 29  tumor-size_49  285 non-null    bool   \n",
            " 30  tumor-size_54  285 non-null    bool   \n",
            " 31  irradiat_0     285 non-null    bool   \n",
            " 32  irradiat_1     285 non-null    bool   \n",
            " 33  breast-quad_1  285 non-null    bool   \n",
            " 34  breast-quad_2  285 non-null    bool   \n",
            " 35  breast-quad_3  285 non-null    bool   \n",
            " 36  breast-quad_4  285 non-null    bool   \n",
            " 37  breast-quad_5  285 non-null    bool   \n",
            " 38  menopause_1    285 non-null    bool   \n",
            " 39  menopause_2    285 non-null    bool   \n",
            " 40  menopause_3    285 non-null    bool   \n",
            "dtypes: bool(37), float64(1), int64(3)\n",
            "memory usage: 19.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'class'\n",
        "target_column = 'class'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Apply PCA for dimensionality reduction and GINI for feature selection\n",
        "# Standard scaling before PCA is essential\n",
        "#scaler = StandardScaler()\n",
        "#X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Applying PCA (reduce to 95% variance retained)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Duplicate the data 10 times\n",
        "X_duplicated = pd.concat([pd.DataFrame(X_pca)] * 10, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 10, ignore_index=True)\n",
        "\n",
        "# Combine features and target for easier manipulation\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1)\n",
        "\n",
        "# Insert rows ensuring a minimum gap of 3 between duplicates\n",
        "final_data = []\n",
        "for i in range(10):  # Loop over the 10 copies\n",
        "    gap_start = i * 3\n",
        "    final_data[gap_start:gap_start] = df_duplicated.iloc[i * len(df_encoded):(i + 1) * len(df_encoded)].values.tolist()\n",
        "\n",
        "# Convert back to DataFrame after interspersing duplicates\n",
        "df_interspersed = pd.DataFrame(final_data, columns=list(df_duplicated.columns) )\n",
        "\n",
        "# Shuffle the final DataFrame in a highly random order\n",
        "df_shuffled = df_interspersed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_shuffled.drop(target_column, axis=1)\n",
        "y_shuffled = df_shuffled[target_column]\n",
        "\n",
        "# Define classifiers and parameter grids\n",
        "classifiers = {\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'NaiveBayes': GaussianNB(),\n",
        "    'SVM': SVC(),\n",
        "    'ZeroR': DummyClassifier(strategy='most_frequent')\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'DecisionTree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'NaiveBayes': {},  # No hyperparameters to tune for GaussianNB\n",
        "    'ZeroR': {}  # No hyperparameters for DummyClassifier (ZeroR)\n",
        "}\n",
        "\n",
        "# Different train-test split ratios\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        print(f\"\\nTraining {clf_name} with split {split_ratio}...\")\n",
        "\n",
        "        if clf_name in param_grids and param_grids[clf_name]:\n",
        "            # Brute force with GridSearchCV for classifiers with hyperparameters\n",
        "            grid_search = GridSearchCV(clf, param_grids[clf_name], n_jobs=-1, verbose=1)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "        else:\n",
        "            # For classifiers without hyperparameters to tune (like NaiveBayes, ZeroR)\n",
        "            clf.fit(X_train, y_train)\n",
        "            best_model = clf\n",
        "\n",
        "        # Evaluate the best model on the test data\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{clf_name} Accuracy with split {split_ratio}: {accuracy}\")\n",
        "        print(f\"{clf_name} Classification Report with split {split_ratio}:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Store the best model for each classifier and split\n",
        "        best_models[(clf_name, split_ratio)] = best_model\n",
        "\n",
        "# After completing all splits, the best models for each classifier and split will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "PRiGf8naI7Mi",
        "outputId": "5d57ed41-a329-4113-e00e-b96a6482d8bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating with train_test_split ratio: 0.95 train, 0.05 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.05...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.05: 0.958041958041958\n",
            "DecisionTree Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.05...\n",
            "NaiveBayes Accuracy with split 0.05: 0.7832167832167832\n",
            "NaiveBayes Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.86      0.86       107\n",
            "         1.0       0.57      0.56      0.56        36\n",
            "\n",
            "    accuracy                           0.78       143\n",
            "   macro avg       0.71      0.71      0.71       143\n",
            "weighted avg       0.78      0.78      0.78       143\n",
            "\n",
            "\n",
            "Training SVM with split 0.05...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.05: 0.958041958041958\n",
            "SVM Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.05...\n",
            "ZeroR Accuracy with split 0.05: 0.7482517482517482\n",
            "ZeroR Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      1.00      0.86       107\n",
            "         1.0       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.75       143\n",
            "   macro avg       0.37      0.50      0.43       143\n",
            "weighted avg       0.56      0.75      0.64       143\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.94 train, 0.06 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.06...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.06: 0.9649122807017544\n",
            "DecisionTree Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.06...\n",
            "NaiveBayes Accuracy with split 0.06: 0.7543859649122807\n",
            "NaiveBayes Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.85      0.83       125\n",
            "         1.0       0.55      0.50      0.52        46\n",
            "\n",
            "    accuracy                           0.75       171\n",
            "   macro avg       0.68      0.67      0.68       171\n",
            "weighted avg       0.75      0.75      0.75       171\n",
            "\n",
            "\n",
            "Training SVM with split 0.06...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.06: 0.9649122807017544\n",
            "SVM Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.06...\n",
            "ZeroR Accuracy with split 0.06: 0.7309941520467836\n",
            "ZeroR Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      1.00      0.84       125\n",
            "         1.0       0.00      0.00      0.00        46\n",
            "\n",
            "    accuracy                           0.73       171\n",
            "   macro avg       0.37      0.50      0.42       171\n",
            "weighted avg       0.53      0.73      0.62       171\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9299999999999999 train, 0.07 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.07...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.07: 0.97\n",
            "DecisionTree Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.07...\n",
            "NaiveBayes Accuracy with split 0.07: 0.765\n",
            "NaiveBayes Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.86      0.84       145\n",
            "         1.0       0.58      0.53      0.55        55\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.70      0.69      0.70       200\n",
            "weighted avg       0.76      0.77      0.76       200\n",
            "\n",
            "\n",
            "Training SVM with split 0.07...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.07: 0.97\n",
            "SVM Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.07...\n",
            "ZeroR Accuracy with split 0.07: 0.725\n",
            "ZeroR Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      1.00      0.84       145\n",
            "         1.0       0.00      0.00      0.00        55\n",
            "\n",
            "    accuracy                           0.72       200\n",
            "   macro avg       0.36      0.50      0.42       200\n",
            "weighted avg       0.53      0.72      0.61       200\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.92 train, 0.08 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.08...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.08: 0.9692982456140351\n",
            "DecisionTree Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       162\n",
            "         1.0       0.94      0.95      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.96      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.08...\n",
            "NaiveBayes Accuracy with split 0.08: 0.75\n",
            "NaiveBayes Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.85      0.83       162\n",
            "         1.0       0.58      0.50      0.54        66\n",
            "\n",
            "    accuracy                           0.75       228\n",
            "   macro avg       0.69      0.68      0.68       228\n",
            "weighted avg       0.74      0.75      0.74       228\n",
            "\n",
            "\n",
            "Training SVM with split 0.08...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.08: 0.9692982456140351\n",
            "SVM Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       162\n",
            "         1.0       0.93      0.97      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.97      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.08...\n",
            "ZeroR Accuracy with split 0.08: 0.7105263157894737\n",
            "ZeroR Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       162\n",
            "         1.0       0.00      0.00      0.00        66\n",
            "\n",
            "    accuracy                           0.71       228\n",
            "   macro avg       0.36      0.50      0.42       228\n",
            "weighted avg       0.50      0.71      0.59       228\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.91 train, 0.09 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.09...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.09: 0.9727626459143969\n",
            "DecisionTree Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       182\n",
            "         1.0       0.95      0.96      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.97      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.09...\n",
            "NaiveBayes Accuracy with split 0.09: 0.7587548638132295\n",
            "NaiveBayes Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84       182\n",
            "         1.0       0.61      0.49      0.54        75\n",
            "\n",
            "    accuracy                           0.76       257\n",
            "   macro avg       0.71      0.68      0.69       257\n",
            "weighted avg       0.75      0.76      0.75       257\n",
            "\n",
            "\n",
            "Training SVM with split 0.09...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.09: 0.9727626459143969\n",
            "SVM Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       182\n",
            "         1.0       0.94      0.97      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.96      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.09...\n",
            "ZeroR Accuracy with split 0.09: 0.708171206225681\n",
            "ZeroR Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       182\n",
            "         1.0       0.00      0.00      0.00        75\n",
            "\n",
            "    accuracy                           0.71       257\n",
            "   macro avg       0.35      0.50      0.41       257\n",
            "weighted avg       0.50      0.71      0.59       257\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9 train, 0.1 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.1...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.1: 0.9719298245614035\n",
            "DecisionTree Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       199\n",
            "         1.0       0.94      0.97      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.1...\n",
            "NaiveBayes Accuracy with split 0.1: 0.7508771929824561\n",
            "NaiveBayes Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.83       199\n",
            "         1.0       0.61      0.48      0.54        86\n",
            "\n",
            "    accuracy                           0.75       285\n",
            "   macro avg       0.70      0.67      0.68       285\n",
            "weighted avg       0.74      0.75      0.74       285\n",
            "\n",
            "\n",
            "Training SVM with split 0.1...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.1: 0.9719298245614035\n",
            "SVM Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       199\n",
            "         1.0       0.93      0.98      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.1...\n",
            "ZeroR Accuracy with split 0.1: 0.6982456140350877\n",
            "ZeroR Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       199\n",
            "         1.0       0.00      0.00      0.00        86\n",
            "\n",
            "    accuracy                           0.70       285\n",
            "   macro avg       0.35      0.50      0.41       285\n",
            "weighted avg       0.49      0.70      0.57       285\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.89 train, 0.11 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.11...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.11: 0.9745222929936306\n",
            "DecisionTree Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       220\n",
            "         1.0       0.95      0.97      0.96        94\n",
            "\n",
            "    accuracy                           0.97       314\n",
            "   macro avg       0.97      0.97      0.97       314\n",
            "weighted avg       0.97      0.97      0.97       314\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.11...\n",
            "NaiveBayes Accuracy with split 0.11: 0.7484076433121019\n",
            "NaiveBayes Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.86      0.83       220\n",
            "         1.0       0.60      0.49      0.54        94\n",
            "\n",
            "    accuracy                           0.75       314\n",
            "   macro avg       0.70      0.67      0.68       314\n",
            "weighted avg       0.74      0.75      0.74       314\n",
            "\n",
            "\n",
            "Training SVM with split 0.11...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.11: 0.9745222929936306\n",
            "SVM Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       220\n",
            "         1.0       0.94      0.98      0.96        94\n",
            "\n",
            "    accuracy                           0.97       314\n",
            "   macro avg       0.96      0.98      0.97       314\n",
            "weighted avg       0.98      0.97      0.97       314\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.11...\n",
            "ZeroR Accuracy with split 0.11: 0.7006369426751592\n",
            "ZeroR Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       220\n",
            "         1.0       0.00      0.00      0.00        94\n",
            "\n",
            "    accuracy                           0.70       314\n",
            "   macro avg       0.35      0.50      0.41       314\n",
            "weighted avg       0.49      0.70      0.58       314\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.88 train, 0.12 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.12...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.12: 0.9678362573099415\n",
            "DecisionTree Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       235\n",
            "         1.0       0.94      0.95      0.95       107\n",
            "\n",
            "    accuracy                           0.97       342\n",
            "   macro avg       0.96      0.96      0.96       342\n",
            "weighted avg       0.97      0.97      0.97       342\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.12...\n",
            "NaiveBayes Accuracy with split 0.12: 0.7397660818713451\n",
            "NaiveBayes Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       235\n",
            "         1.0       0.61      0.48      0.53       107\n",
            "\n",
            "    accuracy                           0.74       342\n",
            "   macro avg       0.70      0.67      0.68       342\n",
            "weighted avg       0.73      0.74      0.73       342\n",
            "\n",
            "\n",
            "Training SVM with split 0.12...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.12: 0.9678362573099415\n",
            "SVM Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       235\n",
            "         1.0       0.94      0.95      0.95       107\n",
            "\n",
            "    accuracy                           0.97       342\n",
            "   macro avg       0.96      0.96      0.96       342\n",
            "weighted avg       0.97      0.97      0.97       342\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.12...\n",
            "ZeroR Accuracy with split 0.12: 0.6871345029239766\n",
            "ZeroR Classification Report with split 0.12:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       235\n",
            "         1.0       0.00      0.00      0.00       107\n",
            "\n",
            "    accuracy                           0.69       342\n",
            "   macro avg       0.34      0.50      0.41       342\n",
            "weighted avg       0.47      0.69      0.56       342\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.87 train, 0.13 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.13...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.13: 0.9703504043126685\n",
            "DecisionTree Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       251\n",
            "         1.0       0.95      0.96      0.95       120\n",
            "\n",
            "    accuracy                           0.97       371\n",
            "   macro avg       0.97      0.97      0.97       371\n",
            "weighted avg       0.97      0.97      0.97       371\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.13...\n",
            "NaiveBayes Accuracy with split 0.13: 0.738544474393531\n",
            "NaiveBayes Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       251\n",
            "         1.0       0.63      0.47      0.54       120\n",
            "\n",
            "    accuracy                           0.74       371\n",
            "   macro avg       0.70      0.67      0.68       371\n",
            "weighted avg       0.73      0.74      0.73       371\n",
            "\n",
            "\n",
            "Training SVM with split 0.13...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.13: 0.9703504043126685\n",
            "SVM Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       251\n",
            "         1.0       0.95      0.96      0.95       120\n",
            "\n",
            "    accuracy                           0.97       371\n",
            "   macro avg       0.97      0.97      0.97       371\n",
            "weighted avg       0.97      0.97      0.97       371\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.13...\n",
            "ZeroR Accuracy with split 0.13: 0.6765498652291105\n",
            "ZeroR Classification Report with split 0.13:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       251\n",
            "         1.0       0.00      0.00      0.00       120\n",
            "\n",
            "    accuracy                           0.68       371\n",
            "   macro avg       0.34      0.50      0.40       371\n",
            "weighted avg       0.46      0.68      0.55       371\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.86 train, 0.14 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.14...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.14: 0.9725\n",
            "DecisionTree Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       274\n",
            "         1.0       0.95      0.96      0.96       126\n",
            "\n",
            "    accuracy                           0.97       400\n",
            "   macro avg       0.97      0.97      0.97       400\n",
            "weighted avg       0.97      0.97      0.97       400\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.14...\n",
            "NaiveBayes Accuracy with split 0.14: 0.735\n",
            "NaiveBayes Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       274\n",
            "         1.0       0.60      0.47      0.53       126\n",
            "\n",
            "    accuracy                           0.73       400\n",
            "   macro avg       0.69      0.66      0.67       400\n",
            "weighted avg       0.72      0.73      0.72       400\n",
            "\n",
            "\n",
            "Training SVM with split 0.14...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.14: 0.9725\n",
            "SVM Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       274\n",
            "         1.0       0.95      0.96      0.96       126\n",
            "\n",
            "    accuracy                           0.97       400\n",
            "   macro avg       0.97      0.97      0.97       400\n",
            "weighted avg       0.97      0.97      0.97       400\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.14...\n",
            "ZeroR Accuracy with split 0.14: 0.685\n",
            "ZeroR Classification Report with split 0.14:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       274\n",
            "         1.0       0.00      0.00      0.00       126\n",
            "\n",
            "    accuracy                           0.69       400\n",
            "   macro avg       0.34      0.50      0.41       400\n",
            "weighted avg       0.47      0.69      0.56       400\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.85 train, 0.15 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.15...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.15: 0.969626168224299\n",
            "DecisionTree Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       293\n",
            "         1.0       0.94      0.96      0.95       135\n",
            "\n",
            "    accuracy                           0.97       428\n",
            "   macro avg       0.96      0.97      0.97       428\n",
            "weighted avg       0.97      0.97      0.97       428\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.15...\n",
            "NaiveBayes Accuracy with split 0.15: 0.735981308411215\n",
            "NaiveBayes Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       293\n",
            "         1.0       0.60      0.47      0.53       135\n",
            "\n",
            "    accuracy                           0.74       428\n",
            "   macro avg       0.69      0.67      0.67       428\n",
            "weighted avg       0.72      0.74      0.73       428\n",
            "\n",
            "\n",
            "Training SVM with split 0.15...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.15: 0.969626168224299\n",
            "SVM Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       293\n",
            "         1.0       0.94      0.96      0.95       135\n",
            "\n",
            "    accuracy                           0.97       428\n",
            "   macro avg       0.96      0.97      0.97       428\n",
            "weighted avg       0.97      0.97      0.97       428\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.15...\n",
            "ZeroR Accuracy with split 0.15: 0.6845794392523364\n",
            "ZeroR Classification Report with split 0.15:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       293\n",
            "         1.0       0.00      0.00      0.00       135\n",
            "\n",
            "    accuracy                           0.68       428\n",
            "   macro avg       0.34      0.50      0.41       428\n",
            "weighted avg       0.47      0.68      0.56       428\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.84 train, 0.16 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.16...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.16: 0.9671052631578947\n",
            "DecisionTree Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       315\n",
            "         1.0       0.93      0.96      0.95       141\n",
            "\n",
            "    accuracy                           0.97       456\n",
            "   macro avg       0.96      0.97      0.96       456\n",
            "weighted avg       0.97      0.97      0.97       456\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.16...\n",
            "NaiveBayes Accuracy with split 0.16: 0.7412280701754386\n",
            "NaiveBayes Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       315\n",
            "         1.0       0.61      0.47      0.53       141\n",
            "\n",
            "    accuracy                           0.74       456\n",
            "   macro avg       0.69      0.67      0.67       456\n",
            "weighted avg       0.73      0.74      0.73       456\n",
            "\n",
            "\n",
            "Training SVM with split 0.16...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.16: 0.9671052631578947\n",
            "SVM Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       315\n",
            "         1.0       0.93      0.96      0.95       141\n",
            "\n",
            "    accuracy                           0.97       456\n",
            "   macro avg       0.96      0.97      0.96       456\n",
            "weighted avg       0.97      0.97      0.97       456\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.16...\n",
            "ZeroR Accuracy with split 0.16: 0.6907894736842105\n",
            "ZeroR Classification Report with split 0.16:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       315\n",
            "         1.0       0.00      0.00      0.00       141\n",
            "\n",
            "    accuracy                           0.69       456\n",
            "   macro avg       0.35      0.50      0.41       456\n",
            "weighted avg       0.48      0.69      0.56       456\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.83 train, 0.17 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.17...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.17: 0.9670103092783505\n",
            "DecisionTree Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       335\n",
            "         1.0       0.94      0.95      0.95       150\n",
            "\n",
            "    accuracy                           0.97       485\n",
            "   macro avg       0.96      0.96      0.96       485\n",
            "weighted avg       0.97      0.97      0.97       485\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.17...\n",
            "NaiveBayes Accuracy with split 0.17: 0.7422680412371134\n",
            "NaiveBayes Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       335\n",
            "         1.0       0.61      0.47      0.53       150\n",
            "\n",
            "    accuracy                           0.74       485\n",
            "   macro avg       0.70      0.67      0.68       485\n",
            "weighted avg       0.73      0.74      0.73       485\n",
            "\n",
            "\n",
            "Training SVM with split 0.17...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.17: 0.9670103092783505\n",
            "SVM Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       335\n",
            "         1.0       0.94      0.95      0.95       150\n",
            "\n",
            "    accuracy                           0.97       485\n",
            "   macro avg       0.96      0.96      0.96       485\n",
            "weighted avg       0.97      0.97      0.97       485\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.17...\n",
            "ZeroR Accuracy with split 0.17: 0.6907216494845361\n",
            "ZeroR Classification Report with split 0.17:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       335\n",
            "         1.0       0.00      0.00      0.00       150\n",
            "\n",
            "    accuracy                           0.69       485\n",
            "   macro avg       0.35      0.50      0.41       485\n",
            "weighted avg       0.48      0.69      0.56       485\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8200000000000001 train, 0.18 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.18...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.18: 0.9688109161793372\n",
            "DecisionTree Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       352\n",
            "         1.0       0.94      0.96      0.95       161\n",
            "\n",
            "    accuracy                           0.97       513\n",
            "   macro avg       0.96      0.97      0.96       513\n",
            "weighted avg       0.97      0.97      0.97       513\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.18...\n",
            "NaiveBayes Accuracy with split 0.18: 0.7309941520467836\n",
            "NaiveBayes Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.86      0.82       352\n",
            "         1.0       0.60      0.44      0.51       161\n",
            "\n",
            "    accuracy                           0.73       513\n",
            "   macro avg       0.68      0.65      0.66       513\n",
            "weighted avg       0.72      0.73      0.72       513\n",
            "\n",
            "\n",
            "Training SVM with split 0.18...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.18: 0.9688109161793372\n",
            "SVM Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       352\n",
            "         1.0       0.94      0.96      0.95       161\n",
            "\n",
            "    accuracy                           0.97       513\n",
            "   macro avg       0.96      0.97      0.96       513\n",
            "weighted avg       0.97      0.97      0.97       513\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.18...\n",
            "ZeroR Accuracy with split 0.18: 0.6861598440545809\n",
            "ZeroR Classification Report with split 0.18:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       352\n",
            "         1.0       0.00      0.00      0.00       161\n",
            "\n",
            "    accuracy                           0.69       513\n",
            "   macro avg       0.34      0.50      0.41       513\n",
            "weighted avg       0.47      0.69      0.56       513\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.81 train, 0.19 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.19...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.19: 0.9704797047970479\n",
            "DecisionTree Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       370\n",
            "         1.0       0.95      0.96      0.95       172\n",
            "\n",
            "    accuracy                           0.97       542\n",
            "   macro avg       0.96      0.97      0.97       542\n",
            "weighted avg       0.97      0.97      0.97       542\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.19...\n",
            "NaiveBayes Accuracy with split 0.19: 0.7380073800738007\n",
            "NaiveBayes Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       370\n",
            "         1.0       0.61      0.47      0.53       172\n",
            "\n",
            "    accuracy                           0.74       542\n",
            "   macro avg       0.70      0.67      0.68       542\n",
            "weighted avg       0.73      0.74      0.73       542\n",
            "\n",
            "\n",
            "Training SVM with split 0.19...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.19: 0.9704797047970479\n",
            "SVM Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       370\n",
            "         1.0       0.95      0.96      0.95       172\n",
            "\n",
            "    accuracy                           0.97       542\n",
            "   macro avg       0.96      0.97      0.97       542\n",
            "weighted avg       0.97      0.97      0.97       542\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.19...\n",
            "ZeroR Accuracy with split 0.19: 0.6826568265682657\n",
            "ZeroR Classification Report with split 0.19:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       370\n",
            "         1.0       0.00      0.00      0.00       172\n",
            "\n",
            "    accuracy                           0.68       542\n",
            "   macro avg       0.34      0.50      0.41       542\n",
            "weighted avg       0.47      0.68      0.55       542\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.8 train, 0.2 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.2...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.2: 0.9701754385964912\n",
            "DecisionTree Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       394\n",
            "         1.0       0.93      0.97      0.95       176\n",
            "\n",
            "    accuracy                           0.97       570\n",
            "   macro avg       0.96      0.97      0.97       570\n",
            "weighted avg       0.97      0.97      0.97       570\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.2...\n",
            "NaiveBayes Accuracy with split 0.2: 0.7421052631578947\n",
            "NaiveBayes Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       394\n",
            "         1.0       0.61      0.47      0.53       176\n",
            "\n",
            "    accuracy                           0.74       570\n",
            "   macro avg       0.70      0.67      0.68       570\n",
            "weighted avg       0.73      0.74      0.73       570\n",
            "\n",
            "\n",
            "Training SVM with split 0.2...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.2: 0.9701754385964912\n",
            "SVM Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       394\n",
            "         1.0       0.93      0.97      0.95       176\n",
            "\n",
            "    accuracy                           0.97       570\n",
            "   macro avg       0.96      0.97      0.97       570\n",
            "weighted avg       0.97      0.97      0.97       570\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.2...\n",
            "ZeroR Accuracy with split 0.2: 0.6912280701754386\n",
            "ZeroR Classification Report with split 0.2:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       394\n",
            "         1.0       0.00      0.00      0.00       176\n",
            "\n",
            "    accuracy                           0.69       570\n",
            "   macro avg       0.35      0.50      0.41       570\n",
            "weighted avg       0.48      0.69      0.57       570\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.79 train, 0.21 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.21...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.21: 0.9699499165275459\n",
            "DecisionTree Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       416\n",
            "         1.0       0.94      0.97      0.95       183\n",
            "\n",
            "    accuracy                           0.97       599\n",
            "   macro avg       0.96      0.97      0.96       599\n",
            "weighted avg       0.97      0.97      0.97       599\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.21...\n",
            "NaiveBayes Accuracy with split 0.21: 0.7378964941569283\n",
            "NaiveBayes Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       416\n",
            "         1.0       0.59      0.45      0.51       183\n",
            "\n",
            "    accuracy                           0.74       599\n",
            "   macro avg       0.69      0.66      0.67       599\n",
            "weighted avg       0.72      0.74      0.73       599\n",
            "\n",
            "\n",
            "Training SVM with split 0.21...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.21: 0.9699499165275459\n",
            "SVM Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       416\n",
            "         1.0       0.94      0.97      0.95       183\n",
            "\n",
            "    accuracy                           0.97       599\n",
            "   macro avg       0.96      0.97      0.96       599\n",
            "weighted avg       0.97      0.97      0.97       599\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.21...\n",
            "ZeroR Accuracy with split 0.21: 0.6944908180300501\n",
            "ZeroR Classification Report with split 0.21:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       416\n",
            "         1.0       0.00      0.00      0.00       183\n",
            "\n",
            "    accuracy                           0.69       599\n",
            "   macro avg       0.35      0.50      0.41       599\n",
            "weighted avg       0.48      0.69      0.57       599\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.78 train, 0.22 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.22...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.22: 0.9681020733652312\n",
            "DecisionTree Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       438\n",
            "         1.0       0.93      0.96      0.95       189\n",
            "\n",
            "    accuracy                           0.97       627\n",
            "   macro avg       0.96      0.97      0.96       627\n",
            "weighted avg       0.97      0.97      0.97       627\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.22...\n",
            "NaiveBayes Accuracy with split 0.22: 0.7384370015948963\n",
            "NaiveBayes Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       438\n",
            "         1.0       0.58      0.47      0.52       189\n",
            "\n",
            "    accuracy                           0.74       627\n",
            "   macro avg       0.69      0.66      0.67       627\n",
            "weighted avg       0.73      0.74      0.73       627\n",
            "\n",
            "\n",
            "Training SVM with split 0.22...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.22: 0.9681020733652312\n",
            "SVM Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       438\n",
            "         1.0       0.93      0.96      0.95       189\n",
            "\n",
            "    accuracy                           0.97       627\n",
            "   macro avg       0.96      0.97      0.96       627\n",
            "weighted avg       0.97      0.97      0.97       627\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.22...\n",
            "ZeroR Accuracy with split 0.22: 0.6985645933014354\n",
            "ZeroR Classification Report with split 0.22:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       438\n",
            "         1.0       0.00      0.00      0.00       189\n",
            "\n",
            "    accuracy                           0.70       627\n",
            "   macro avg       0.35      0.50      0.41       627\n",
            "weighted avg       0.49      0.70      0.57       627\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.77 train, 0.23 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.23...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.23: 0.9679878048780488\n",
            "DecisionTree Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       459\n",
            "         1.0       0.94      0.96      0.95       197\n",
            "\n",
            "    accuracy                           0.97       656\n",
            "   macro avg       0.96      0.97      0.96       656\n",
            "weighted avg       0.97      0.97      0.97       656\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.23...\n",
            "NaiveBayes Accuracy with split 0.23: 0.7423780487804879\n",
            "NaiveBayes Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.82       459\n",
            "         1.0       0.59      0.45      0.51       197\n",
            "\n",
            "    accuracy                           0.74       656\n",
            "   macro avg       0.69      0.66      0.67       656\n",
            "weighted avg       0.73      0.74      0.73       656\n",
            "\n",
            "\n",
            "Training SVM with split 0.23...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.23: 0.9679878048780488\n",
            "SVM Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       459\n",
            "         1.0       0.94      0.96      0.95       197\n",
            "\n",
            "    accuracy                           0.97       656\n",
            "   macro avg       0.96      0.97      0.96       656\n",
            "weighted avg       0.97      0.97      0.97       656\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.23...\n",
            "ZeroR Accuracy with split 0.23: 0.6996951219512195\n",
            "ZeroR Classification Report with split 0.23:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       459\n",
            "         1.0       0.00      0.00      0.00       197\n",
            "\n",
            "    accuracy                           0.70       656\n",
            "   macro avg       0.35      0.50      0.41       656\n",
            "weighted avg       0.49      0.70      0.58       656\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.76 train, 0.24 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.24...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.24: 0.9692982456140351\n",
            "DecisionTree Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       477\n",
            "         1.0       0.94      0.96      0.95       207\n",
            "\n",
            "    accuracy                           0.97       684\n",
            "   macro avg       0.96      0.97      0.96       684\n",
            "weighted avg       0.97      0.97      0.97       684\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.24...\n",
            "NaiveBayes Accuracy with split 0.24: 0.7412280701754386\n",
            "NaiveBayes Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.82       477\n",
            "         1.0       0.59      0.45      0.52       207\n",
            "\n",
            "    accuracy                           0.74       684\n",
            "   macro avg       0.69      0.66      0.67       684\n",
            "weighted avg       0.73      0.74      0.73       684\n",
            "\n",
            "\n",
            "Training SVM with split 0.24...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.24: 0.9692982456140351\n",
            "SVM Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       477\n",
            "         1.0       0.94      0.96      0.95       207\n",
            "\n",
            "    accuracy                           0.97       684\n",
            "   macro avg       0.96      0.97      0.96       684\n",
            "weighted avg       0.97      0.97      0.97       684\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.24...\n",
            "ZeroR Accuracy with split 0.24: 0.6973684210526315\n",
            "ZeroR Classification Report with split 0.24:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       477\n",
            "         1.0       0.00      0.00      0.00       207\n",
            "\n",
            "    accuracy                           0.70       684\n",
            "   macro avg       0.35      0.50      0.41       684\n",
            "weighted avg       0.49      0.70      0.57       684\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.75 train, 0.25 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.25...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.25: 0.97054698457223\n",
            "DecisionTree Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       495\n",
            "         1.0       0.94      0.96      0.95       218\n",
            "\n",
            "    accuracy                           0.97       713\n",
            "   macro avg       0.96      0.97      0.97       713\n",
            "weighted avg       0.97      0.97      0.97       713\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.25...\n",
            "NaiveBayes Accuracy with split 0.25: 0.7349228611500701\n",
            "NaiveBayes Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       495\n",
            "         1.0       0.59      0.44      0.51       218\n",
            "\n",
            "    accuracy                           0.73       713\n",
            "   macro avg       0.68      0.65      0.66       713\n",
            "weighted avg       0.72      0.73      0.72       713\n",
            "\n",
            "\n",
            "Training SVM with split 0.25...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.25: 0.97054698457223\n",
            "SVM Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       495\n",
            "         1.0       0.94      0.96      0.95       218\n",
            "\n",
            "    accuracy                           0.97       713\n",
            "   macro avg       0.96      0.97      0.97       713\n",
            "weighted avg       0.97      0.97      0.97       713\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.25...\n",
            "ZeroR Accuracy with split 0.25: 0.6942496493688639\n",
            "ZeroR Classification Report with split 0.25:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       495\n",
            "         1.0       0.00      0.00      0.00       218\n",
            "\n",
            "    accuracy                           0.69       713\n",
            "   macro avg       0.35      0.50      0.41       713\n",
            "weighted avg       0.48      0.69      0.57       713\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.74 train, 0.26 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.26...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.26: 0.970310391363023\n",
            "DecisionTree Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       518\n",
            "         1.0       0.94      0.96      0.95       223\n",
            "\n",
            "    accuracy                           0.97       741\n",
            "   macro avg       0.96      0.97      0.96       741\n",
            "weighted avg       0.97      0.97      0.97       741\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.26...\n",
            "NaiveBayes Accuracy with split 0.26: 0.7368421052631579\n",
            "NaiveBayes Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       518\n",
            "         1.0       0.58      0.44      0.50       223\n",
            "\n",
            "    accuracy                           0.74       741\n",
            "   macro avg       0.68      0.65      0.66       741\n",
            "weighted avg       0.72      0.74      0.73       741\n",
            "\n",
            "\n",
            "Training SVM with split 0.26...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.26: 0.970310391363023\n",
            "SVM Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       518\n",
            "         1.0       0.94      0.96      0.95       223\n",
            "\n",
            "    accuracy                           0.97       741\n",
            "   macro avg       0.96      0.97      0.96       741\n",
            "weighted avg       0.97      0.97      0.97       741\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.26...\n",
            "ZeroR Accuracy with split 0.26: 0.699055330634278\n",
            "ZeroR Classification Report with split 0.26:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       518\n",
            "         1.0       0.00      0.00      0.00       223\n",
            "\n",
            "    accuracy                           0.70       741\n",
            "   macro avg       0.35      0.50      0.41       741\n",
            "weighted avg       0.49      0.70      0.58       741\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.73 train, 0.27 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.27...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.27: 0.9714285714285714\n",
            "DecisionTree Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       535\n",
            "         1.0       0.95      0.95      0.95       235\n",
            "\n",
            "    accuracy                           0.97       770\n",
            "   macro avg       0.97      0.97      0.97       770\n",
            "weighted avg       0.97      0.97      0.97       770\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.27...\n",
            "NaiveBayes Accuracy with split 0.27: 0.7363636363636363\n",
            "NaiveBayes Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       535\n",
            "         1.0       0.59      0.44      0.51       235\n",
            "\n",
            "    accuracy                           0.74       770\n",
            "   macro avg       0.69      0.65      0.66       770\n",
            "weighted avg       0.72      0.74      0.72       770\n",
            "\n",
            "\n",
            "Training SVM with split 0.27...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.27: 0.9714285714285714\n",
            "SVM Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       535\n",
            "         1.0       0.95      0.95      0.95       235\n",
            "\n",
            "    accuracy                           0.97       770\n",
            "   macro avg       0.97      0.97      0.97       770\n",
            "weighted avg       0.97      0.97      0.97       770\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.27...\n",
            "ZeroR Accuracy with split 0.27: 0.6948051948051948\n",
            "ZeroR Classification Report with split 0.27:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       535\n",
            "         1.0       0.00      0.00      0.00       235\n",
            "\n",
            "    accuracy                           0.69       770\n",
            "   macro avg       0.35      0.50      0.41       770\n",
            "weighted avg       0.48      0.69      0.57       770\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.72 train, 0.28 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.28...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.28: 0.9724655819774718\n",
            "DecisionTree Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       556\n",
            "         1.0       0.95      0.95      0.95       243\n",
            "\n",
            "    accuracy                           0.97       799\n",
            "   macro avg       0.97      0.97      0.97       799\n",
            "weighted avg       0.97      0.97      0.97       799\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.28...\n",
            "NaiveBayes Accuracy with split 0.28: 0.7396745932415519\n",
            "NaiveBayes Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       556\n",
            "         1.0       0.60      0.45      0.51       243\n",
            "\n",
            "    accuracy                           0.74       799\n",
            "   macro avg       0.69      0.66      0.67       799\n",
            "weighted avg       0.73      0.74      0.73       799\n",
            "\n",
            "\n",
            "Training SVM with split 0.28...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.28: 0.9724655819774718\n",
            "SVM Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       556\n",
            "         1.0       0.95      0.95      0.95       243\n",
            "\n",
            "    accuracy                           0.97       799\n",
            "   macro avg       0.97      0.97      0.97       799\n",
            "weighted avg       0.97      0.97      0.97       799\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.28...\n",
            "ZeroR Accuracy with split 0.28: 0.6958698372966208\n",
            "ZeroR Classification Report with split 0.28:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       556\n",
            "         1.0       0.00      0.00      0.00       243\n",
            "\n",
            "    accuracy                           0.70       799\n",
            "   macro avg       0.35      0.50      0.41       799\n",
            "weighted avg       0.48      0.70      0.57       799\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.71 train, 0.29 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.29...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.29: 0.973397823458283\n",
            "DecisionTree Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       574\n",
            "         1.0       0.96      0.96      0.96       253\n",
            "\n",
            "    accuracy                           0.97       827\n",
            "   macro avg       0.97      0.97      0.97       827\n",
            "weighted avg       0.97      0.97      0.97       827\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.29...\n",
            "NaiveBayes Accuracy with split 0.29: 0.7424425634824667\n",
            "NaiveBayes Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       574\n",
            "         1.0       0.61      0.45      0.52       253\n",
            "\n",
            "    accuracy                           0.74       827\n",
            "   macro avg       0.69      0.66      0.67       827\n",
            "weighted avg       0.73      0.74      0.73       827\n",
            "\n",
            "\n",
            "Training SVM with split 0.29...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.29: 0.973397823458283\n",
            "SVM Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       574\n",
            "         1.0       0.96      0.96      0.96       253\n",
            "\n",
            "    accuracy                           0.97       827\n",
            "   macro avg       0.97      0.97      0.97       827\n",
            "weighted avg       0.97      0.97      0.97       827\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.29...\n",
            "ZeroR Accuracy with split 0.29: 0.694074969770254\n",
            "ZeroR Classification Report with split 0.29:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       574\n",
            "         1.0       0.00      0.00      0.00       253\n",
            "\n",
            "    accuracy                           0.69       827\n",
            "   macro avg       0.35      0.50      0.41       827\n",
            "weighted avg       0.48      0.69      0.57       827\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.7 train, 0.3 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.3...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.3: 0.9742690058479532\n",
            "DecisionTree Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       592\n",
            "         1.0       0.96      0.96      0.96       263\n",
            "\n",
            "    accuracy                           0.97       855\n",
            "   macro avg       0.97      0.97      0.97       855\n",
            "weighted avg       0.97      0.97      0.97       855\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.3...\n",
            "NaiveBayes Accuracy with split 0.3: 0.7380116959064328\n",
            "NaiveBayes Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       592\n",
            "         1.0       0.60      0.45      0.52       263\n",
            "\n",
            "    accuracy                           0.74       855\n",
            "   macro avg       0.69      0.66      0.67       855\n",
            "weighted avg       0.72      0.74      0.73       855\n",
            "\n",
            "\n",
            "Training SVM with split 0.3...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.3: 0.9742690058479532\n",
            "SVM Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       592\n",
            "         1.0       0.96      0.96      0.96       263\n",
            "\n",
            "    accuracy                           0.97       855\n",
            "   macro avg       0.97      0.97      0.97       855\n",
            "weighted avg       0.97      0.97      0.97       855\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.3...\n",
            "ZeroR Accuracy with split 0.3: 0.6923976608187135\n",
            "ZeroR Classification Report with split 0.3:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       592\n",
            "         1.0       0.00      0.00      0.00       263\n",
            "\n",
            "    accuracy                           0.69       855\n",
            "   macro avg       0.35      0.50      0.41       855\n",
            "weighted avg       0.48      0.69      0.57       855\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.69 train, 0.31 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.31...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.31: 0.9728506787330317\n",
            "DecisionTree Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       611\n",
            "         1.0       0.96      0.96      0.96       273\n",
            "\n",
            "    accuracy                           0.97       884\n",
            "   macro avg       0.97      0.97      0.97       884\n",
            "weighted avg       0.97      0.97      0.97       884\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.31...\n",
            "NaiveBayes Accuracy with split 0.31: 0.7375565610859729\n",
            "NaiveBayes Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       611\n",
            "         1.0       0.60      0.45      0.52       273\n",
            "\n",
            "    accuracy                           0.74       884\n",
            "   macro avg       0.69      0.66      0.67       884\n",
            "weighted avg       0.72      0.74      0.73       884\n",
            "\n",
            "\n",
            "Training SVM with split 0.31...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.31: 0.9728506787330317\n",
            "SVM Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       611\n",
            "         1.0       0.96      0.96      0.96       273\n",
            "\n",
            "    accuracy                           0.97       884\n",
            "   macro avg       0.97      0.97      0.97       884\n",
            "weighted avg       0.97      0.97      0.97       884\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.31...\n",
            "ZeroR Accuracy with split 0.31: 0.6911764705882353\n",
            "ZeroR Classification Report with split 0.31:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       611\n",
            "         1.0       0.00      0.00      0.00       273\n",
            "\n",
            "    accuracy                           0.69       884\n",
            "   macro avg       0.35      0.50      0.41       884\n",
            "weighted avg       0.48      0.69      0.56       884\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6799999999999999 train, 0.32 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.32...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.32: 0.9725877192982456\n",
            "DecisionTree Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       629\n",
            "         1.0       0.96      0.95      0.96       283\n",
            "\n",
            "    accuracy                           0.97       912\n",
            "   macro avg       0.97      0.97      0.97       912\n",
            "weighted avg       0.97      0.97      0.97       912\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.32...\n",
            "NaiveBayes Accuracy with split 0.32: 0.743421052631579\n",
            "NaiveBayes Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       629\n",
            "         1.0       0.61      0.48      0.54       283\n",
            "\n",
            "    accuracy                           0.74       912\n",
            "   macro avg       0.70      0.67      0.68       912\n",
            "weighted avg       0.73      0.74      0.73       912\n",
            "\n",
            "\n",
            "Training SVM with split 0.32...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.32: 0.9725877192982456\n",
            "SVM Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       629\n",
            "         1.0       0.96      0.95      0.96       283\n",
            "\n",
            "    accuracy                           0.97       912\n",
            "   macro avg       0.97      0.97      0.97       912\n",
            "weighted avg       0.97      0.97      0.97       912\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.32...\n",
            "ZeroR Accuracy with split 0.32: 0.6896929824561403\n",
            "ZeroR Classification Report with split 0.32:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       629\n",
            "         1.0       0.00      0.00      0.00       283\n",
            "\n",
            "    accuracy                           0.69       912\n",
            "   macro avg       0.34      0.50      0.41       912\n",
            "weighted avg       0.48      0.69      0.56       912\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6699999999999999 train, 0.33 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.33...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.33: 0.973432518597237\n",
            "DecisionTree Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       649\n",
            "         1.0       0.96      0.96      0.96       292\n",
            "\n",
            "    accuracy                           0.97       941\n",
            "   macro avg       0.97      0.97      0.97       941\n",
            "weighted avg       0.97      0.97      0.97       941\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.33...\n",
            "NaiveBayes Accuracy with split 0.33: 0.7523910733262487\n",
            "NaiveBayes Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.87      0.83       649\n",
            "         1.0       0.63      0.50      0.55       292\n",
            "\n",
            "    accuracy                           0.75       941\n",
            "   macro avg       0.71      0.68      0.69       941\n",
            "weighted avg       0.74      0.75      0.74       941\n",
            "\n",
            "\n",
            "Training SVM with split 0.33...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.33: 0.973432518597237\n",
            "SVM Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       649\n",
            "         1.0       0.96      0.96      0.96       292\n",
            "\n",
            "    accuracy                           0.97       941\n",
            "   macro avg       0.97      0.97      0.97       941\n",
            "weighted avg       0.97      0.97      0.97       941\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.33...\n",
            "ZeroR Accuracy with split 0.33: 0.6896918172157279\n",
            "ZeroR Classification Report with split 0.33:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       649\n",
            "         1.0       0.00      0.00      0.00       292\n",
            "\n",
            "    accuracy                           0.69       941\n",
            "   macro avg       0.34      0.50      0.41       941\n",
            "weighted avg       0.48      0.69      0.56       941\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6599999999999999 train, 0.34 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.34...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.34: 0.9742268041237113\n",
            "DecisionTree Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       669\n",
            "         1.0       0.96      0.96      0.96       301\n",
            "\n",
            "    accuracy                           0.97       970\n",
            "   macro avg       0.97      0.97      0.97       970\n",
            "weighted avg       0.97      0.97      0.97       970\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.34...\n",
            "NaiveBayes Accuracy with split 0.34: 0.7422680412371134\n",
            "NaiveBayes Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       669\n",
            "         1.0       0.61      0.49      0.54       301\n",
            "\n",
            "    accuracy                           0.74       970\n",
            "   macro avg       0.70      0.67      0.68       970\n",
            "weighted avg       0.73      0.74      0.73       970\n",
            "\n",
            "\n",
            "Training SVM with split 0.34...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.34: 0.9742268041237113\n",
            "SVM Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       669\n",
            "         1.0       0.96      0.96      0.96       301\n",
            "\n",
            "    accuracy                           0.97       970\n",
            "   macro avg       0.97      0.97      0.97       970\n",
            "weighted avg       0.97      0.97      0.97       970\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.34...\n",
            "ZeroR Accuracy with split 0.34: 0.6896907216494845\n",
            "ZeroR Classification Report with split 0.34:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82       669\n",
            "         1.0       0.00      0.00      0.00       301\n",
            "\n",
            "    accuracy                           0.69       970\n",
            "   macro avg       0.34      0.50      0.41       970\n",
            "weighted avg       0.48      0.69      0.56       970\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.65 train, 0.35 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.35...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.35: 0.9749498997995992\n",
            "DecisionTree Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       683\n",
            "         1.0       0.96      0.96      0.96       315\n",
            "\n",
            "    accuracy                           0.97       998\n",
            "   macro avg       0.97      0.97      0.97       998\n",
            "weighted avg       0.97      0.97      0.97       998\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.35...\n",
            "NaiveBayes Accuracy with split 0.35: 0.7404809619238477\n",
            "NaiveBayes Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       683\n",
            "         1.0       0.61      0.48      0.54       315\n",
            "\n",
            "    accuracy                           0.74       998\n",
            "   macro avg       0.70      0.67      0.68       998\n",
            "weighted avg       0.73      0.74      0.73       998\n",
            "\n",
            "\n",
            "Training SVM with split 0.35...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.35: 0.9749498997995992\n",
            "SVM Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       683\n",
            "         1.0       0.96      0.96      0.96       315\n",
            "\n",
            "    accuracy                           0.97       998\n",
            "   macro avg       0.97      0.97      0.97       998\n",
            "weighted avg       0.97      0.97      0.97       998\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.35...\n",
            "ZeroR Accuracy with split 0.35: 0.6843687374749499\n",
            "ZeroR Classification Report with split 0.35:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       683\n",
            "         1.0       0.00      0.00      0.00       315\n",
            "\n",
            "    accuracy                           0.68       998\n",
            "   macro avg       0.34      0.50      0.41       998\n",
            "weighted avg       0.47      0.68      0.56       998\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.64 train, 0.36 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.36...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.36: 0.9756335282651072\n",
            "DecisionTree Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       701\n",
            "         1.0       0.96      0.96      0.96       325\n",
            "\n",
            "    accuracy                           0.98      1026\n",
            "   macro avg       0.97      0.97      0.97      1026\n",
            "weighted avg       0.98      0.98      0.98      1026\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.36...\n",
            "NaiveBayes Accuracy with split 0.36: 0.7456140350877193\n",
            "NaiveBayes Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       701\n",
            "         1.0       0.62      0.49      0.55       325\n",
            "\n",
            "    accuracy                           0.75      1026\n",
            "   macro avg       0.71      0.68      0.69      1026\n",
            "weighted avg       0.73      0.75      0.74      1026\n",
            "\n",
            "\n",
            "Training SVM with split 0.36...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.36: 0.9756335282651072\n",
            "SVM Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       701\n",
            "         1.0       0.96      0.96      0.96       325\n",
            "\n",
            "    accuracy                           0.98      1026\n",
            "   macro avg       0.97      0.97      0.97      1026\n",
            "weighted avg       0.98      0.98      0.98      1026\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.36...\n",
            "ZeroR Accuracy with split 0.36: 0.6832358674463938\n",
            "ZeroR Classification Report with split 0.36:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       701\n",
            "         1.0       0.00      0.00      0.00       325\n",
            "\n",
            "    accuracy                           0.68      1026\n",
            "   macro avg       0.34      0.50      0.41      1026\n",
            "weighted avg       0.47      0.68      0.55      1026\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.63 train, 0.37 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.37...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.37: 0.9753554502369668\n",
            "DecisionTree Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       720\n",
            "         1.0       0.96      0.96      0.96       335\n",
            "\n",
            "    accuracy                           0.98      1055\n",
            "   macro avg       0.97      0.97      0.97      1055\n",
            "weighted avg       0.98      0.98      0.98      1055\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.37...\n",
            "NaiveBayes Accuracy with split 0.37: 0.7450236966824645\n",
            "NaiveBayes Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.86      0.82       720\n",
            "         1.0       0.63      0.49      0.55       335\n",
            "\n",
            "    accuracy                           0.75      1055\n",
            "   macro avg       0.71      0.68      0.69      1055\n",
            "weighted avg       0.73      0.75      0.74      1055\n",
            "\n",
            "\n",
            "Training SVM with split 0.37...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.37: 0.9753554502369668\n",
            "SVM Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       720\n",
            "         1.0       0.95      0.97      0.96       335\n",
            "\n",
            "    accuracy                           0.98      1055\n",
            "   macro avg       0.97      0.97      0.97      1055\n",
            "weighted avg       0.98      0.98      0.98      1055\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.37...\n",
            "ZeroR Accuracy with split 0.37: 0.6824644549763034\n",
            "ZeroR Classification Report with split 0.37:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       720\n",
            "         1.0       0.00      0.00      0.00       335\n",
            "\n",
            "    accuracy                           0.68      1055\n",
            "   macro avg       0.34      0.50      0.41      1055\n",
            "weighted avg       0.47      0.68      0.55      1055\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.62 train, 0.38 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.38...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.38: 0.9759926131117267\n",
            "DecisionTree Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       739\n",
            "         1.0       0.96      0.96      0.96       344\n",
            "\n",
            "    accuracy                           0.98      1083\n",
            "   macro avg       0.97      0.97      0.97      1083\n",
            "weighted avg       0.98      0.98      0.98      1083\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.38...\n",
            "NaiveBayes Accuracy with split 0.38: 0.7460757156048015\n",
            "NaiveBayes Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.86      0.82       739\n",
            "         1.0       0.63      0.49      0.55       344\n",
            "\n",
            "    accuracy                           0.75      1083\n",
            "   macro avg       0.71      0.68      0.69      1083\n",
            "weighted avg       0.74      0.75      0.74      1083\n",
            "\n",
            "\n",
            "Training SVM with split 0.38...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.38: 0.9695290858725761\n",
            "SVM Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       739\n",
            "         1.0       0.95      0.95      0.95       344\n",
            "\n",
            "    accuracy                           0.97      1083\n",
            "   macro avg       0.96      0.97      0.96      1083\n",
            "weighted avg       0.97      0.97      0.97      1083\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.38...\n",
            "ZeroR Accuracy with split 0.38: 0.6823638042474608\n",
            "ZeroR Classification Report with split 0.38:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       739\n",
            "         1.0       0.00      0.00      0.00       344\n",
            "\n",
            "    accuracy                           0.68      1083\n",
            "   macro avg       0.34      0.50      0.41      1083\n",
            "weighted avg       0.47      0.68      0.55      1083\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.61 train, 0.39 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.39...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.39: 0.9766187050359713\n",
            "DecisionTree Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       759\n",
            "         1.0       0.96      0.96      0.96       353\n",
            "\n",
            "    accuracy                           0.98      1112\n",
            "   macro avg       0.97      0.97      0.97      1112\n",
            "weighted avg       0.98      0.98      0.98      1112\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.39...\n",
            "NaiveBayes Accuracy with split 0.39: 0.7455035971223022\n",
            "NaiveBayes Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       759\n",
            "         1.0       0.63      0.49      0.55       353\n",
            "\n",
            "    accuracy                           0.75      1112\n",
            "   macro avg       0.71      0.68      0.69      1112\n",
            "weighted avg       0.73      0.75      0.74      1112\n",
            "\n",
            "\n",
            "Training SVM with split 0.39...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.39: 0.9703237410071942\n",
            "SVM Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       759\n",
            "         1.0       0.95      0.95      0.95       353\n",
            "\n",
            "    accuracy                           0.97      1112\n",
            "   macro avg       0.97      0.97      0.97      1112\n",
            "weighted avg       0.97      0.97      0.97      1112\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.39...\n",
            "ZeroR Accuracy with split 0.39: 0.6825539568345323\n",
            "ZeroR Classification Report with split 0.39:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       759\n",
            "         1.0       0.00      0.00      0.00       353\n",
            "\n",
            "    accuracy                           0.68      1112\n",
            "   macro avg       0.34      0.50      0.41      1112\n",
            "weighted avg       0.47      0.68      0.55      1112\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.6 train, 0.4 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.4...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.4: 0.9771929824561404\n",
            "DecisionTree Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       775\n",
            "         1.0       0.96      0.96      0.96       365\n",
            "\n",
            "    accuracy                           0.98      1140\n",
            "   macro avg       0.97      0.97      0.97      1140\n",
            "weighted avg       0.98      0.98      0.98      1140\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.4...\n",
            "NaiveBayes Accuracy with split 0.4: 0.7491228070175439\n",
            "NaiveBayes Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.83       775\n",
            "         1.0       0.64      0.49      0.55       365\n",
            "\n",
            "    accuracy                           0.75      1140\n",
            "   macro avg       0.71      0.68      0.69      1140\n",
            "weighted avg       0.74      0.75      0.74      1140\n",
            "\n",
            "\n",
            "Training SVM with split 0.4...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.4: 0.9710526315789474\n",
            "SVM Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       775\n",
            "         1.0       0.95      0.96      0.95       365\n",
            "\n",
            "    accuracy                           0.97      1140\n",
            "   macro avg       0.97      0.97      0.97      1140\n",
            "weighted avg       0.97      0.97      0.97      1140\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.4...\n",
            "ZeroR Accuracy with split 0.4: 0.6798245614035088\n",
            "ZeroR Classification Report with split 0.4:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       775\n",
            "         1.0       0.00      0.00      0.00       365\n",
            "\n",
            "    accuracy                           0.68      1140\n",
            "   macro avg       0.34      0.50      0.40      1140\n",
            "weighted avg       0.46      0.68      0.55      1140\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5900000000000001 train, 0.41 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.41...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.41: 0.9769033361847733\n",
            "DecisionTree Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       795\n",
            "         1.0       0.97      0.96      0.96       374\n",
            "\n",
            "    accuracy                           0.98      1169\n",
            "   macro avg       0.97      0.97      0.97      1169\n",
            "weighted avg       0.98      0.98      0.98      1169\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.41...\n",
            "NaiveBayes Accuracy with split 0.41: 0.7442258340461934\n",
            "NaiveBayes Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       795\n",
            "         1.0       0.63      0.48      0.54       374\n",
            "\n",
            "    accuracy                           0.74      1169\n",
            "   macro avg       0.71      0.67      0.68      1169\n",
            "weighted avg       0.73      0.74      0.73      1169\n",
            "\n",
            "\n",
            "Training SVM with split 0.41...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.41: 0.9717707442258341\n",
            "SVM Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       795\n",
            "         1.0       0.97      0.94      0.96       374\n",
            "\n",
            "    accuracy                           0.97      1169\n",
            "   macro avg       0.97      0.96      0.97      1169\n",
            "weighted avg       0.97      0.97      0.97      1169\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.41...\n",
            "ZeroR Accuracy with split 0.41: 0.6800684345594525\n",
            "ZeroR Classification Report with split 0.41:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       795\n",
            "         1.0       0.00      0.00      0.00       374\n",
            "\n",
            "    accuracy                           0.68      1169\n",
            "   macro avg       0.34      0.50      0.40      1169\n",
            "weighted avg       0.46      0.68      0.55      1169\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5800000000000001 train, 0.42 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.42...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.42: 0.9774436090225563\n",
            "DecisionTree Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       813\n",
            "         1.0       0.97      0.96      0.96       384\n",
            "\n",
            "    accuracy                           0.98      1197\n",
            "   macro avg       0.97      0.97      0.97      1197\n",
            "weighted avg       0.98      0.98      0.98      1197\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.42...\n",
            "NaiveBayes Accuracy with split 0.42: 0.7410192147034252\n",
            "NaiveBayes Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82       813\n",
            "         1.0       0.63      0.47      0.54       384\n",
            "\n",
            "    accuracy                           0.74      1197\n",
            "   macro avg       0.70      0.67      0.68      1197\n",
            "weighted avg       0.73      0.74      0.73      1197\n",
            "\n",
            "\n",
            "Training SVM with split 0.42...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.42: 0.9724310776942355\n",
            "SVM Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       813\n",
            "         1.0       0.97      0.94      0.96       384\n",
            "\n",
            "    accuracy                           0.97      1197\n",
            "   macro avg       0.97      0.96      0.97      1197\n",
            "weighted avg       0.97      0.97      0.97      1197\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.42...\n",
            "ZeroR Accuracy with split 0.42: 0.6791979949874687\n",
            "ZeroR Classification Report with split 0.42:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       813\n",
            "         1.0       0.00      0.00      0.00       384\n",
            "\n",
            "    accuracy                           0.68      1197\n",
            "   macro avg       0.34      0.50      0.40      1197\n",
            "weighted avg       0.46      0.68      0.55      1197\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5700000000000001 train, 0.43 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.43...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.43: 0.9771615008156607\n",
            "DecisionTree Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       829\n",
            "         1.0       0.97      0.96      0.96       397\n",
            "\n",
            "    accuracy                           0.98      1226\n",
            "   macro avg       0.97      0.97      0.97      1226\n",
            "weighted avg       0.98      0.98      0.98      1226\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.43...\n",
            "NaiveBayes Accuracy with split 0.43: 0.736541598694943\n",
            "NaiveBayes Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       829\n",
            "         1.0       0.62      0.47      0.53       397\n",
            "\n",
            "    accuracy                           0.74      1226\n",
            "   macro avg       0.70      0.67      0.68      1226\n",
            "weighted avg       0.72      0.74      0.72      1226\n",
            "\n",
            "\n",
            "Training SVM with split 0.43...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.43: 0.9722675367047309\n",
            "SVM Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       829\n",
            "         1.0       0.97      0.94      0.96       397\n",
            "\n",
            "    accuracy                           0.97      1226\n",
            "   macro avg       0.97      0.96      0.97      1226\n",
            "weighted avg       0.97      0.97      0.97      1226\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.43...\n",
            "ZeroR Accuracy with split 0.43: 0.6761827079934747\n",
            "ZeroR Classification Report with split 0.43:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       829\n",
            "         1.0       0.00      0.00      0.00       397\n",
            "\n",
            "    accuracy                           0.68      1226\n",
            "   macro avg       0.34      0.50      0.40      1226\n",
            "weighted avg       0.46      0.68      0.55      1226\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.56 train, 0.44 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.44...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.44: 0.9776714513556619\n",
            "DecisionTree Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       852\n",
            "         1.0       0.98      0.95      0.96       402\n",
            "\n",
            "    accuracy                           0.98      1254\n",
            "   macro avg       0.98      0.97      0.97      1254\n",
            "weighted avg       0.98      0.98      0.98      1254\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.44...\n",
            "NaiveBayes Accuracy with split 0.44: 0.7320574162679426\n",
            "NaiveBayes Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.81       852\n",
            "         1.0       0.61      0.45      0.52       402\n",
            "\n",
            "    accuracy                           0.73      1254\n",
            "   macro avg       0.69      0.66      0.67      1254\n",
            "weighted avg       0.72      0.73      0.72      1254\n",
            "\n",
            "\n",
            "Training SVM with split 0.44...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.44: 0.9728867623604466\n",
            "SVM Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       852\n",
            "         1.0       0.98      0.93      0.96       402\n",
            "\n",
            "    accuracy                           0.97      1254\n",
            "   macro avg       0.98      0.96      0.97      1254\n",
            "weighted avg       0.97      0.97      0.97      1254\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.44...\n",
            "ZeroR Accuracy with split 0.44: 0.6794258373205742\n",
            "ZeroR Classification Report with split 0.44:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       852\n",
            "         1.0       0.00      0.00      0.00       402\n",
            "\n",
            "    accuracy                           0.68      1254\n",
            "   macro avg       0.34      0.50      0.40      1254\n",
            "weighted avg       0.46      0.68      0.55      1254\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.55 train, 0.45 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.45...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.45: 0.9781761496492596\n",
            "DecisionTree Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       876\n",
            "         1.0       0.98      0.95      0.97       407\n",
            "\n",
            "    accuracy                           0.98      1283\n",
            "   macro avg       0.98      0.97      0.97      1283\n",
            "weighted avg       0.98      0.98      0.98      1283\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.45...\n",
            "NaiveBayes Accuracy with split 0.45: 0.7357755261106781\n",
            "NaiveBayes Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       876\n",
            "         1.0       0.62      0.44      0.52       407\n",
            "\n",
            "    accuracy                           0.74      1283\n",
            "   macro avg       0.69      0.66      0.67      1283\n",
            "weighted avg       0.72      0.74      0.72      1283\n",
            "\n",
            "\n",
            "Training SVM with split 0.45...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.45: 0.9734996102883866\n",
            "SVM Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       876\n",
            "         1.0       0.98      0.93      0.96       407\n",
            "\n",
            "    accuracy                           0.97      1283\n",
            "   macro avg       0.98      0.96      0.97      1283\n",
            "weighted avg       0.97      0.97      0.97      1283\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.45...\n",
            "ZeroR Accuracy with split 0.45: 0.6827747466874513\n",
            "ZeroR Classification Report with split 0.45:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       876\n",
            "         1.0       0.00      0.00      0.00       407\n",
            "\n",
            "    accuracy                           0.68      1283\n",
            "   macro avg       0.34      0.50      0.41      1283\n",
            "weighted avg       0.47      0.68      0.55      1283\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.54 train, 0.46 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.46...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.46: 0.977116704805492\n",
            "DecisionTree Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       900\n",
            "         1.0       0.96      0.96      0.96       411\n",
            "\n",
            "    accuracy                           0.98      1311\n",
            "   macro avg       0.97      0.97      0.97      1311\n",
            "weighted avg       0.98      0.98      0.98      1311\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.46...\n",
            "NaiveBayes Accuracy with split 0.46: 0.738367658276125\n",
            "NaiveBayes Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       900\n",
            "         1.0       0.61      0.45      0.52       411\n",
            "\n",
            "    accuracy                           0.74      1311\n",
            "   macro avg       0.69      0.66      0.67      1311\n",
            "weighted avg       0.72      0.74      0.73      1311\n",
            "\n",
            "\n",
            "Training SVM with split 0.46...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.46: 0.9725400457665904\n",
            "SVM Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       900\n",
            "         1.0       0.98      0.93      0.96       411\n",
            "\n",
            "    accuracy                           0.97      1311\n",
            "   macro avg       0.97      0.96      0.97      1311\n",
            "weighted avg       0.97      0.97      0.97      1311\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.46...\n",
            "ZeroR Accuracy with split 0.46: 0.6864988558352403\n",
            "ZeroR Classification Report with split 0.46:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       900\n",
            "         1.0       0.00      0.00      0.00       411\n",
            "\n",
            "    accuracy                           0.69      1311\n",
            "   macro avg       0.34      0.50      0.41      1311\n",
            "weighted avg       0.47      0.69      0.56      1311\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.53 train, 0.47 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.47...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.47: 0.9776119402985075\n",
            "DecisionTree Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       918\n",
            "         1.0       0.98      0.95      0.96       422\n",
            "\n",
            "    accuracy                           0.98      1340\n",
            "   macro avg       0.98      0.97      0.97      1340\n",
            "weighted avg       0.98      0.98      0.98      1340\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.47...\n",
            "NaiveBayes Accuracy with split 0.47: 0.7373134328358208\n",
            "NaiveBayes Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       918\n",
            "         1.0       0.62      0.44      0.51       422\n",
            "\n",
            "    accuracy                           0.74      1340\n",
            "   macro avg       0.69      0.66      0.67      1340\n",
            "weighted avg       0.72      0.74      0.72      1340\n",
            "\n",
            "\n",
            "Training SVM with split 0.47...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.47: 0.9731343283582089\n",
            "SVM Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       918\n",
            "         1.0       0.98      0.94      0.96       422\n",
            "\n",
            "    accuracy                           0.97      1340\n",
            "   macro avg       0.97      0.96      0.97      1340\n",
            "weighted avg       0.97      0.97      0.97      1340\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.47...\n",
            "ZeroR Accuracy with split 0.47: 0.6850746268656717\n",
            "ZeroR Classification Report with split 0.47:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       918\n",
            "         1.0       0.00      0.00      0.00       422\n",
            "\n",
            "    accuracy                           0.69      1340\n",
            "   macro avg       0.34      0.50      0.41      1340\n",
            "weighted avg       0.47      0.69      0.56      1340\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.52 train, 0.48 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.48...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.48: 0.9780701754385965\n",
            "DecisionTree Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       937\n",
            "         1.0       0.98      0.95      0.96       431\n",
            "\n",
            "    accuracy                           0.98      1368\n",
            "   macro avg       0.98      0.97      0.97      1368\n",
            "weighted avg       0.98      0.98      0.98      1368\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.48...\n",
            "NaiveBayes Accuracy with split 0.48: 0.72953216374269\n",
            "NaiveBayes Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.87      0.82       937\n",
            "         1.0       0.60      0.42      0.49       431\n",
            "\n",
            "    accuracy                           0.73      1368\n",
            "   macro avg       0.68      0.65      0.65      1368\n",
            "weighted avg       0.71      0.73      0.71      1368\n",
            "\n",
            "\n",
            "Training SVM with split 0.48...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.48: 0.9736842105263158\n",
            "SVM Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       937\n",
            "         1.0       0.98      0.94      0.96       431\n",
            "\n",
            "    accuracy                           0.97      1368\n",
            "   macro avg       0.97      0.96      0.97      1368\n",
            "weighted avg       0.97      0.97      0.97      1368\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.48...\n",
            "ZeroR Accuracy with split 0.48: 0.6849415204678363\n",
            "ZeroR Classification Report with split 0.48:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      1.00      0.81       937\n",
            "         1.0       0.00      0.00      0.00       431\n",
            "\n",
            "    accuracy                           0.68      1368\n",
            "   macro avg       0.34      0.50      0.41      1368\n",
            "weighted avg       0.47      0.68      0.56      1368\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.51 train, 0.49 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.49...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.49: 0.9778095919828204\n",
            "DecisionTree Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       959\n",
            "         1.0       0.98      0.95      0.96       438\n",
            "\n",
            "    accuracy                           0.98      1397\n",
            "   macro avg       0.98      0.97      0.97      1397\n",
            "weighted avg       0.98      0.98      0.98      1397\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.49...\n",
            "NaiveBayes Accuracy with split 0.49: 0.737294201861131\n",
            "NaiveBayes Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82       959\n",
            "         1.0       0.62      0.42      0.50       438\n",
            "\n",
            "    accuracy                           0.74      1397\n",
            "   macro avg       0.69      0.65      0.66      1397\n",
            "weighted avg       0.72      0.74      0.72      1397\n",
            "\n",
            "\n",
            "Training SVM with split 0.49...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.49: 0.9735146743020758\n",
            "SVM Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       959\n",
            "         1.0       0.98      0.94      0.96       438\n",
            "\n",
            "    accuracy                           0.97      1397\n",
            "   macro avg       0.97      0.96      0.97      1397\n",
            "weighted avg       0.97      0.97      0.97      1397\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.49...\n",
            "ZeroR Accuracy with split 0.49: 0.686471009305655\n",
            "ZeroR Classification Report with split 0.49:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       959\n",
            "         1.0       0.00      0.00      0.00       438\n",
            "\n",
            "    accuracy                           0.69      1397\n",
            "   macro avg       0.34      0.50      0.41      1397\n",
            "weighted avg       0.47      0.69      0.56      1397\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.5 train, 0.5 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.5...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.5: 0.9775438596491228\n",
            "DecisionTree Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       979\n",
            "         1.0       0.98      0.95      0.96       446\n",
            "\n",
            "    accuracy                           0.98      1425\n",
            "   macro avg       0.98      0.97      0.97      1425\n",
            "weighted avg       0.98      0.98      0.98      1425\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.5...\n",
            "NaiveBayes Accuracy with split 0.5: 0.7452631578947368\n",
            "NaiveBayes Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.83       979\n",
            "         1.0       0.63      0.44      0.52       446\n",
            "\n",
            "    accuracy                           0.75      1425\n",
            "   macro avg       0.71      0.66      0.67      1425\n",
            "weighted avg       0.73      0.75      0.73      1425\n",
            "\n",
            "\n",
            "Training SVM with split 0.5...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.5: 0.9733333333333334\n",
            "SVM Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       979\n",
            "         1.0       0.98      0.93      0.96       446\n",
            "\n",
            "    accuracy                           0.97      1425\n",
            "   macro avg       0.97      0.96      0.97      1425\n",
            "weighted avg       0.97      0.97      0.97      1425\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.5...\n",
            "ZeroR Accuracy with split 0.5: 0.6870175438596491\n",
            "ZeroR Classification Report with split 0.5:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       979\n",
            "         1.0       0.00      0.00      0.00       446\n",
            "\n",
            "    accuracy                           0.69      1425\n",
            "   macro avg       0.34      0.50      0.41      1425\n",
            "weighted avg       0.47      0.69      0.56      1425\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.49 train, 0.51 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.51...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.51: 0.9766162310866575\n",
            "DecisionTree Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98       999\n",
            "         1.0       0.98      0.95      0.96       455\n",
            "\n",
            "    accuracy                           0.98      1454\n",
            "   macro avg       0.98      0.97      0.97      1454\n",
            "weighted avg       0.98      0.98      0.98      1454\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.51...\n",
            "NaiveBayes Accuracy with split 0.51: 0.7427785419532325\n",
            "NaiveBayes Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.83       999\n",
            "         1.0       0.63      0.44      0.51       455\n",
            "\n",
            "    accuracy                           0.74      1454\n",
            "   macro avg       0.70      0.66      0.67      1454\n",
            "weighted avg       0.73      0.74      0.73      1454\n",
            "\n",
            "\n",
            "Training SVM with split 0.51...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.51: 0.9724896836313618\n",
            "SVM Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98       999\n",
            "         1.0       0.99      0.92      0.95       455\n",
            "\n",
            "    accuracy                           0.97      1454\n",
            "   macro avg       0.98      0.96      0.97      1454\n",
            "weighted avg       0.97      0.97      0.97      1454\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.51...\n",
            "ZeroR Accuracy with split 0.51: 0.68707015130674\n",
            "ZeroR Classification Report with split 0.51:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81       999\n",
            "         1.0       0.00      0.00      0.00       455\n",
            "\n",
            "    accuracy                           0.69      1454\n",
            "   macro avg       0.34      0.50      0.41      1454\n",
            "weighted avg       0.47      0.69      0.56      1454\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.48 train, 0.52 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.52...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.52: 0.9763832658569501\n",
            "DecisionTree Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1019\n",
            "         1.0       0.99      0.94      0.96       463\n",
            "\n",
            "    accuracy                           0.98      1482\n",
            "   macro avg       0.98      0.97      0.97      1482\n",
            "weighted avg       0.98      0.98      0.98      1482\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.52...\n",
            "NaiveBayes Accuracy with split 0.52: 0.7415654520917678\n",
            "NaiveBayes Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.88      0.82      1019\n",
            "         1.0       0.62      0.43      0.51       463\n",
            "\n",
            "    accuracy                           0.74      1482\n",
            "   macro avg       0.70      0.66      0.67      1482\n",
            "weighted avg       0.73      0.74      0.73      1482\n",
            "\n",
            "\n",
            "Training SVM with split 0.52...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.52: 0.97165991902834\n",
            "SVM Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      1.00      0.98      1019\n",
            "         1.0       0.99      0.92      0.95       463\n",
            "\n",
            "    accuracy                           0.97      1482\n",
            "   macro avg       0.98      0.96      0.97      1482\n",
            "weighted avg       0.97      0.97      0.97      1482\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.52...\n",
            "ZeroR Accuracy with split 0.52: 0.6875843454790823\n",
            "ZeroR Classification Report with split 0.52:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.81      1019\n",
            "         1.0       0.00      0.00      0.00       463\n",
            "\n",
            "    accuracy                           0.69      1482\n",
            "   macro avg       0.34      0.50      0.41      1482\n",
            "weighted avg       0.47      0.69      0.56      1482\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.47 train, 0.53 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.53...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.53: 0.9761747187293184\n",
            "DecisionTree Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1045\n",
            "         1.0       0.97      0.95      0.96       466\n",
            "\n",
            "    accuracy                           0.98      1511\n",
            "   macro avg       0.98      0.97      0.97      1511\n",
            "weighted avg       0.98      0.98      0.98      1511\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.53...\n",
            "NaiveBayes Accuracy with split 0.53: 0.7405691594970218\n",
            "NaiveBayes Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.82      1045\n",
            "         1.0       0.61      0.43      0.51       466\n",
            "\n",
            "    accuracy                           0.74      1511\n",
            "   macro avg       0.69      0.66      0.67      1511\n",
            "weighted avg       0.73      0.74      0.73      1511\n",
            "\n",
            "\n",
            "Training SVM with split 0.53...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.53: 0.972203838517538\n",
            "SVM Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1045\n",
            "         1.0       0.99      0.92      0.95       466\n",
            "\n",
            "    accuracy                           0.97      1511\n",
            "   macro avg       0.98      0.96      0.97      1511\n",
            "weighted avg       0.97      0.97      0.97      1511\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.53...\n",
            "ZeroR Accuracy with split 0.53: 0.6915949702183984\n",
            "ZeroR Classification Report with split 0.53:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1045\n",
            "         1.0       0.00      0.00      0.00       466\n",
            "\n",
            "    accuracy                           0.69      1511\n",
            "   macro avg       0.35      0.50      0.41      1511\n",
            "weighted avg       0.48      0.69      0.57      1511\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.45999999999999996 train, 0.54 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.54...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.54: 0.9766081871345029\n",
            "DecisionTree Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1070\n",
            "         1.0       0.97      0.95      0.96       469\n",
            "\n",
            "    accuracy                           0.98      1539\n",
            "   macro avg       0.98      0.97      0.97      1539\n",
            "weighted avg       0.98      0.98      0.98      1539\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.54...\n",
            "NaiveBayes Accuracy with split 0.54: 0.7439896036387265\n",
            "NaiveBayes Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.83      1070\n",
            "         1.0       0.61      0.45      0.52       469\n",
            "\n",
            "    accuracy                           0.74      1539\n",
            "   macro avg       0.70      0.66      0.67      1539\n",
            "weighted avg       0.73      0.74      0.73      1539\n",
            "\n",
            "\n",
            "Training SVM with split 0.54...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.54: 0.9727095516569201\n",
            "SVM Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1070\n",
            "         1.0       0.99      0.92      0.95       469\n",
            "\n",
            "    accuracy                           0.97      1539\n",
            "   macro avg       0.98      0.96      0.97      1539\n",
            "weighted avg       0.97      0.97      0.97      1539\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.54...\n",
            "ZeroR Accuracy with split 0.54: 0.6952566601689408\n",
            "ZeroR Classification Report with split 0.54:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82      1070\n",
            "         1.0       0.00      0.00      0.00       469\n",
            "\n",
            "    accuracy                           0.70      1539\n",
            "   macro avg       0.35      0.50      0.41      1539\n",
            "weighted avg       0.48      0.70      0.57      1539\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.44999999999999996 train, 0.55 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.55...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.55: 0.9770408163265306\n",
            "DecisionTree Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1089\n",
            "         1.0       0.97      0.95      0.96       479\n",
            "\n",
            "    accuracy                           0.98      1568\n",
            "   macro avg       0.98      0.97      0.97      1568\n",
            "weighted avg       0.98      0.98      0.98      1568\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.55...\n",
            "NaiveBayes Accuracy with split 0.55: 0.7429846938775511\n",
            "NaiveBayes Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.83      1089\n",
            "         1.0       0.61      0.44      0.51       479\n",
            "\n",
            "    accuracy                           0.74      1568\n",
            "   macro avg       0.70      0.66      0.67      1568\n",
            "weighted avg       0.73      0.74      0.73      1568\n",
            "\n",
            "\n",
            "Training SVM with split 0.55...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.55: 0.9732142857142857\n",
            "SVM Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1089\n",
            "         1.0       0.99      0.92      0.95       479\n",
            "\n",
            "    accuracy                           0.97      1568\n",
            "   macro avg       0.98      0.96      0.97      1568\n",
            "weighted avg       0.97      0.97      0.97      1568\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.55...\n",
            "ZeroR Accuracy with split 0.55: 0.6945153061224489\n",
            "ZeroR Classification Report with split 0.55:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1089\n",
            "         1.0       0.00      0.00      0.00       479\n",
            "\n",
            "    accuracy                           0.69      1568\n",
            "   macro avg       0.35      0.50      0.41      1568\n",
            "weighted avg       0.48      0.69      0.57      1568\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43999999999999995 train, 0.56 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.56...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.56: 0.9774577332498434\n",
            "DecisionTree Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1110\n",
            "         1.0       0.97      0.95      0.96       487\n",
            "\n",
            "    accuracy                           0.98      1597\n",
            "   macro avg       0.98      0.97      0.97      1597\n",
            "weighted avg       0.98      0.98      0.98      1597\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.56...\n",
            "NaiveBayes Accuracy with split 0.56: 0.7470256731371321\n",
            "NaiveBayes Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.83      1110\n",
            "         1.0       0.62      0.44      0.51       487\n",
            "\n",
            "    accuracy                           0.75      1597\n",
            "   macro avg       0.70      0.66      0.67      1597\n",
            "weighted avg       0.73      0.75      0.73      1597\n",
            "\n",
            "\n",
            "Training SVM with split 0.56...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.56: 0.9737006887914841\n",
            "SVM Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1110\n",
            "         1.0       0.99      0.92      0.96       487\n",
            "\n",
            "    accuracy                           0.97      1597\n",
            "   macro avg       0.98      0.96      0.97      1597\n",
            "weighted avg       0.97      0.97      0.97      1597\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.56...\n",
            "ZeroR Accuracy with split 0.56: 0.6950532247964935\n",
            "ZeroR Classification Report with split 0.56:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82      1110\n",
            "         1.0       0.00      0.00      0.00       487\n",
            "\n",
            "    accuracy                           0.70      1597\n",
            "   macro avg       0.35      0.50      0.41      1597\n",
            "weighted avg       0.48      0.70      0.57      1597\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.43000000000000005 train, 0.57 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.57...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.57: 0.9778461538461538\n",
            "DecisionTree Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1128\n",
            "         1.0       0.98      0.95      0.96       497\n",
            "\n",
            "    accuracy                           0.98      1625\n",
            "   macro avg       0.98      0.97      0.97      1625\n",
            "weighted avg       0.98      0.98      0.98      1625\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.57...\n",
            "NaiveBayes Accuracy with split 0.57: 0.7415384615384616\n",
            "NaiveBayes Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.88      0.82      1128\n",
            "         1.0       0.61      0.44      0.51       497\n",
            "\n",
            "    accuracy                           0.74      1625\n",
            "   macro avg       0.69      0.66      0.67      1625\n",
            "weighted avg       0.73      0.74      0.73      1625\n",
            "\n",
            "\n",
            "Training SVM with split 0.57...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.57: 0.9741538461538461\n",
            "SVM Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      1.00      0.98      1128\n",
            "         1.0       0.99      0.93      0.96       497\n",
            "\n",
            "    accuracy                           0.97      1625\n",
            "   macro avg       0.98      0.96      0.97      1625\n",
            "weighted avg       0.97      0.97      0.97      1625\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.57...\n",
            "ZeroR Accuracy with split 0.57: 0.6941538461538461\n",
            "ZeroR Classification Report with split 0.57:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1128\n",
            "         1.0       0.00      0.00      0.00       497\n",
            "\n",
            "    accuracy                           0.69      1625\n",
            "   macro avg       0.35      0.50      0.41      1625\n",
            "weighted avg       0.48      0.69      0.57      1625\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.42000000000000004 train, 0.58 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.58...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.58: 0.9776164549304295\n",
            "DecisionTree Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1147\n",
            "         1.0       0.98      0.95      0.96       506\n",
            "\n",
            "    accuracy                           0.98      1653\n",
            "   macro avg       0.98      0.97      0.97      1653\n",
            "weighted avg       0.98      0.98      0.98      1653\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.58...\n",
            "NaiveBayes Accuracy with split 0.58: 0.736237144585602\n",
            "NaiveBayes Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82      1147\n",
            "         1.0       0.59      0.44      0.50       506\n",
            "\n",
            "    accuracy                           0.74      1653\n",
            "   macro avg       0.69      0.65      0.66      1653\n",
            "weighted avg       0.72      0.74      0.72      1653\n",
            "\n",
            "\n",
            "Training SVM with split 0.58...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.58: 0.9739866908650938\n",
            "SVM Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98      1147\n",
            "         1.0       0.98      0.94      0.96       506\n",
            "\n",
            "    accuracy                           0.97      1653\n",
            "   macro avg       0.97      0.96      0.97      1653\n",
            "weighted avg       0.97      0.97      0.97      1653\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.58...\n",
            "ZeroR Accuracy with split 0.58: 0.6938898971566848\n",
            "ZeroR Classification Report with split 0.58:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1147\n",
            "         1.0       0.00      0.00      0.00       506\n",
            "\n",
            "    accuracy                           0.69      1653\n",
            "   macro avg       0.35      0.50      0.41      1653\n",
            "weighted avg       0.48      0.69      0.57      1653\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.41000000000000003 train, 0.59 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.59...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.59: 0.9774078478002378\n",
            "DecisionTree Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1167\n",
            "         1.0       0.97      0.95      0.96       515\n",
            "\n",
            "    accuracy                           0.98      1682\n",
            "   macro avg       0.98      0.97      0.97      1682\n",
            "weighted avg       0.98      0.98      0.98      1682\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.59...\n",
            "NaiveBayes Accuracy with split 0.59: 0.7360285374554102\n",
            "NaiveBayes Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82      1167\n",
            "         1.0       0.59      0.43      0.50       515\n",
            "\n",
            "    accuracy                           0.74      1682\n",
            "   macro avg       0.69      0.65      0.66      1682\n",
            "weighted avg       0.72      0.74      0.72      1682\n",
            "\n",
            "\n",
            "Training SVM with split 0.59...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.59: 0.9738406658739596\n",
            "SVM Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98      1167\n",
            "         1.0       0.98      0.94      0.96       515\n",
            "\n",
            "    accuracy                           0.97      1682\n",
            "   macro avg       0.97      0.96      0.97      1682\n",
            "weighted avg       0.97      0.97      0.97      1682\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.59...\n",
            "ZeroR Accuracy with split 0.59: 0.6938168846611177\n",
            "ZeroR Classification Report with split 0.59:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1167\n",
            "         1.0       0.00      0.00      0.00       515\n",
            "\n",
            "    accuracy                           0.69      1682\n",
            "   macro avg       0.35      0.50      0.41      1682\n",
            "weighted avg       0.48      0.69      0.57      1682\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.4 train, 0.6 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.6...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.6: 0.9777777777777777\n",
            "DecisionTree Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98      1184\n",
            "         1.0       0.97      0.95      0.96       526\n",
            "\n",
            "    accuracy                           0.98      1710\n",
            "   macro avg       0.98      0.97      0.97      1710\n",
            "weighted avg       0.98      0.98      0.98      1710\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.6...\n",
            "NaiveBayes Accuracy with split 0.6: 0.7345029239766082\n",
            "NaiveBayes Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82      1184\n",
            "         1.0       0.59      0.43      0.50       526\n",
            "\n",
            "    accuracy                           0.73      1710\n",
            "   macro avg       0.68      0.65      0.66      1710\n",
            "weighted avg       0.72      0.73      0.72      1710\n",
            "\n",
            "\n",
            "Training SVM with split 0.6...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.6: 0.9742690058479532\n",
            "SVM Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.99      0.98      1184\n",
            "         1.0       0.98      0.94      0.96       526\n",
            "\n",
            "    accuracy                           0.97      1710\n",
            "   macro avg       0.97      0.96      0.97      1710\n",
            "weighted avg       0.97      0.97      0.97      1710\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.6...\n",
            "ZeroR Accuracy with split 0.6: 0.6923976608187135\n",
            "ZeroR Classification Report with split 0.6:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      1.00      0.82      1184\n",
            "         1.0       0.00      0.00      0.00       526\n",
            "\n",
            "    accuracy                           0.69      1710\n",
            "   macro avg       0.35      0.50      0.41      1710\n",
            "weighted avg       0.48      0.69      0.57      1710\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df_encoded' is your encoded DataFrame with the target column 'class'\n",
        "target_column = 'class'\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop(target_column, axis=1)\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "# Apply PCA for dimensionality reduction and GINI for feature selection\n",
        "# Standard scaling before PCA is essential\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Applying PCA (reduce to 95% variance retained)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Duplicate the data 10 times\n",
        "X_duplicated = pd.concat([pd.DataFrame(X_pca)] * 10, ignore_index=True)\n",
        "y_duplicated = pd.concat([y] * 10, ignore_index=True)\n",
        "\n",
        "# Combine features and target for easier manipulation\n",
        "df_duplicated = pd.concat([X_duplicated, y_duplicated], axis=1)\n",
        "\n",
        "# Insert rows ensuring a minimum gap of 3 between duplicates\n",
        "final_data = []\n",
        "for i in range(10):  # Loop over the 10 copies\n",
        "    gap_start = i * 3\n",
        "    final_data[gap_start:gap_start] = df_duplicated.iloc[i * len(df_encoded):(i + 1) * len(df_encoded)].values.tolist()\n",
        "\n",
        "# Convert back to DataFrame after interspersing duplicates\n",
        "df_interspersed = pd.DataFrame(final_data, columns=list(df_duplicated.columns) )\n",
        "\n",
        "# Shuffle the final DataFrame in a highly random order\n",
        "df_shuffled = df_interspersed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split shuffled data back into features and target\n",
        "X_shuffled = df_shuffled.drop(target_column, axis=1)\n",
        "y_shuffled = df_shuffled[target_column]\n",
        "\n",
        "# Define classifiers and parameter grids\n",
        "classifiers = {\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'NaiveBayes': GaussianNB(),\n",
        "    'SVM': SVC(),\n",
        "    'ZeroR': DummyClassifier(strategy='most_frequent')\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'DecisionTree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'NaiveBayes': {},  # No hyperparameters to tune for GaussianNB\n",
        "    'ZeroR': {}  # No hyperparameters for DummyClassifier (ZeroR)\n",
        "}\n",
        "\n",
        "# Different train-test split ratios\n",
        "train_test_splits = [\n",
        "    0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
        "    0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24,\n",
        "    0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
        "    0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44,\n",
        "    0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54,\n",
        "    0.55, 0.56, 0.57, 0.58, 0.59, 0.6\n",
        "]\n",
        "\n",
        "best_models = {}\n",
        "for split_ratio in train_test_splits:\n",
        "    print(f\"\\nEvaluating with train_test_split ratio: {1 - split_ratio} train, {split_ratio} test\\n\")\n",
        "\n",
        "    # Split the shuffled data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        print(f\"\\nTraining {clf_name} with split {split_ratio}...\")\n",
        "\n",
        "        if clf_name in param_grids and param_grids[clf_name]:\n",
        "            # Brute force with GridSearchCV for classifiers with hyperparameters\n",
        "            grid_search = GridSearchCV(clf, param_grids[clf_name], n_jobs=-1, verbose=1)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "        else:\n",
        "            # For classifiers without hyperparameters to tune (like NaiveBayes, ZeroR)\n",
        "            clf.fit(X_train, y_train)\n",
        "            best_model = clf\n",
        "\n",
        "        # Evaluate the best model on the test data\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{clf_name} Accuracy with split {split_ratio}: {accuracy}\")\n",
        "        print(f\"{clf_name} Classification Report with split {split_ratio}:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Store the best model for each classifier and split\n",
        "        best_models[(clf_name, split_ratio)] = best_model\n",
        "\n",
        "# After completing all splits, the best models for each classifier and split will be stored in `best_models`\n"
      ],
      "metadata": {
        "id": "qq2cQ1FKTNcX",
        "outputId": "08aa7d59-ca97-45df-a86e-affe13d9a6f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating with train_test_split ratio: 0.95 train, 0.05 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.05...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.05: 0.958041958041958\n",
            "DecisionTree Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.05...\n",
            "NaiveBayes Accuracy with split 0.05: 0.7832167832167832\n",
            "NaiveBayes Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.84      0.85       107\n",
            "         1.0       0.56      0.61      0.59        36\n",
            "\n",
            "    accuracy                           0.78       143\n",
            "   macro avg       0.71      0.73      0.72       143\n",
            "weighted avg       0.79      0.78      0.79       143\n",
            "\n",
            "\n",
            "Training SVM with split 0.05...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.05: 0.958041958041958\n",
            "SVM Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.96      0.97       107\n",
            "         1.0       0.89      0.94      0.92        36\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.94      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.05...\n",
            "ZeroR Accuracy with split 0.05: 0.7482517482517482\n",
            "ZeroR Classification Report with split 0.05:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      1.00      0.86       107\n",
            "         1.0       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.75       143\n",
            "   macro avg       0.37      0.50      0.43       143\n",
            "weighted avg       0.56      0.75      0.64       143\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.94 train, 0.06 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.06...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.06: 0.9649122807017544\n",
            "DecisionTree Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.06...\n",
            "NaiveBayes Accuracy with split 0.06: 0.7602339181286549\n",
            "NaiveBayes Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.83      0.84       125\n",
            "         1.0       0.55      0.57      0.56        46\n",
            "\n",
            "    accuracy                           0.76       171\n",
            "   macro avg       0.70      0.70      0.70       171\n",
            "weighted avg       0.76      0.76      0.76       171\n",
            "\n",
            "\n",
            "Training SVM with split 0.06...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.06: 0.9649122807017544\n",
            "SVM Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       125\n",
            "         1.0       0.92      0.96      0.94        46\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.95      0.96      0.96       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.06...\n",
            "ZeroR Accuracy with split 0.06: 0.7309941520467836\n",
            "ZeroR Classification Report with split 0.06:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      1.00      0.84       125\n",
            "         1.0       0.00      0.00      0.00        46\n",
            "\n",
            "    accuracy                           0.73       171\n",
            "   macro avg       0.37      0.50      0.42       171\n",
            "weighted avg       0.53      0.73      0.62       171\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9299999999999999 train, 0.07 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.07...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.07: 0.97\n",
            "DecisionTree Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.07...\n",
            "NaiveBayes Accuracy with split 0.07: 0.765\n",
            "NaiveBayes Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.83      0.84       145\n",
            "         1.0       0.57      0.58      0.58        55\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.71      0.71      0.71       200\n",
            "weighted avg       0.77      0.77      0.77       200\n",
            "\n",
            "\n",
            "Training SVM with split 0.07...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.07: 0.97\n",
            "SVM Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       145\n",
            "         1.0       0.93      0.96      0.95        55\n",
            "\n",
            "    accuracy                           0.97       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.97      0.97       200\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.07...\n",
            "ZeroR Accuracy with split 0.07: 0.725\n",
            "ZeroR Classification Report with split 0.07:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      1.00      0.84       145\n",
            "         1.0       0.00      0.00      0.00        55\n",
            "\n",
            "    accuracy                           0.72       200\n",
            "   macro avg       0.36      0.50      0.42       200\n",
            "weighted avg       0.53      0.72      0.61       200\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.92 train, 0.08 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.08...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.08: 0.9692982456140351\n",
            "DecisionTree Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       162\n",
            "         1.0       0.94      0.95      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.96      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.08...\n",
            "NaiveBayes Accuracy with split 0.08: 0.7587719298245614\n",
            "NaiveBayes Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.85      0.83       162\n",
            "         1.0       0.59      0.55      0.57        66\n",
            "\n",
            "    accuracy                           0.76       228\n",
            "   macro avg       0.71      0.70      0.70       228\n",
            "weighted avg       0.75      0.76      0.76       228\n",
            "\n",
            "\n",
            "Training SVM with split 0.08...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.08: 0.9692982456140351\n",
            "SVM Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       162\n",
            "         1.0       0.93      0.97      0.95        66\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.96      0.97      0.96       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.08...\n",
            "ZeroR Accuracy with split 0.08: 0.7105263157894737\n",
            "ZeroR Classification Report with split 0.08:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       162\n",
            "         1.0       0.00      0.00      0.00        66\n",
            "\n",
            "    accuracy                           0.71       228\n",
            "   macro avg       0.36      0.50      0.42       228\n",
            "weighted avg       0.50      0.71      0.59       228\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.91 train, 0.09 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.09...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.09: 0.9727626459143969\n",
            "DecisionTree Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       182\n",
            "         1.0       0.95      0.96      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.97      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.09...\n",
            "NaiveBayes Accuracy with split 0.09: 0.7704280155642024\n",
            "NaiveBayes Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.86      0.84       182\n",
            "         1.0       0.62      0.55      0.58        75\n",
            "\n",
            "    accuracy                           0.77       257\n",
            "   macro avg       0.72      0.70      0.71       257\n",
            "weighted avg       0.76      0.77      0.77       257\n",
            "\n",
            "\n",
            "Training SVM with split 0.09...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.09: 0.9727626459143969\n",
            "SVM Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       182\n",
            "         1.0       0.94      0.97      0.95        75\n",
            "\n",
            "    accuracy                           0.97       257\n",
            "   macro avg       0.96      0.97      0.97       257\n",
            "weighted avg       0.97      0.97      0.97       257\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.09...\n",
            "ZeroR Accuracy with split 0.09: 0.708171206225681\n",
            "ZeroR Classification Report with split 0.09:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83       182\n",
            "         1.0       0.00      0.00      0.00        75\n",
            "\n",
            "    accuracy                           0.71       257\n",
            "   macro avg       0.35      0.50      0.41       257\n",
            "weighted avg       0.50      0.71      0.59       257\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.9 train, 0.1 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.1...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.1: 0.9719298245614035\n",
            "DecisionTree Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.97      0.98       199\n",
            "         1.0       0.94      0.97      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.1...\n",
            "NaiveBayes Accuracy with split 0.1: 0.7719298245614035\n",
            "NaiveBayes Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.87      0.84       199\n",
            "         1.0       0.64      0.55      0.59        86\n",
            "\n",
            "    accuracy                           0.77       285\n",
            "   macro avg       0.73      0.71      0.72       285\n",
            "weighted avg       0.76      0.77      0.77       285\n",
            "\n",
            "\n",
            "Training SVM with split 0.1...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "SVM Accuracy with split 0.1: 0.9719298245614035\n",
            "SVM Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98       199\n",
            "         1.0       0.93      0.98      0.95        86\n",
            "\n",
            "    accuracy                           0.97       285\n",
            "   macro avg       0.96      0.97      0.97       285\n",
            "weighted avg       0.97      0.97      0.97       285\n",
            "\n",
            "\n",
            "Training ZeroR with split 0.1...\n",
            "ZeroR Accuracy with split 0.1: 0.6982456140350877\n",
            "ZeroR Classification Report with split 0.1:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      1.00      0.82       199\n",
            "         1.0       0.00      0.00      0.00        86\n",
            "\n",
            "    accuracy                           0.70       285\n",
            "   macro avg       0.35      0.50      0.41       285\n",
            "weighted avg       0.49      0.70      0.57       285\n",
            "\n",
            "\n",
            "Evaluating with train_test_split ratio: 0.89 train, 0.11 test\n",
            "\n",
            "\n",
            "Training DecisionTree with split 0.11...\n",
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "DecisionTree Accuracy with split 0.11: 0.9745222929936306\n",
            "DecisionTree Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.98       220\n",
            "         1.0       0.95      0.97      0.96        94\n",
            "\n",
            "    accuracy                           0.97       314\n",
            "   macro avg       0.97      0.97      0.97       314\n",
            "weighted avg       0.97      0.97      0.97       314\n",
            "\n",
            "\n",
            "Training NaiveBayes with split 0.11...\n",
            "NaiveBayes Accuracy with split 0.11: 0.7643312101910829\n",
            "NaiveBayes Classification Report with split 0.11:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.86      0.84       220\n",
            "         1.0       0.62      0.53      0.57        94\n",
            "\n",
            "    accuracy                           0.76       314\n",
            "   macro avg       0.72      0.70      0.71       314\n",
            "weighted avg       0.76      0.76      0.76       314\n",
            "\n",
            "\n",
            "Training SVM with split 0.11...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkVu69bYQBCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}